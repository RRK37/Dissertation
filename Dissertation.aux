\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\catcode `"\active 
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\xpginauxfiletrue }
\@writefile{lof}{\xpginauxfiletrue }
\@writefile{lot}{\xpginauxfiletrue }
\@writefile{toc}{\selectlanguage *[variant=us]{english}}
\@writefile{toc}{\selectlanguage *[variant=us]{english}}
\citation{StateAI2025}
\citation{kindigAIPowerConsumption}
\citation{maassNetworksSpikingNeurons1997}
\citation{maassComputationalPowerCircuits2004}
\citation{merollaMillionSpikingneuronIntegrated2014}
\citation{watersOpenAIsMindbogglingGrowth2025}
\citation{bradshawMetasInvestmentVR2025}
\citation{actonAppleDelaysIPhone2025}
\citation{wiggersRabbitBuildingAI2023}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Shows a 2nd generation neuron, the output is the sum of the inputs, plus a bias, passed through an activation function φ. (b) Shows a 3rd generation neuron, the current spikes entering the neuron are summed, when the sum reached \(v_{th}\) a spike is released at the output (\(y\)).}}{4}{figure.caption.3}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:second_vs_third_gen}{{1}{4}{(a) Shows a 2nd generation neuron, the output is the sum of the inputs, plus a bias, passed through an activation function φ. (b) Shows a 3rd generation neuron, the current spikes entering the neuron are summed, when the sum reached \(v_{th}\) a spike is released at the output (\(y\))}{figure.caption.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}{section.1}\protected@file@percent }
\newlabel{introduction}{{1}{4}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}The Problem to Solve}{5}{subsection.1.1}\protected@file@percent }
\newlabel{the-problem-to-solve}{{1.1}{5}{The Problem to Solve}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.2}Objectives}{5}{subsection.1.2}\protected@file@percent }
\newlabel{objectives}{{1.2}{5}{Objectives}{subsection.1.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Context and Past Work}{5}{section.2}\protected@file@percent }
\newlabel{context-and-past-work}{{2}{5}{Context and Past Work}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Spiking Neuron Model}{5}{subsection.2.1}\protected@file@percent }
\newlabel{spiking-neuron-model}{{2.1}{5}{Spiking Neuron Model}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Hodgkin--Huxley}{5}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{hodgkinhuxley}{{2.1.1}{5}{Hodgkin--Huxley}{subsubsection.2.1.1}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Diagram of neuron membrane.}}{6}{figure.caption.4}\protected@file@percent }
\newlabel{fig:membrane_diagram}{{2}{6}{Diagram of neuron membrane}{figure.caption.4}{}}
\newlabel{eqn:hh1}{{1}{6}{Hodgkin--Huxley}{equation.2.1}{}}
\newlabel{eqn:hh2}{{2}{7}{Hodgkin--Huxley}{equation.2.2}{}}
\newlabel{eqn:hh3}{{3}{7}{Hodgkin--Huxley}{equation.2.3}{}}
\newlabel{eqn:hh4}{{4}{7}{Hodgkin--Huxley}{equation.2.4}{}}
\newlabel{eqn:hh5}{{5}{7}{Hodgkin--Huxley}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Leaky Integrate-and-Fire Neuron}{7}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{leaky-integrate-and-fire-neuron}{{2.1.2}{7}{Leaky Integrate-and-Fire Neuron}{subsubsection.2.1.2}{}}
\newlabel{eqn:lif1}{{6}{7}{Leaky Integrate-and-Fire Neuron}{equation.2.6}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Voltage response of the Hodgkin--Huxley model (blue) to a 5ms step function input current (red).}}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:response_of_hh_1}{{3}{8}{Voltage response of the Hodgkin--Huxley model (blue) to a 5ms step function input current (red)}{figure.caption.5}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Voltage response of the Hodgkin--Huxley model (blue) to a 10ms step function input current (red).}}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:response_of_hh_15}{{4}{8}{Voltage response of the Hodgkin--Huxley model (blue) to a 10ms step function input current (red)}{figure.caption.6}{}}
\citation{izhikevichSimpleModelSpiking2003}
\citation{izhikevichSimpleModelSpiking2003}
\citation{izhikevichWhichModelUse2004}
\citation{izhikevichSimpleModelSpiking2003}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Voltage response of the Hodgkin--Huxley model (blue) to a 15ms step function input current (red).}}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:response_of_hh_2}{{5}{9}{Voltage response of the Hodgkin--Huxley model (blue) to a 15ms step function input current (red)}{figure.caption.7}{}}
\newlabel{eqn:lif2}{{7}{9}{Leaky Integrate-and-Fire Neuron}{equation.2.7}{}}
\newlabel{eqn:lif3}{{8}{9}{Leaky Integrate-and-Fire Neuron}{equation.2.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Izhikevich}{9}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{izhikevich}{{2.1.3}{9}{Izhikevich}{subsubsection.2.1.3}{}}
\newlabel{eqn:iz1}{{9}{9}{Izhikevich}{equation.2.9}{}}
\newlabel{eqn:iz2}{{10}{9}{Izhikevich}{equation.2.10}{}}
\citation{izhikevichWhichModelUse2004}
\citation{dengMachineLearningParadigms2013,hintonDeepNeuralNetworks2012}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Display of the the spiking and decaying behaviour of a leaky integrate-and-fire neuron. Spikes are released at the output of the neuron at \(t_1\), \(t_2\), \(t_3\), and \(t_4\).}}{10}{figure.caption.8}\protected@file@percent }
\newlabel{fig:lif_neuron_dynamics}{{6}{10}{Display of the the spiking and decaying behaviour of a leaky integrate-and-fire neuron. Spikes are released at the output of the neuron at \(t_1\), \(t_2\), \(t_3\), and \(t_4\)}{figure.caption.8}{}}
\newlabel{eqn:iz3}{{11}{10}{Izhikevich}{equation.2.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Neural Networks}{10}{subsubsection.2.1.4}\protected@file@percent }
\newlabel{neural-networks}{{2.1.4}{10}{Neural Networks}{subsubsection.2.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training Neural Networks}{10}{subsection.2.2}\protected@file@percent }
\newlabel{training-neural-networks}{{2.2}{10}{Training Neural Networks}{subsection.2.2}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Diagram of a 3 layer neural network. The weights of the synapses are visualised by the darkness of the connections.}}{11}{figure.caption.9}\protected@file@percent }
\newlabel{fig:simple_neural_network}{{7}{11}{Diagram of a 3 layer neural network. The weights of the synapses are visualised by the darkness of the connections}{figure.caption.9}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Backpropagation}{11}{subsubsection.2.2.1}\protected@file@percent }
\newlabel{backpropagation}{{2.2.1}{11}{Backpropagation}{subsubsection.2.2.1}{}}
\newlabel{eqn:loss_mse}{{12}{11}{Backpropagation}{equation.2.12}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The loss as a function of the parameters \(\theta _1\) and \(\theta _2\) plotted.}}{12}{figure.caption.10}\protected@file@percent }
\newlabel{fig:gradient_descent_3d_axis}{{8}{12}{The loss as a function of the parameters \(\theta _1\) and \(\theta _2\) plotted}{figure.caption.10}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The input. \(\mathbf  {x}\), is a vector of length 3. \(\mathbf  {h}\) is a hidden layer of length 6. \(\mathbf  {W}^{xh}\) is the matrix containing the values of each of the weights connecting \(\mathbf  {x}\) and \(\mathbf  {h}\), thus it is of size 3 by 6. \(\mathbf  {y}\) is the output vector of length 3. \(\mathbf  {W}^{hy}\) is the matrix containing the values of each of the weights connecting \(\mathbf  {h}\) and \(\mathbf  {y}\). thus it is of size 6 by 3.}}{12}{figure.caption.11}\protected@file@percent }
\newlabel{fig:mlp_3_layer}{{9}{12}{The input. \(\mathbf {x}\), is a vector of length 3. \(\mathbf {h}\) is a hidden layer of length 6. \(\mathbf {W}^{xh}\) is the matrix containing the values of each of the weights connecting \(\mathbf {x}\) and \(\mathbf {h}\), thus it is of size 3 by 6. \(\mathbf {y}\) is the output vector of length 3. \(\mathbf {W}^{hy}\) is the matrix containing the values of each of the weights connecting \(\mathbf {h}\) and \(\mathbf {y}\). thus it is of size 6 by 3}{figure.caption.11}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Matrix calculation of neural network values. The highlighted values of vector \(\mathbf  {x}\) are multiplied by highlighted values of the \(\mathbf  {W}\) matrix and summed to get \(\mathbf  {h}_1\).}}{13}{figure.caption.12}\protected@file@percent }
\newlabel{fig:matrix_of_network}{{10}{13}{Matrix calculation of neural network values. The highlighted values of vector \(\mathbf {x}\) are multiplied by highlighted values of the \(\mathbf {W}\) matrix and summed to get \(\mathbf {h}_1\)}{figure.caption.12}{}}
\newlabel{eqn:weightedsum2}{{13}{13}{Backpropagation}{equation.2.13}{}}
\citation{wuDeepSpikingNeural2020,bittarSurrogateGradientSpiking2022}
\citation{bellecBiologicallyInspiredAlternatives2019}
\citation{neftciSurrogateGradientLearning2019}
\citation{bellecBiologicallyInspiredAlternatives2019}
\citation{bittarSurrogateGradientSpiking2022,zhouDirectTrainingHighperformance2024}
\citation{bittarSurrogateGradientSpiking2022}
\citation{zhouDirectTrainingHighperformance2024}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}ANN-to-SNN Conversion}{14}{subsubsection.2.2.2}\protected@file@percent }
\newlabel{ann-to-snn-conversion}{{2.2.2}{14}{ANN-to-SNN Conversion}{subsubsection.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Backpropagation-Through-Time}{14}{subsubsection.2.2.3}\protected@file@percent }
\newlabel{backpropagation-through-time}{{2.2.3}{14}{Backpropagation-Through-Time}{subsubsection.2.2.3}{}}
\citation{arnaudyargaAcceleratingSpikingNeural2025,yargaAcceleratingSNNTraining2023}
\citation{arnaudyargaAcceleratingSpikingNeural2025}
\citation{arnaudyargaAcceleratingSpikingNeural2025,yargaAcceleratingSNNTraining2023}
\citation{yargaAcceleratingSNNTraining2023}
\citation{koopmanOvercomingLimitationsLayer2024,zhongSPikESSMSparsePrecise2024}
\citation{bellecEligibilityTracesProvide2019}
\citation{bellecEligibilityTracesProvide2019a,rostamiEpropSpiNNaker22022}
\citation{rostamiEpropSpiNNaker22022a}
\citation{rostamiEpropSpiNNaker22022}
\citation{bellecEligibilityTracesProvide2019}
\citation{bellecEligibilityTracesProvide2019a}
\newlabel{parallelisable-lif}{{2.2.3}{15}{Parallelisable LIF}{section*.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Parallelisable LIF}{15}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Eligibility Propagation}{15}{subsubsection.2.2.4}\protected@file@percent }
\newlabel{eligibility-propagation}{{2.2.4}{15}{Eligibility Propagation}{subsubsection.2.2.4}{}}
\citation{chenEssentialCharacteristicsMemristors2023}
\citation{liResearchProgressNeural2023}
\citation{weilenmannSingleNeuromorphicMemristor2024}
\citation{vlasovSpokenDigitsClassification2022}
\citation{sboevSpokenDigitsClassification2024}
\citation{guoDirectLearningbasedDeep2023}
\citation{leeTrainingDeepSpiking2018}
\citation{bittarSurrogateGradientSpiking2022a}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces The change of synaptic weight plotted against the difference in timing of the pre- and post-synaptic neuron spikes, showing the long term potential and long term depression phenomena.}}{16}{figure.caption.14}\protected@file@percent }
\newlabel{fig:stdp_ltp_ltd}{{11}{16}{The change of synaptic weight plotted against the difference in timing of the pre- and post-synaptic neuron spikes, showing the long term potential and long term depression phenomena}{figure.caption.14}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}Spike-Time-Dependent-Plasticity}{16}{subsubsection.2.2.5}\protected@file@percent }
\newlabel{spike-time-dependent-plasticity}{{2.2.5}{16}{Spike-Time-Dependent-Plasticity}{subsubsection.2.2.5}{}}
\citation{nowotnyLossShapingEnhances2025}
\citation{wunderlichEventbasedBackpropagationCan2021}
\citation{shoesmithEventpropTrainingEfficient2025}
\citation{nowotnyLossShapingEnhances2025}
\citation{nowotnyLossShapingEnhances2025}
\citation{nowotnyLossShapingEnhances2025}
\citation{nowotnyLossShapingEnhances2025}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Spikes propagated forward according to LIF neuron dynamics. Error signals propagated backward via computing adjoint variables backwards in time. \citep  {nowotnyLossShapingEnhances2025}}}{17}{figure.caption.15}\protected@file@percent }
\newlabel{fig:eventprop_backpropagation}{{12}{17}{Spikes propagated forward according to LIF neuron dynamics. Error signals propagated backward via computing adjoint variables backwards in time. \citep {nowotnyLossShapingEnhances2025}}{figure.caption.15}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6}Eventprop}{17}{subsubsection.2.2.6}\protected@file@percent }
\newlabel{eventprop}{{2.2.6}{17}{Eventprop}{subsubsection.2.2.6}{}}
\citation{wunderlichEventbasedBackpropagationCan2021}
\citation{nowotnyLossShapingEnhances2025}
\citation{nowotnyLossShapingEnhances2025}
\citation{nowotnyLossShapingEnhances2025}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Spikes propagated forward according to LIF neuron dynamics. Error signals are propagated backword according to the adjoint system. \citep  {nowotnyLossShapingEnhances2025}}}{18}{figure.caption.17}\protected@file@percent }
\newlabel{fig:eventprop_free_dynamics}{{13}{18}{Spikes propagated forward according to LIF neuron dynamics. Error signals are propagated backword according to the adjoint system. \citep {nowotnyLossShapingEnhances2025}}{figure.caption.17}{}}
\newlabel{mathematics-of-eventprop}{{2.2.6}{18}{Mathematics of Eventprop}{section*.16}{}}
\@writefile{toc}{\contentsline {paragraph}{Mathematics of Eventprop}{18}{section*.16}\protected@file@percent }
\newlabel{eqn:loss_grad_weight}{{21}{18}{Mathematics of Eventprop}{equation.2.21}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The membrane voltage of an output neuron which receives 2 spikes at the end of the trial. Spike 1 causes the voltage to increase and then decay to 0. Spike 2 increases the voltage, but before the voltage can decay to 0, the trial ends, so the tail end of the decay is cut off.}}{19}{figure.caption.18}\protected@file@percent }
\newlabel{fig:voltage_cutoff}{{14}{19}{The membrane voltage of an output neuron which receives 2 spikes at the end of the trial. Spike 1 causes the voltage to increase and then decay to 0. Spike 2 increases the voltage, but before the voltage can decay to 0, the trial ends, so the tail end of the decay is cut off}{figure.caption.18}{}}
\newlabel{eqn:lsum}{{22}{19}{Mathematics of Eventprop}{equation.2.22}{}}
\citation{SpikingHeidelbergDigits}
\citation{PapersCodeSSC}
\citation{garofoloTIMITAcousticPhoneticContinuous1993}
\citation{panayotovLibrispeechASRCorpus2015}
\citation{r.garyleonardTIDIGITS1993}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces A raster plot of the spikes transmitted to the output neurons of the network from the hidden neurons. There is a large number if spikes very early on.}}{20}{figure.caption.19}\protected@file@percent }
\newlabel{fig:spikes_hidden_to_output}{{15}{20}{A raster plot of the spikes transmitted to the output neurons of the network from the hidden neurons. There is a large number if spikes very early on}{figure.caption.19}{}}
\newlabel{eqn:lsumexp}{{23}{20}{Mathematics of Eventprop}{equation.2.23}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{20}{section.3}\protected@file@percent }
\newlabel{methods}{{3}{20}{Methods}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dataset used}{20}{subsection.3.1}\protected@file@percent }
\newlabel{dataset-used}{{3.1}{20}{Dataset used}{subsection.3.1}{}}
\citation{savareseLolemacsPytorcheventprop2024}
\citation{nowotnyTnowotnyGenn_eventprop2025}
\citation{nowotnyTnowotnyGenn_eventprop2025}
\citation{nowotnyLossShapingEnhances2025}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Software Libraries}{21}{subsection.3.2}\protected@file@percent }
\newlabel{software-libraries}{{3.2}{21}{Software Libraries}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Training the Network}{21}{subsection.3.3}\protected@file@percent }
\newlabel{training-the-network}{{3.3}{21}{Training the Network}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Deriving an New and Improved Loss Function and Eventprop Scheme}{21}{subsection.3.4}\protected@file@percent }
\newlabel{deriving-an-new-and-improved-loss-function-and-eventprop-scheme}{{3.4}{21}{Deriving an New and Improved Loss Function and Eventprop Scheme}{subsection.3.4}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Plot of the weightings of the exponential function (red) and the bell curve function (blue) against -t/T from 0 to 1.}}{22}{figure.caption.20}\protected@file@percent }
\newlabel{fig:exp_vs_bell_plot}{{16}{22}{Plot of the weightings of the exponential function (red) and the bell curve function (blue) against -t/T from 0 to 1}{figure.caption.20}{}}
\newlabel{eqn:exp_sum_simple}{{24}{22}{Deriving an New and Improved Loss Function and Eventprop Scheme}{equation.3.24}{}}
\newlabel{eqn:bell_sum_simple}{{25}{22}{Deriving an New and Improved Loss Function and Eventprop Scheme}{equation.3.25}{}}
\newlabel{eqn:gaus_sum_simple}{{26}{22}{Deriving an New and Improved Loss Function and Eventprop Scheme}{equation.3.26}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Plot of the weightings of the exponential function (red) and the Gaussian based function (green) against -t/T from 0 to 1.}}{23}{figure.caption.21}\protected@file@percent }
\newlabel{fig:exp_vs_gaus_plot}{{17}{23}{Plot of the weightings of the exponential function (red) and the Gaussian based function (green) against -t/T from 0 to 1}{figure.caption.21}{}}
\newlabel{eqn:lsumgaus}{{27}{23}{Deriving an New and Improved Loss Function and Eventprop Scheme}{equation.3.27}{}}
\newlabel{eqn:lambda_ode}{{28}{23}{Deriving an New and Improved Loss Function and Eventprop Scheme}{equation.3.28}{}}
\newlabel{eqn:lambda_gradient_ode}{{29}{23}{Deriving an New and Improved Loss Function and Eventprop Scheme}{equation.3.29}{}}
\newlabel{eqn:derivative_of_loss}{{30}{23}{Deriving an New and Improved Loss Function and Eventprop Scheme}{equation.3.30}{}}
\newlabel{eqn:lambda_i_tilde}{{31}{23}{Deriving an New and Improved Loss Function and Eventprop Scheme}{equation.3.31}{}}
\newlabel{eqn:lambda_v_tilde}{{32}{23}{Deriving an New and Improved Loss Function and Eventprop Scheme}{equation.3.32}{}}
\citation{zehraComparativeAnalysisPython2020}
\citation{nowotnyLossShapingEnhances2025}
\citation{nowotnyLossShapingEnhances2025}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Implementing the Novel Eventprop Scheme in Software}{26}{subsection.3.5}\protected@file@percent }
\newlabel{implementing-the-novel-eventprop-scheme-in-software}{{3.5}{26}{Implementing the Novel Eventprop Scheme in Software}{subsection.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Profiling the Power Usage of GPU vs CPU}{26}{subsection.3.6}\protected@file@percent }
\newlabel{profiling-the-power-usage-of-gpu-vs-cpu}{{3.6}{26}{Profiling the Power Usage of GPU vs CPU}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{26}{section.4}\protected@file@percent }
\newlabel{results}{{4}{26}{Results}{section.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Reproducing State-Of-The-Art Accuracy}{26}{subsection.4.1}\protected@file@percent }
\newlabel{reproducing-state-of-the-art-accuracy}{{4.1}{26}{Reproducing State-Of-The-Art Accuracy}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}New Loss Function Improves Training}{26}{subsection.4.2}\protected@file@percent }
\newlabel{new-loss-function-improves-training}{{4.2}{26}{New Loss Function Improves Training}{subsection.4.2}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Code to implement the mathematics of the Eventprop scheme for a Gaussian based loss.}}{27}{figure.caption.22}\protected@file@percent }
\newlabel{fig:code_of_evp}{{18}{27}{Code to implement the mathematics of the Eventprop scheme for a Gaussian based loss}{figure.caption.22}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces A view of the training in progress on the left, and the GPU being profiled on the right.}}{28}{figure.caption.23}\protected@file@percent }
\newlabel{fig:terminal_view}{{19}{28}{A view of the training in progress on the left, and the GPU being profiled on the right}{figure.caption.23}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Evaluation correctness vs epoch number, exponential weighting loss function. Red arrow shows maximum value reached.}}{28}{figure.caption.24}\protected@file@percent }
\newlabel{fig:result_of_exp_simple}{{20}{28}{Evaluation correctness vs epoch number, exponential weighting loss function. Red arrow shows maximum value reached}{figure.caption.24}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Evaluation correctness vs epoch number, Gaussian weighting loss function. Red arrow shows maximum value reached.}}{29}{figure.caption.25}\protected@file@percent }
\newlabel{fig:result_of_gaus_simple}{{21}{29}{Evaluation correctness vs epoch number, Gaussian weighting loss function. Red arrow shows maximum value reached}{figure.caption.25}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Plot of the mean correct evaluation accuracy per epoch of the Gaussian based sum loss (red) and the exponential sum loss (blue), over 5 runs each.}}{29}{figure.caption.26}\protected@file@percent }
\newlabel{fig:gaus_vs_exp_plot}{{22}{29}{Plot of the mean correct evaluation accuracy per epoch of the Gaussian based sum loss (red) and the exponential sum loss (blue), over 5 runs each}{figure.caption.26}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Bar graph of the time cost and energy cost for CPU and GPU using different batch sizes.}}{30}{figure.caption.27}\protected@file@percent }
\newlabel{fig:time_and_energy_cost}{{23}{30}{Bar graph of the time cost and energy cost for CPU and GPU using different batch sizes}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Power Usage of GPU and CPU}{30}{subsection.4.3}\protected@file@percent }
\newlabel{power-usage-of-gpu-and-cpu}{{4.3}{30}{Power Usage of GPU and CPU}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{30}{section.5}\protected@file@percent }
\newlabel{discussion}{{5}{30}{Discussion}{section.5}{}}
\citation{nowotnyLossShapingEnhances2025}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{31}{section.6}\protected@file@percent }
\newlabel{conclusion}{{6}{31}{Conclusion}{section.6}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Kamaji - my dad's GPU.}}{32}{figure.caption.28}\protected@file@percent }
\newlabel{fig:gpu_father}{{24}{32}{Kamaji - my dad's GPU}{figure.caption.28}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Project Review}{32}{section.7}\protected@file@percent }
\newlabel{project-review}{{7}{32}{Project Review}{section.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Acknowledgments}{32}{section.8}\protected@file@percent }
\newlabel{acknowledgments}{{8}{32}{Acknowledgments}{section.8}{}}
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{StateAI2025}{{1}{}{{}}{{}}}
\bibcite{kindigAIPowerConsumption}{{2}{}{{}}{{}}}
\bibcite{maassNetworksSpikingNeurons1997}{{3}{}{{}}{{}}}
\bibcite{maassComputationalPowerCircuits2004}{{4}{}{{}}{{}}}
\bibcite{merollaMillionSpikingneuronIntegrated2014}{{5}{}{{}}{{}}}
\bibcite{watersOpenAIsMindbogglingGrowth2025}{{6}{}{{}}{{}}}
\bibcite{bradshawMetasInvestmentVR2025}{{7}{}{{}}{{}}}
\bibcite{actonAppleDelaysIPhone2025}{{8}{}{{}}{{}}}
\bibcite{wiggersRabbitBuildingAI2023}{{9}{}{{}}{{}}}
\bibcite{izhikevichSimpleModelSpiking2003}{{10}{}{{}}{{}}}
\bibcite{izhikevichWhichModelUse2004}{{11}{}{{}}{{}}}
\bibcite{dengMachineLearningParadigms2013}{{12}{}{{}}{{}}}
\bibcite{hintonDeepNeuralNetworks2012}{{13}{}{{}}{{}}}
\bibcite{wuDeepSpikingNeural2020}{{14}{}{{}}{{}}}
\bibcite{bittarSurrogateGradientSpiking2022}{{15}{}{{}}{{}}}
\bibcite{bellecBiologicallyInspiredAlternatives2019}{{16}{}{{}}{{}}}
\bibcite{neftciSurrogateGradientLearning2019}{{17}{}{{}}{{}}}
\bibcite{zhouDirectTrainingHighperformance2024}{{18}{}{{}}{{}}}
\bibcite{arnaudyargaAcceleratingSpikingNeural2025}{{19}{}{{}}{{}}}
\bibcite{yargaAcceleratingSNNTraining2023}{{20}{}{{}}{{}}}
\bibcite{koopmanOvercomingLimitationsLayer2024}{{21}{}{{}}{{}}}
\bibcite{zhongSPikESSMSparsePrecise2024}{{22}{}{{}}{{}}}
\bibcite{bellecEligibilityTracesProvide2019}{{23}{}{{}}{{}}}
\bibcite{bellecEligibilityTracesProvide2019a}{{24}{}{{}}{{}}}
\bibcite{rostamiEpropSpiNNaker22022}{{25}{}{{}}{{}}}
\bibcite{rostamiEpropSpiNNaker22022a}{{26}{}{{}}{{}}}
\bibcite{chenEssentialCharacteristicsMemristors2023}{{27}{}{{}}{{}}}
\bibcite{liResearchProgressNeural2023}{{28}{}{{}}{{}}}
\bibcite{weilenmannSingleNeuromorphicMemristor2024}{{29}{}{{}}{{}}}
\bibcite{vlasovSpokenDigitsClassification2022}{{30}{}{{}}{{}}}
\bibcite{sboevSpokenDigitsClassification2024}{{31}{}{{}}{{}}}
\bibcite{guoDirectLearningbasedDeep2023}{{32}{}{{}}{{}}}
\bibcite{leeTrainingDeepSpiking2018}{{33}{}{{}}{{}}}
\bibcite{bittarSurrogateGradientSpiking2022a}{{34}{}{{}}{{}}}
\bibcite{nowotnyLossShapingEnhances2025}{{35}{}{{}}{{}}}
\bibcite{wunderlichEventbasedBackpropagationCan2021}{{36}{}{{}}{{}}}
\bibcite{shoesmithEventpropTrainingEfficient2025}{{37}{}{{}}{{}}}
\bibcite{SpikingHeidelbergDigits}{{38}{}{{}}{{}}}
\bibcite{PapersCodeSSC}{{39}{}{{}}{{}}}
\bibcite{garofoloTIMITAcousticPhoneticContinuous1993}{{40}{}{{}}{{}}}
\bibcite{panayotovLibrispeechASRCorpus2015}{{41}{}{{}}{{}}}
\bibcite{r.garyleonardTIDIGITS1993}{{42}{}{{}}{{}}}
\bibcite{savareseLolemacsPytorcheventprop2024}{{43}{}{{}}{{}}}
\bibcite{nowotnyTnowotnyGenn_eventprop2025}{{44}{}{{}}{{}}}
\bibcite{zehraComparativeAnalysisPython2020}{{45}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\xpginauxfilefalse }
\@writefile{lof}{\xpginauxfilefalse }
\@writefile{lot}{\xpginauxfilefalse }
\gdef \@abspage@last{35}
