\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand*\HyPL@Entry[1]{}
\catcode `"\active 
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\xpginauxfiletrue }
\@writefile{lof}{\xpginauxfiletrue }
\@writefile{lot}{\xpginauxfiletrue }
\@writefile{toc}{\selectlanguage *[variant=us]{english}}
\@writefile{toc}{\selectlanguage *[variant=us]{english}}
\citation{StateAI2025}
\citation{kindigAIPowerConsumption}
\citation{maassNetworksSpikingNeurons1997}
\citation{maassComputationalPowerCircuits2004}
\citation{merollaMillionSpikingneuronIntegrated2014}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces (a) Shows a 2nd generation neuron. (b) Shows a 3rd generation neuron.}}{4}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:second_vs_third_gen}{{1}{4}{(a) Shows a 2nd generation neuron. (b) Shows a 3rd generation neuron}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}{section.1}\protected@file@percent }
\newlabel{introduction}{{1}{4}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {1.1}Problem to Solve}{4}{subsection.1.1}\protected@file@percent }
\newlabel{problem-to-solve}{{1.1}{4}{Problem to Solve}{subsection.1.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Context}{5}{section.2}\protected@file@percent }
\newlabel{context}{{2}{5}{Context}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Spiking Neuron Model}{5}{subsection.2.1}\protected@file@percent }
\newlabel{spiking-neuron-model}{{2.1}{5}{Spiking Neuron Model}{subsection.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Hodgkin--Huxley}{5}{subsubsection.2.1.1}\protected@file@percent }
\newlabel{hodgkinhuxley}{{2.1.1}{5}{Hodgkin--Huxley}{subsubsection.2.1.1}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Diagram of neuron membrane.}}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:membrane_diagram}{{2}{6}{Diagram of neuron membrane}{figure.caption.3}{}}
\newlabel{eqn:hh1}{{1}{6}{Hodgkin--Huxley}{equation.2.1}{}}
\newlabel{eqn:hh2}{{2}{6}{Hodgkin--Huxley}{equation.2.2}{}}
\newlabel{eqn:hh3}{{3}{6}{Hodgkin--Huxley}{equation.2.3}{}}
\newlabel{eqn:hh4}{{4}{6}{Hodgkin--Huxley}{equation.2.4}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Response of the Hodgkin--Huxley model (blue) to a 5ms step function input current (red).}}{7}{figure.caption.4}\protected@file@percent }
\newlabel{fig:response_of_hh_1}{{3}{7}{Response of the Hodgkin--Huxley model (blue) to a 5ms step function input current (red)}{figure.caption.4}{}}
\newlabel{eqn:hh5}{{5}{7}{Hodgkin--Huxley}{equation.2.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Leaky Integrate-and-Fire Neuron}{7}{subsubsection.2.1.2}\protected@file@percent }
\newlabel{leaky-integrate-and-fire-neuron}{{2.1.2}{7}{Leaky Integrate-and-Fire Neuron}{subsubsection.2.1.2}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Response of the Hodgkin--Huxley model (blue) to a 10ms step function input current (red).}}{8}{figure.caption.5}\protected@file@percent }
\newlabel{fig:response_of_hh_15}{{4}{8}{Response of the Hodgkin--Huxley model (blue) to a 10ms step function input current (red)}{figure.caption.5}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Response of the Hodgkin--Huxley model (blue) to a 15ms step function input current (red).}}{8}{figure.caption.6}\protected@file@percent }
\newlabel{fig:response_of_hh_2}{{5}{8}{Response of the Hodgkin--Huxley model (blue) to a 15ms step function input current (red)}{figure.caption.6}{}}
\citation{izhikevichSimpleModelSpiking2003}
\citation{izhikevichSimpleModelSpiking2003}
\citation{izhikevichWhichModelUse2004}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces LIF neuron plot of \(V_{mem}\).}}{9}{figure.caption.7}\protected@file@percent }
\newlabel{fig:lif_neuron_dynamics}{{6}{9}{LIF neuron plot of \(V_{mem}\)}{figure.caption.7}{}}
\newlabel{eqn:lif1}{{6}{9}{Leaky Integrate-and-Fire Neuron}{equation.2.6}{}}
\newlabel{eqn:lif2}{{7}{9}{Leaky Integrate-and-Fire Neuron}{equation.2.7}{}}
\newlabel{eqn:lif3}{{8}{9}{Leaky Integrate-and-Fire Neuron}{equation.2.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Izhikevich}{9}{subsubsection.2.1.3}\protected@file@percent }
\newlabel{izhikevich}{{2.1.3}{9}{Izhikevich}{subsubsection.2.1.3}{}}
\newlabel{eqn:iz1}{{9}{9}{Izhikevich}{equation.2.9}{}}
\citation{izhikevichSimpleModelSpiking2003}
\citation{izhikevichWhichModelUse2004}
\citation{dengMachineLearningParadigms2013,hintonDeepNeuralNetworks2012}
\newlabel{eqn:iz2}{{10}{10}{Izhikevich}{equation.2.10}{}}
\newlabel{eqn:iz3}{{11}{10}{Izhikevich}{equation.2.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Neural Networks}{10}{subsubsection.2.1.4}\protected@file@percent }
\newlabel{neural-networks}{{2.1.4}{10}{Neural Networks}{subsubsection.2.1.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Training Neural Networks}{10}{subsection.2.2}\protected@file@percent }
\newlabel{training-neural-networks}{{2.2}{10}{Training Neural Networks}{subsection.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Backpropagation}{10}{subsubsection.2.2.1}\protected@file@percent }
\newlabel{backpropagation}{{2.2.1}{10}{Backpropagation}{subsubsection.2.2.1}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Diagram of a 3 layer neural network. The weights of the synapses are visualised by the colours of the connections.}}{11}{figure.caption.8}\protected@file@percent }
\newlabel{fig:simple_neural_network}{{7}{11}{Diagram of a 3 layer neural network. The weights of the synapses are visualised by the colours of the connections}{figure.caption.8}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces The loss as a function of the parameters \(\theta _1\) and \(\theta _2\) plotted.}}{12}{figure.caption.9}\protected@file@percent }
\newlabel{fig:gradient_descent_3d_axis}{{8}{12}{The loss as a function of the parameters \(\theta _1\) and \(\theta _2\) plotted}{figure.caption.9}{}}
\newlabel{eqn:loss_mse}{{12}{12}{Backpropagation}{equation.2.12}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces The input. \(\mathbf  {x}\), is a vector of length 3. \(\mathbf  {h}\) is a hidden layer of length 6. \(\mathbf  {W}^{xh}\) is the matrix containing the values of each of the weights connecting \(\mathbf  {x}\) and \(\mathbf  {h}\), thus it is of size 3 by 6. \(\mathbf  {y}\) is the output vector of length 3. \(\mathbf  {W}^{hy}\) is the matrix containing the values of each of the weights connecting \(\mathbf  {h}\) and \(\mathbf  {y}\). thus it is of size 6 by 3.}}{13}{figure.caption.10}\protected@file@percent }
\newlabel{fig:mlp_3_layer}{{9}{13}{The input. \(\mathbf {x}\), is a vector of length 3. \(\mathbf {h}\) is a hidden layer of length 6. \(\mathbf {W}^{xh}\) is the matrix containing the values of each of the weights connecting \(\mathbf {x}\) and \(\mathbf {h}\), thus it is of size 3 by 6. \(\mathbf {y}\) is the output vector of length 3. \(\mathbf {W}^{hy}\) is the matrix containing the values of each of the weights connecting \(\mathbf {h}\) and \(\mathbf {y}\). thus it is of size 6 by 3}{figure.caption.10}{}}
\newlabel{eqn:weightedsum2}{{13}{13}{Backpropagation}{equation.2.13}{}}
\citation{GradientDescentGradient2019}
\citation{GradientDescentGradient2019}
\citation{wuDeepSpikingNeural2020,bittarSurrogateGradientSpiking2022}
\citation{bellecBiologicallyInspiredAlternatives2019}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Matrix calculation of neural network values. The highlighted values of vector \(\mathbf  {x}\) are multiplied by highlighted values of the \(\mathbf  {W}\) matrix and summed to get \(\mathbf  {h}_1\).}}{14}{figure.caption.11}\protected@file@percent }
\newlabel{fig:matrix_of_network}{{10}{14}{Matrix calculation of neural network values. The highlighted values of vector \(\mathbf {x}\) are multiplied by highlighted values of the \(\mathbf {W}\) matrix and summed to get \(\mathbf {h}_1\)}{figure.caption.11}{}}
\citation{neftciSurrogateGradientLearning2019}
\citation{bellecBiologicallyInspiredAlternatives2019}
\citation{bittarSurrogateGradientSpiking2022,zhouDirectTrainingHighperformance2024}
\citation{bittarSurrogateGradientSpiking2022}
\citation{zhouDirectTrainingHighperformance2024}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Loss function 3D surface being descended \citep  {GradientDescentGradient2019}.}}{15}{figure.caption.12}\protected@file@percent }
\newlabel{fig:gradient_descent_3d}{{11}{15}{Loss function 3D surface being descended \citep {GradientDescentGradient2019}}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}ANN-to-SNN Conversion}{15}{subsubsection.2.2.2}\protected@file@percent }
\newlabel{ann-to-snn-conversion}{{2.2.2}{15}{ANN-to-SNN Conversion}{subsubsection.2.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Backpropagation-Through-Time}{15}{subsubsection.2.2.3}\protected@file@percent }
\newlabel{backpropagation-through-time}{{2.2.3}{15}{Backpropagation-Through-Time}{subsubsection.2.2.3}{}}
\citation{arnaudyargaAcceleratingSpikingNeural2025,yargaAcceleratingSNNTraining2023}
\citation{arnaudyargaAcceleratingSpikingNeural2025}
\citation{arnaudyargaAcceleratingSpikingNeural2025}
\citation{arnaudyargaAcceleratingSpikingNeural2025}
\citation{arnaudyargaAcceleratingSpikingNeural2025,yargaAcceleratingSNNTraining2023}
\citation{yargaAcceleratingSNNTraining2023}
\citation{maasNetworksSpikingNeurons1997}
\citation{koopmanOvercomingLimitationsLayer2024,zhongSPikESSMSparsePrecise2024}
\citation{bellecEligibilityTracesProvide2019}
\citation{bellecEligibilityTracesProvide2019a,rostamiEpropSpiNNaker22022}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces A) Feedforward LIF processing flow. B) Feedforward ParaLIF processing flow. \citep  {arnaudyargaAcceleratingSpikingNeural2025}}}{16}{figure.caption.14}\protected@file@percent }
\newlabel{fig:parallel_lif}{{12}{16}{A) Feedforward LIF processing flow. B) Feedforward ParaLIF processing flow. \citep {arnaudyargaAcceleratingSpikingNeural2025}}{figure.caption.14}{}}
\newlabel{parallelisable-lif}{{2.2.3}{16}{Parallelisable LIF}{section*.13}{}}
\@writefile{toc}{\contentsline {paragraph}{Parallelisable LIF}{16}{section*.13}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Eligibility Propagation}{16}{subsubsection.2.2.4}\protected@file@percent }
\newlabel{eligibility-propagation}{{2.2.4}{16}{Eligibility Propagation}{subsubsection.2.2.4}{}}
\citation{bellecEligibilityTracesProvide2019}
\citation{veenIncludingSTDPEligibility2021}
\citation{rostamiEpropSpiNNaker22022a}
\citation{rostamiEpropSpiNNaker22022,turn0search8}
\citation{bellecEligibilityTracesProvide2019}
\citation{bellecEligibilityTracesProvide2019a}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.5}Spike-Time-Dependent-Plasticity}{17}{subsubsection.2.2.5}\protected@file@percent }
\newlabel{spike-time-dependent-plasticity}{{2.2.5}{17}{Spike-Time-Dependent-Plasticity}{subsubsection.2.2.5}{}}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces The change of synaptic weight plotted against the difference in timing of the pre- and post-synaptic neuron spikes, showing the long term potential and long term depression phenomena.}}{18}{figure.caption.15}\protected@file@percent }
\newlabel{fig:stdp_ltp_ltd}{{13}{18}{The change of synaptic weight plotted against the difference in timing of the pre- and post-synaptic neuron spikes, showing the long term potential and long term depression phenomena}{figure.caption.15}{}}
\citation{chenEssentialCharacteristicsMemristors2023}
\citation{liResearchProgressNeural2023}
\citation{weilenmannSingleNeuromorphicMemristor2024}
\citation{vlasovSpokenDigitsClassification2022}
\citation{sboevSpokenDigitsClassification2024}
\citation{guoDirectLearningbasedDeep2023}
\citation{liuSSTDPSupervisedSpike2021}
\citation{leeTrainingDeepSpiking2018}
\citation{bittarSurrogateGradientSpiking2022a}
\citation{nowotnyLossShapingEnhances2025}
\citation{wunderlichEventbasedBackpropagationCan2021}
\citation{shoesmithEventpropTrainingEfficient2025}
\citation{nowotnyLossShapingEnhances2025}
\citation{nowotnyLossShapingEnhances2025}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.6}Eventprop}{19}{subsubsection.2.2.6}\protected@file@percent }
\newlabel{eventprop}{{2.2.6}{19}{Eventprop}{subsubsection.2.2.6}{}}
\citation{SpikingHeidelbergDigits}
\@writefile{lof}{\setforeignlanguage {english}}
\@writefile{lot}{\setforeignlanguage {english}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Spikes propagated forward according to LIF neuron dynamics. Error signals propagated backward via computing adjoint variables backwards in time.}}{20}{figure.caption.16}\protected@file@percent }
\newlabel{fig:eventprop_backpropagation}{{14}{20}{Spikes propagated forward according to LIF neuron dynamics. Error signals propagated backward via computing adjoint variables backwards in time}{figure.caption.16}{}}
\newlabel{mathematics-of-eventprop}{{2.2.6}{20}{Mathematics of Eventprop}{section*.17}{}}
\@writefile{toc}{\contentsline {paragraph}{Mathematics of Eventprop}{20}{section*.17}\protected@file@percent }
\newlabel{eqn:lsumexp}{{19}{20}{Mathematics of Eventprop}{equation.2.19}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Methods}{20}{section.3}\protected@file@percent }
\newlabel{methods}{{3}{20}{Methods}{section.3}{}}
\citation{PapersCodeSSC}
\citation{nowotnyLossShapingEnhances2025}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Dataset used}{21}{subsection.3.1}\protected@file@percent }
\newlabel{dataset-used}{{3.1}{21}{Dataset used}{subsection.3.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Software Libraries}{21}{subsection.3.2}\protected@file@percent }
\newlabel{software-libraries}{{3.2}{21}{Software Libraries}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Accessing GPU Resources}{21}{subsection.3.3}\protected@file@percent }
\newlabel{accessing-gpu-resources}{{3.3}{21}{Accessing GPU Resources}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Splitting Training and Evaluation Data}{21}{subsection.3.4}\protected@file@percent }
\newlabel{splitting-training-and-evaluation-data}{{3.4}{21}{Splitting Training and Evaluation Data}{subsection.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Training the Network}{21}{subsection.3.5}\protected@file@percent }
\newlabel{training-the-network}{{3.5}{21}{Training the Network}{subsection.3.5}{}}
\citation{nowotnyLossShapingEnhances2025}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}Deriving and implementing a Novel Loss Function}{22}{subsection.3.6}\protected@file@percent }
\newlabel{deriving-and-implementing-a-novel-loss-function}{{3.6}{22}{Deriving and implementing a Novel Loss Function}{subsection.3.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}Neural Network Model Description}{22}{subsection.3.7}\protected@file@percent }
\newlabel{neural-network-model-description}{{3.7}{22}{Neural Network Model Description}{subsection.3.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}Bayesian Optimisation of Hyperparameters}{22}{subsection.3.8}\protected@file@percent }
\newlabel{bayesian-optimisation-of-hyperparameters}{{3.8}{22}{Bayesian Optimisation of Hyperparameters}{subsection.3.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Results}{22}{section.4}\protected@file@percent }
\newlabel{results}{{4}{22}{Results}{section.4}{}}
\bibstyle{IEEEtran}
\bibdata{references}
\bibcite{StateAI2025}{{1}{}{{}}{{}}}
\bibcite{kindigAIPowerConsumption}{{2}{}{{}}{{}}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Reproducing State-Of-The-Art Accuracy}{23}{subsection.4.1}\protected@file@percent }
\newlabel{reproducing-state-of-the-art-accuracy}{{4.1}{23}{Reproducing State-Of-The-Art Accuracy}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Comparing Existing Loss Functions}{23}{subsection.4.2}\protected@file@percent }
\newlabel{comparing-existing-loss-functions}{{4.2}{23}{Comparing Existing Loss Functions}{subsection.4.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}New Loss Function Improves Training}{23}{subsection.4.3}\protected@file@percent }
\newlabel{new-loss-function-improves-training}{{4.3}{23}{New Loss Function Improves Training}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}More Efficient Hyperparameter Optimisation Using Bayesian Optimisation}{23}{subsection.4.4}\protected@file@percent }
\newlabel{more-efficient-hyperparameter-optimisation-using-bayesian-optimisation}{{4.4}{23}{More Efficient Hyperparameter Optimisation Using Bayesian Optimisation}{subsection.4.4}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Discussion}{23}{section.5}\protected@file@percent }
\newlabel{discussion}{{5}{23}{Discussion}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Conclusion}{23}{section.6}\protected@file@percent }
\newlabel{conclusion}{{6}{23}{Conclusion}{section.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Project Review}{23}{section.7}\protected@file@percent }
\newlabel{project-review}{{7}{23}{Project Review}{section.7}{}}
\bibcite{maassNetworksSpikingNeurons1997}{{3}{}{{}}{{}}}
\bibcite{maassComputationalPowerCircuits2004}{{4}{}{{}}{{}}}
\bibcite{merollaMillionSpikingneuronIntegrated2014}{{5}{}{{}}{{}}}
\bibcite{izhikevichSimpleModelSpiking2003}{{6}{}{{}}{{}}}
\bibcite{izhikevichWhichModelUse2004}{{7}{}{{}}{{}}}
\bibcite{dengMachineLearningParadigms2013}{{8}{}{{}}{{}}}
\bibcite{hintonDeepNeuralNetworks2012}{{9}{}{{}}{{}}}
\bibcite{wuDeepSpikingNeural2020}{{10}{}{{}}{{}}}
\bibcite{bittarSurrogateGradientSpiking2022}{{11}{}{{}}{{}}}
\bibcite{bellecBiologicallyInspiredAlternatives2019}{{12}{}{{}}{{}}}
\bibcite{neftciSurrogateGradientLearning2019}{{13}{}{{}}{{}}}
\bibcite{zhouDirectTrainingHighperformance2024}{{14}{}{{}}{{}}}
\bibcite{arnaudyargaAcceleratingSpikingNeural2025}{{15}{}{{}}{{}}}
\bibcite{yargaAcceleratingSNNTraining2023}{{16}{}{{}}{{}}}
\bibcite{maasNetworksSpikingNeurons1997}{{17}{}{{}}{{}}}
\bibcite{koopmanOvercomingLimitationsLayer2024}{{18}{}{{}}{{}}}
\bibcite{zhongSPikESSMSparsePrecise2024}{{19}{}{{}}{{}}}
\bibcite{bellecEligibilityTracesProvide2019}{{20}{}{{}}{{}}}
\bibcite{bellecEligibilityTracesProvide2019a}{{21}{}{{}}{{}}}
\bibcite{rostamiEpropSpiNNaker22022}{{22}{}{{}}{{}}}
\bibcite{veenIncludingSTDPEligibility2021}{{23}{}{{}}{{}}}
\bibcite{rostamiEpropSpiNNaker22022a}{{24}{}{{}}{{}}}
\bibcite{chenEssentialCharacteristicsMemristors2023}{{25}{}{{}}{{}}}
\bibcite{liResearchProgressNeural2023}{{26}{}{{}}{{}}}
\bibcite{weilenmannSingleNeuromorphicMemristor2024}{{27}{}{{}}{{}}}
\bibcite{vlasovSpokenDigitsClassification2022}{{28}{}{{}}{{}}}
\bibcite{sboevSpokenDigitsClassification2024}{{29}{}{{}}{{}}}
\bibcite{guoDirectLearningbasedDeep2023}{{30}{}{{}}{{}}}
\bibcite{liuSSTDPSupervisedSpike2021}{{31}{}{{}}{{}}}
\bibcite{leeTrainingDeepSpiking2018}{{32}{}{{}}{{}}}
\bibcite{bittarSurrogateGradientSpiking2022a}{{33}{}{{}}{{}}}
\bibcite{nowotnyLossShapingEnhances2025}{{34}{}{{}}{{}}}
\bibcite{shoesmithEventpropTrainingEfficient2025}{{35}{}{{}}{{}}}
\bibcite{SpikingHeidelbergDigits}{{36}{}{{}}{{}}}
\providecommand\NAT@force@numbers{}\NAT@force@numbers
\@writefile{toc}{\xpginauxfilefalse }
\@writefile{lof}{\xpginauxfilefalse }
\@writefile{lot}{\xpginauxfilefalse }
\gdef \@abspage@last{25}
