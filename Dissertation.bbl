% Generated by IEEEtran.bst, version: 1.14 (2015/08/26)
\begin{thebibliography}{10}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus
  \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{StateAI2025}
``The state of {{AI}} in 2025: {{Global}} survey {\textbar} {{McKinsey}},''
  https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai.

\bibitem{kindigAIPowerConsumption}
B.~Kindig, ``{{AI Power Consumption}}: {{Rapidly Becoming Mission-Critical}},''
  https://www.forbes.com/sites/bethkindig/2024/06/20/ai-power-consumption-rapidly-becoming-mission-critical/.

\bibitem{maassNetworksSpikingNeurons1997}
W.~Maass, ``Networks of spiking neurons: {{The}} third generation of neural
  network models,'' \emph{Neural Networks}, vol.~10, no.~9, pp. 1659--1671,
  Dec. 1997.

\bibitem{maassComputationalPowerCircuits2004}
W.~Maass and H.~Markram, ``On the computational power of circuits of spiking
  neurons,'' \emph{Journal of Computer and System Sciences}, vol.~69, no.~4,
  pp. 593--616, Dec. 2004.

\bibitem{merollaMillionSpikingneuronIntegrated2014}
P.~A. Merolla, J.~V. Arthur, R.~{Alvarez-Icaza}, A.~S. Cassidy, J.~Sawada,
  F.~Akopyan, B.~L. Jackson, N.~Imam, C.~Guo, Y.~Nakamura, B.~Brezzo, I.~Vo,
  S.~K. Esser, R.~Appuswamy, B.~Taba, A.~Amir, M.~D. Flickner, W.~P. Risk,
  R.~Manohar, and D.~S. Modha, ``A million spiking-neuron integrated circuit
  with a scalable communication network and interface,'' \emph{Science}, vol.
  345, no. 6197, pp. 668--673, 2014.

\bibitem{watersOpenAIsMindbogglingGrowth2025}
R.~Waters, ``{{OpenAI}}'s mind-boggling growth masks challenges,''
  \emph{Financial Times}, Apr. 2025.

\bibitem{bradshawMetasInvestmentVR2025}
T.~Bradshaw and H.~Murphy, ``Meta's investment in {{VR}} and smart glasses on
  track to top \$100bn,'' \emph{Financial Times}, Feb. 2025.

\bibitem{actonAppleDelaysIPhone2025}
M.~Acton, ``Apple delays {{iPhone AI}} features as it stumbles in race with
  rivals,'' \emph{Financial Times}, Mar. 2025.

\bibitem{wiggersRabbitBuildingAI2023}
K.~Wiggers, ``Rabbit is building an {{AI}} model that understands how software
  works,'' Oct. 2023.

\bibitem{izhikevichSimpleModelSpiking2003}
E.~Izhikevich, ``Simple model of spiking neurons,'' \emph{IEEE Transactions on
  Neural Networks}, vol.~14, no.~6, pp. 1569--1572, Nov. 2003.

\bibitem{izhikevichWhichModelUse2004}
------, ``Which model to use for cortical spiking neurons?'' \emph{IEEE
  Transactions on Neural Networks}, vol.~15, no.~5, pp. 1063--1070, Sep. 2004.

\bibitem{dengMachineLearningParadigms2013}
L.~Deng and X.~Li, ``Machine {{Learning Paradigms}} for {{Speech Recognition}}:
  {{An Overview}},'' \emph{IEEE Transactions on Audio, Speech, and Language
  Processing}, vol.~21, no.~5, pp. 1060--1089, May 2013.

\bibitem{hintonDeepNeuralNetworks2012}
G.~Hinton, L.~Deng, D.~Yu, G.~Dahl, A.-r. Mohamed, N.~Jaitly, V.~Vanhoucke,
  P.~Nguyen, T.~Sainath, and B.~Kingsbury, ``Deep {{Neural Networks}} for
  {{Acoustic Modeling}} in {{Speech Recognition}},'' 2012.

\bibitem{wuDeepSpikingNeural2020}
J.~Wu, E.~Y{\i}lmaz, M.~Zhang, H.~Li, and K.~C. Tan, ``Deep {{Spiking Neural
  Networks}} for {{Large Vocabulary Automatic Speech Recognition}},''
  \emph{Frontiers in Neuroscience}, vol.~14, p. 199, Mar. 2020.

\bibitem{bittarSurrogateGradientSpiking2022}
A.~Bittar and P.~N. Garner, ``A surrogate gradient spiking baseline for speech
  command recognition,'' \emph{Frontiers in Neuroscience}, vol.~16, p. 865897,
  Aug. 2022.

\bibitem{bellecBiologicallyInspiredAlternatives2019}
G.~Bellec, F.~Scherr, E.~Hajek, D.~Salaj, R.~Legenstein, and W.~Maass,
  ``Biologically inspired alternatives to backpropagation through time for
  learning in recurrent neural nets,'' \emph{arXiv.org}, Feb. 2019.

\bibitem{neftciSurrogateGradientLearning2019}
E.~O. Neftci, M.~Hesham, and Z.~Friedemann, ``Surrogate {{Gradient Learning}}
  in {{Spiking Neural Networks}}: {{Bringing}} the {{Power}} of
  {{Gradient-Based Optimization}} to {{Spiking Neural Networks}},'' \emph{IEEE
  Signal Processing Magazine}, vol.~36, no.~6, pp. 51--63, Nov. 2019.

\bibitem{zhouDirectTrainingHighperformance2024}
C.~Zhou, H.~Zhang, L.~Yu, Y.~Ye, Z.~Zhou, L.~Huang, Z.~Ma, X.~Fan, H.~Zhou, and
  Y.~Tian, ``Direct training high-performance deep spiking neural networks: A
  review of theories and methods,'' \emph{Frontiers in Neuroscience}, vol.~18,
  Jul. 2024.

\bibitem{arnaudyargaAcceleratingSpikingNeural2025}
S.~Y. Arnaud~Yarga and S.~U.~N. Wood, ``Accelerating spiking neural networks
  with parallelizable leaky integrate-and-fire neurons*,'' \emph{Neuromorphic
  Computing and Engineering}, vol.~5, no. 014012, Mar. 2025.

\bibitem{yargaAcceleratingSNNTraining2023}
S.~Y.~A. Yarga and S.~U.~N. Wood, ``Accelerating {{SNN Training}} with
  {{Stochastic Parallelizable Spiking Neurons}},'' in \emph{2023
  {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})}, Jun.
  2023, pp. 1--8.

\bibitem{koopmanOvercomingLimitationsLayer2024}
R.~Koopman, A.~Yousefzadeh, M.~Shahsavari, G.~Tang, and M.~Sifalakis,
  ``Overcoming the {{Limitations}} of {{Layer Synchronization}} in {{Spiking
  Neural Networks}},'' Aug. 2024.

\bibitem{zhongSPikESSMSparsePrecise2024}
Y.~Zhong, R.~Zhao, C.~Wang, Q.~Guo, J.~Zhang, Z.~Lu, and L.~Leng,
  ``{{SPikE-SSM}}: {{A Sparse}}, {{Precise}}, and {{Efficient Spiking State
  Space Model}} for {{Long Sequences Learning}},'' Oct. 2024.

\bibitem{bellecEligibilityTracesProvide2019}
G.~Bellec, F.~Scherr, E.~Hajek, D.~Salaj, A.~Subramoney, R.~Legenstein, and
  W.~Maass, ``Eligibility traces provide a data-inspired alternative to
  backpropagation through time,'' in \emph{Real {{Neurons}}
  \{{\textbackslash}\&\} {{Hidden Units}}: {{Future}} Directions at the
  Intersection of Neuroscience and Artificial Intelligence @ {{NeurIPS}} 2019},
  Oct. 2019.

\bibitem{bellecEligibilityTracesProvide2019a}
------, ``Eligibility traces provide a data-inspired alternative to
  backpropagation through time,'' in \emph{Real {{Neurons}}
  \{{\textbackslash}\&\} {{Hidden Units}}: {{Future}} Directions at the
  Intersection of Neuroscience and Artificial Intelligence @ {{NeurIPS}} 2019},
  Oct. 2019.

\bibitem{rostamiEpropSpiNNaker22022}
A.~Rostami, B.~Vogginger, Y.~Yan, and C.~G. Mayr, ``E-prop on {{SpiNNaker}} 2:
  {{Exploring}} online learning in spiking {{RNNs}} on neuromorphic hardware,''
  \emph{Frontiers in Neuroscience}, vol.~16, p. 1018006, Nov. 2022.

\bibitem{rostamiEpropSpiNNaker22022a}
------, ``E-prop on {{SpiNNaker}} 2: {{Exploring}} online learning in spiking
  {{RNNs}} on neuromorphic hardware,'' \emph{Frontiers in Neuroscience},
  vol.~16, Nov. 2022.

\bibitem{chenEssentialCharacteristicsMemristors2023}
W.~Chen, L.~Song, S.~Wang, Z.~Zhang, G.~Wang, G.~Hu, and S.~Gao, ``Essential
  {{Characteristics}} of {{Memristors}} for {{Neuromorphic Computing}},''
  \emph{Advanced Electronic Materials}, vol.~9, no.~2, p. 2200833, 2023.

\bibitem{liResearchProgressNeural2023}
Y.~Li, K.~Su, H.~Chen, X.~Zou, C.~Wang, H.~Man, K.~Liu, X.~Xi, and T.~Li,
  ``Research {{Progress}} of {{Neural Synapses Based}} on {{Memristors}},''
  \emph{Electronics}, vol.~12, no.~15, p. 3298, Jan. 2023.

\bibitem{weilenmannSingleNeuromorphicMemristor2024}
C.~Weilenmann, A.~N. Ziogas, T.~Zellweger, K.~Portner, M.~Mladenovi{\'c},
  M.~Kaniselvan, T.~Moraitis, M.~Luisier, and A.~Emboras, ``Single neuromorphic
  memristor closely emulates multiple synaptic mechanisms for energy efficient
  neural networks,'' \emph{Nature Communications}, vol.~15, no.~1, p. 6898,
  Aug. 2024.

\bibitem{vlasovSpokenDigitsClassification2022}
D.~Vlasov, Y.~Davydov, A.~Serenko, R.~Rybka, and A.~Sboev, ``Spoken {{Digits
  Classification Based}} on {{Spiking Neural Networks}} with {{Memristor-Based
  STDP}},'' in \emph{2022 {{International Conference}} on {{Computational
  Science}} and {{Computational Intelligence}} ({{CSCI}})}, Dec. 2022, pp.
  330--335.

\bibitem{sboevSpokenDigitsClassification2024}
A.~Sboev, M.~Balykov, D.~Kunitsyn, and A.~Serenko, ``Spoken {{Digits
  Classification Using}} a {{Spiking Neural Network}} with {{Fixed Synaptic
  Weights}},'' in \emph{Biologically {{Inspired Cognitive Architectures}}
  2023}, A.~V. Samsonovich and T.~Liu, Eds.\hskip 1em plus 0.5em minus
  0.4em\relax Cham: Springer Nature Switzerland, 2024, pp. 767--774.

\bibitem{guoDirectLearningbasedDeep2023}
Y.~Guo, X.~Huang, and Z.~Ma, ``Direct learning-based deep spiking neural
  networks: A review,'' \emph{Frontiers in Neuroscience}, vol.~17, p. 1209795,
  Jun. 2023.

\bibitem{leeTrainingDeepSpiking2018}
C.~Lee, P.~Panda, G.~Srinivasan, and K.~Roy, ``Training {{Deep Spiking
  Convolutional Neural Networks With STDP-Based Unsupervised Pre-training
  Followed}} by {{Supervised Fine-Tuning}},'' \emph{Frontiers in Neuroscience},
  vol.~12, Aug. 2018.

\bibitem{bittarSurrogateGradientSpiking2022a}
A.~Bittar and P.~N. Garner, ``A surrogate gradient spiking baseline for speech
  command recognition,'' \emph{Frontiers in Neuroscience}, vol.~16, Aug. 2022.

\bibitem{nowotnyLossShapingEnhances2025}
T.~Nowotny, J.~P. Turner, and J.~C. Knight, ``Loss shaping enhances exact
  gradient learning with {{Eventprop}} in spiking neural networks,''
  \emph{Neuromorphic Computing and Engineering}, vol.~5, no.~1, p. 014001, Jan.
  2025.

\bibitem{wunderlichEventbasedBackpropagationCan2021}
T.~C. Wunderlich and C.~Pehle, ``Event-based backpropagation can compute exact
  gradients for spiking neural networks,'' \emph{Scientific Reports}, vol.~11,
  no.~1, p. 12829, Jun. 2021.

\bibitem{shoesmithEventpropTrainingEfficient2025}
T.~Shoesmith, J.~C. Knight, B.~M{\'e}sz{\'a}ros, J.~Timcheck, and T.~Nowotny,
  ``Eventprop training for efficient neuromorphic applications,'' Mar. 2025.

\bibitem{SpikingHeidelbergDigits}
``Spiking {{Heidelberg Digits}} and {{Spiking Speech Commands}} -- {{Zenke
  Lab}}.''

\bibitem{PapersCodeSSC}
``Papers with {{Code}} - {{SSC Dataset}},''
  https://paperswithcode.com/dataset/ssc.

\bibitem{garofoloTIMITAcousticPhoneticContinuous1993}
J.~S. Garofolo, L.~F. Lamel, W.~M. Fisher, D.~S. Pallett, N.~L. Dahlgren,
  V.~Zue, and J.~G. Fiscus, ``{{TIMIT Acoustic-Phonetic Continuous Speech
  Corpus}},'' Jan. 1993.

\bibitem{panayotovLibrispeechASRCorpus2015}
V.~Panayotov, G.~Chen, D.~Povey, and S.~Khudanpur, ``Librispeech: {{An ASR}}
  corpus based on public domain audio books,'' in \emph{2015 {{IEEE
  International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal
  Processing}} ({{ICASSP}})}, Apr. 2015, pp. 5206--5210.

\bibitem{r.garyleonardTIDIGITS1993}
{R. Gary Leonard} and {Doddington, George R.}, ``{{TIDIGITS}},'' p. 1597000 KB,
  1993.

\bibitem{savareseLolemacsPytorcheventprop2024}
P.~Savarese, ``Lolemacs/pytorch-eventprop,'' Oct. 2024.

\bibitem{nowotnyTnowotnyGenn_eventprop2025}
T.~Nowotny, ``Tnowotny/genn\_eventprop,'' Jan. 2025.

\bibitem{zehraComparativeAnalysisPython2020}
F.~Zehra, M.~Javed, D.~Khan, and M.~Pasha, ``Comparative {{Analysis}} of
  {{C}}++ and {{Python}} in {{Terms}} of {{Memory}} and {{Tim}},'' 2020.

\end{thebibliography}
