
Literature discussing different parameters to adjust: firing threshold [@wangLTMDLearningImprovement2022], membrane time constant [@fangIncorporatingLearnableMembrane2021a], weights [@rathiDIETSNNLowLatencySpiking2023], activation function between charging and firing [@jiangKLIFOptimizedSpiking2023].

---

Adjustable firing thresholds, which determine when a neuron emits a spike, have been shown to enhance the expressivity of deeper SNNs by allowing each neuron to adapt its firing sensitivity during training. Wang et al. introduced a learnable thresholding mechanism (LTMD) combined with moderate dropout, integrating threshold VthV_{\rm th}Vth​ into the gradient‐based optimization alongside synaptic weights, and demonstrated faster convergence and improved accuracy on spatio‐temporal benchmarks compared to fixed‑threshold networks [NeurIPS Papers](https://papers.neurips.cc/paper_files/paper/2022/file/b5fd95d6b16d3172e307103a97f19e1b-Paper-Conference.pdf?utm_source=chatgpt.com)[NeurIPS Proceedings](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b5fd95d6b16d3172e307103a97f19e1b-Abstract-Conference.html?utm_source=chatgpt.com).

Beyond thresholds, the membrane time constant τm\tau_mτm​, which balances integration of incoming currents against leak, can also be learned. Fang et al. proposed the Parametric LIF (PLIF) model, where distinct τm\tau_mτm​ values per layer are treated as trainable parameters; this heterogeneity better matches biologically observed dynamics, speeds up learning, and reduces sensitivity to initialization [CVF Open Access](https://openaccess.thecvf.com/content/ICCV2021/papers/Fang_Incorporating_Learnable_Membrane_Time_Constant_To_Enhance_Learning_of_Spiking_ICCV_2021_paper.pdf?utm_source=chatgpt.com)[arXiv](https://arxiv.org/abs/2007.05785?utm_source=chatgpt.com).

Synaptic weights www remain fundamental to SNN training, but when optimized jointly with neuron parameters, they yield low‑latency, high‑sparsity networks. Rathi & Roy’s DIET‑SNN framework directly encodes analog inputs and uses backpropagation to adjust both www and neuron properties (membrane leak and threshold), achieving competitive classification accuracy on CIFAR/ImageNet with as few as five timesteps and substantial efficiency gains [PubMed](https://pubmed.ncbi.nlm.nih.gov/34596559/?utm_source=chatgpt.com)[arXiv](https://arxiv.org/abs/2008.03658?utm_source=chatgpt.com).

Finally, surrogate‑gradient methods rely on a smooth approximation of the spike generation nonlinearity; Jiang & Zhang’s KLIF neuron introduces a learnable scaling factor kkk that dynamically tunes the slope and width of the surrogate gradient near threshold, along with a ReLU‑style gating between integration and firing. This adaptive activation function enhances gradient flow without extra computational cost and delivers state‑of‑the‑art performance on both static and neuromorphic datasets [arXiv](https://arxiv.org/abs/2302.09238?utm_source=chatgpt.com)[MIT Press Direct](https://direct.mit.edu/neco/article/36/12/2636/124535/KLIF-An-Optimized-Spiking-Neuron-Unit-for-Tuning?utm_source=chatgpt.com)[Frontiers](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1383844/full?utm_source=chatgpt.com).

Collectively, these studies illustrate that making intrinsic neuron parameters trainable—thresholds, time constants, synaptic leaks, and surrogate activation shapes—enables more flexible, robust, and efficient SNN training, leading to faster convergence and higher accuracy in deep spiking architectures [Frontiers](https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1383844/full?utm_source=chatgpt.com).