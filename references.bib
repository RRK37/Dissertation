@article{arnaudyargaAcceleratingSpikingNeural2025,
  title = {Accelerating Spiking Neural Networks with Parallelizable Leaky Integrate-and-Fire Neurons*},
  author = {Arnaud Yarga, Sidi Yaya and Wood, Sean U N},
  date = {2025-03},
  journaltitle = {Neuromorphic Computing and Engineering},
  shortjournal = {Neuromorph. Comput. Eng.},
  volume = {5},
  number = {1},
  pages = {014012},
  publisher = {IOP Publishing},
  issn = {2634-4386},
  doi = {10.1088/2634-4386/adb7fe},
  url = {https://dx.doi.org/10.1088/2634-4386/adb7fe},
  urldate = {2025-03-24},
  abstract = {Spiking neural networks (SNNs) express higher biological plausibility and excel at learning spatiotemporal features while consuming less energy than conventional artificial neural networks, particularly on neuromorphic hardware. The leaky integrate-and-fire (LIF) neuron stands out as one of the most widely used spiking neurons in deep learning. However, its sequential information processing leads to slow training on lengthy sequences, presenting a critical challenge for real-world applications that rely on extensive datasets. This paper introduces the parallelizable LIF (ParaLIF) neuron, which accelerates SNNs by parallelizing their simulation over time, for both feedforward and recurrent architectures. Compared to LIF in neuromorphic speech, image and gesture classification tasks, ParaLIF demonstrates speeds up to 200 times faster and, on average, achieves greater accuracy with similar sparsity. When integrated into state-of-the-art architectures, ParaLIF’s accuracy matches or exceeds the highest performance reported in the literature on various neuromorphic datasets. These findings highlight ParaLIF as a promising approach for the development of rapid, accurate and energy-efficient SNNs, particularly well-suited for handling massive datasets containing long sequences.},
  langid = {english},
  keywords = {snn-speech},
  file = {/home/pomme/Zotero/storage/KYM8M6PN/Arnaud Yarga and Wood - 2025 - Accelerating spiking neural networks with parallelizable leaky integrate-and-fire neurons.pdf}
}

@software{attarAidinattarSnn2025,
  title = {Aidinattar/Snn},
  author = {Attar, Aidin},
  date = {2025-02-09T04:17:37Z},
  origdate = {2024-05-05T22:33:55Z},
  url = {https://github.com/aidinattar/snn},
  urldate = {2025-03-13},
  abstract = {Implementation of Spiking Neural Networks (SNNs) using SpykeTorch, featuring STDP and R-STDP training methods for efficient neural computation.},
  keywords = {artificial-intelligence,deep-learning,neural-network,pytorch,snn,software,spyketorch,stdp}
}

@article{augeSurveyEncodingTechniques2021,
  title = {A {{Survey}} of {{Encoding Techniques}} for {{Signal Processing}} in {{Spiking Neural Networks}}},
  author = {Auge, Daniel and Hille, Julian and Mueller, Etienne and Knoll, Alois},
  date = {2021-12-01},
  journaltitle = {Neural Processing Letters},
  shortjournal = {Neural Process Lett},
  volume = {53},
  number = {6},
  pages = {4693--4710},
  issn = {1573-773X},
  doi = {10.1007/s11063-021-10562-2},
  url = {https://doi.org/10.1007/s11063-021-10562-2},
  urldate = {2025-03-12},
  abstract = {Biologically inspired spiking neural networks are increasingly popular in the field of artificial intelligence due to their ability to solve complex problems while being power efficient. They do so by leveraging the timing of discrete spikes as main information carrier. Though, industrial applications are still lacking, partially because the question of how to encode incoming data into discrete spike events cannot be uniformly answered. In this paper, we summarise the signal encoding schemes presented in the literature and propose a uniform nomenclature to prevent the vague usage of ambiguous definitions. Therefore we survey both, the theoretical foundations as well as applications of the encoding schemes. This work provides a foundation in spiking signal encoding and gives an overview over different application-oriented implementations which utilise the schemes.},
  langid = {english},
  keywords = {article,Artificial Intelligence,Neural coding,Neuromorphic computing,Rate coding,Spiking neural networks,Temporal coding}
}

@article{bashirCognitiveMachinesEvaluating2025,
  title = {Toward {{Cognitive Machines}}: {{Evaluating Single Device Based Spiking Neural Networks}} for {{Brain-Inspired Computing}}},
  shorttitle = {Toward {{Cognitive Machines}}},
  author = {Bashir, Faisal and Alzahrani, Ali and Abbas, Haider and Zahoor, Furqan},
  date = {2025-02-25},
  journaltitle = {ACS Applied Electronic Materials},
  shortjournal = {ACS Appl. Electron. Mater.},
  volume = {7},
  number = {4},
  pages = {1329--1341},
  publisher = {American Chemical Society},
  doi = {10.1021/acsaelm.4c02015},
  url = {https://doi.org/10.1021/acsaelm.4c02015},
  urldate = {2025-03-11},
  abstract = {A brain-inspired computing paradigm known as “neuromorphic computing” seeks to replicate the information processing processes of biological neural systems in order to create computing systems that are effective, low-power, and adaptable. Spiking neural networks (SNNs) based on a single device are at the forefront of brain-inspired computing, which aims to mimic the processing powers of the human brain. Neuromorphic devices, which enable the hardware implementation of artificial neural networks (ANNs), are at the heart of neuromorphic computing. These devices replicate the dynamics and functions of neurons and synapses. This mini-review assesses the latest advancements in neuromorphic computing, with an emphasis on small, energy-efficient devices that mimic biological synapses and neurons. Key neuromorphic functions like spike-timing-dependent plasticity, multistate storage, and dynamic filtering are demonstrated by a variety of single-device models, such as memristors, transistors, and magnetic and ferroelectric devices. The integrate-and-fire (IF) neuron is a key model in these systems because it allows for mathematical analysis while successfully capturing key aspects of neural processing. This review examines the potential of SNNs for scalable, low-power neuromorphic computing applications, highlighting both the benefits and constraints of implementing them with single-device architectures. This review highlights the increasing importance of single-device SNNs in the creation of effective, flexible cognitive devices.},
  keywords = {article}
}

@article{bellecBiologicallyInspiredAlternatives2019,
  title = {Biologically Inspired Alternatives to Backpropagation through Time for Learning in Recurrent Neural Nets},
  author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  date = {2019-02-21},
  journaltitle = {arXiv.org},
  publisher = {Cornell University Library arXiv.org},
  location = {Ithaca, United States},
  url = {https://www.proquest.com/publiccontent/docview/2172488501?pq-origsite=primo&sourcetype=Working%20Papers},
  urldate = {2025-03-20},
  abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
  langid = {english},
  keywords = {Algorithms,article,Back propagation networks,Brain,Computation,Data processing,Error signals,Hardware,Machine learning,Neural networks,Neurons,Recurrent neural networks,Spiking,Synapses}
}

@article{caoSpikingDeepConvolutional2015,
  title = {Spiking {{Deep Convolutional Neural Networks}} for {{Energy-Efficient Object Recognition}}},
  author = {Cao, Yongqiang and Chen, Yang and Khosla, Deepak},
  date = {2015-05-01},
  journaltitle = {International Journal of Computer Vision},
  shortjournal = {Int J Comput Vis},
  volume = {113},
  number = {1},
  pages = {54--66},
  issn = {1573-1405},
  doi = {10.1007/s11263-014-0788-3},
  url = {https://doi.org/10.1007/s11263-014-0788-3},
  urldate = {2025-03-12},
  abstract = {Deep-learning neural networks such as convolutional neural network (CNN) have shown great potential as a solution for difficult vision problems, such as object recognition. Spiking neural networks (SNN)-based architectures have shown great potential as a solution for realizing ultra-low power consumption using spike-based neuromorphic hardware. This work describes a novel approach for converting a deep CNN into a SNN that enables mapping CNN to spike-based hardware architectures. Our approach first tailors the CNN architecture to fit the requirements of SNN, then trains the tailored CNN in the same way as one would with CNN, and finally applies the learned network weights to an SNN architecture derived from the tailored CNN. We evaluate the resulting SNN on publicly available Defense Advanced Research Projects Agency (DARPA) Neovision2 Tower and CIFAR-10 datasets and show similar object recognition accuracy as the original CNN. Our SNN implementation is amenable to direct mapping to spike-based neuromorphic hardware, such as the ones being developed under the DARPA SyNAPSE program. Our hardware mapping analysis suggests that SNN implementation on such spike-based hardware is two orders of magnitude more energy-efficient than the original CNN implementation on off-the-shelf FPGA-based hardware.},
  langid = {english},
  keywords = {article,Artificial Intelligence,Convolutional neural networks,Deep learning,Machine learning,Neuromorphic circuits,Object recognition,Spiking neural networks}
}

@inproceedings{fangIncorporatingLearnableMembrane2021,
  title = {Incorporating {{Learnable Membrane Time Constant To Enhance Learning}} of {{Spiking Neural Networks}}},
  author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timothée and Huang, Tiejun and Tian, Yonghong},
  date = {2021},
  pages = {2661--2671},
  url = {https://openaccess.thecvf.com/content/ICCV2021/html/Fang_Incorporating_Learnable_Membrane_Time_Constant_To_Enhance_Learning_of_Spiking_ICCV_2021_paper.html},
  urldate = {2025-03-27},
  eventtitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  langid = {english},
  keywords = {timeconstant},
  file = {/home/pomme/Zotero/storage/3B8HHKKS/Fang et al. - 2021 - Incorporating Learnable Membrane Time Constant To Enhance Learning of Spiking Neural Networks.pdf}
}

@inproceedings{fangIncorporatingLearnableMembrane2021a,
  title = {Incorporating {{Learnable Membrane Time Constant}} to {{Enhance Learning}} of {{Spiking Neural Networks}}},
  author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timothée and Huang, Tiejun and Tian, Yonghong},
  date = {2021-10-01},
  pages = {2641--2651},
  publisher = {IEEE Computer Society},
  doi = {10.1109/ICCV48922.2021.00266},
  url = {https://www.computer.org/csdl/proceedings-article/iccv/2021/281200c641/1BmEpXS0r1m},
  urldate = {2025-03-27},
  abstract = {Spiking Neural Networks (SNNs) have attracted enormous research interest due to temporal information processing capability, low power consumption, and high biological plausibility. However, the formulation of efficient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spiking neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neurons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain regions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the membrane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning. In addition, we reevaluate the pooling methods in SNNs and find that max-pooling will not lead to significant information loss and have the advantage of low computation cost and binary compatibility. We evaluate the proposed method for image classification tasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment results show that the proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.},
  eventtitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  isbn = {978-1-6654-2812-5},
  langid = {english},
  file = {/home/pomme/Zotero/storage/ZJF93NK4/Fang et al. - 2021 - Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks.pdf}
}

@article{fedorovaAdvancingNeuralNetworks2024,
  title = {Advancing {{Neural Networks}}: {{Innovations}} and {{Impacts}} on {{Energy Consumption}}},
  shorttitle = {Advancing {{Neural Networks}}},
  author = {Fedorova, Alina and Jovišić, Nikola and Vallverdù, Jordi and Battistoni, Silvia and Jovičić, Miloš and Medojević, Milovan and Toschev, Alexander and Alshanskaia, Evgeniia and Talanov, Max and Erokhin, Victor},
  date = {2024},
  journaltitle = {Advanced Electronic Materials},
  volume = {10},
  number = {12},
  pages = {2400258},
  issn = {2199-160X},
  doi = {10.1002/aelm.202400258},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/aelm.202400258},
  urldate = {2025-03-12},
  abstract = {The energy efficiency of Artificial Intelligence (AI) systems is a crucial and actual issue that may have an important impact on an ecological, economic and technological level. Spiking Neural Networks (SNNs) are strongly suggested as valid candidates able to overcome Artificial Neural Networks (ANNs) in this specific contest. In this study, the proposal involves the review and comparison of energy consumption of the popular Artificial Neural Network architectures implemented on the CPU and GPU hardware compared with Spiking Neural Networks implemented in specialized memristive hardware and biological neural network human brain. As a result, the energy efficiency of Spiking Neural Networks can be indicated from 5 to 8 orders of magnitude. Some Spiking Neural Networks solutions are proposed including continuous feedback-driven self-learning approaches inspired by biological Spiking Neural Networks as well as pure memristive solutions for Spiking Neural Networks.},
  langid = {english},
  keywords = {article,artificial neural network,energy consumption,LSTM,memristive device,ResNet,spiking neural network,transformer}
}

@article{florescuLearningPreciseSpike2019,
  title = {Learning with {{Precise Spike Times}}: {{A New Decoding Algorithm}} for {{Liquid State Machines}}},
  shorttitle = {Learning with {{Precise Spike Times}}},
  author = {Florescu, Dorian and Coca, Daniel},
  date = {2019-09-01},
  journaltitle = {Neural Computation},
  shortjournal = {Neural Computation},
  volume = {31},
  number = {9},
  pages = {1825--1852},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01218},
  url = {https://doi.org/10.1162/neco_a_01218},
  urldate = {2025-03-12},
  abstract = {There is extensive evidence that biological neural networks encode information in the precise timing of the spikes generated and transmitted by neurons, which offers several advantages over rate-based codes. Here we adopt a vector space formulation of spike train sequences and introduce a new liquid state machine (LSM) network architecture and a new forward orthogonal regression algorithm to learn an input-output signal mapping or to decode the brain activity. The proposed algorithm uses precise spike timing to select the presynaptic neurons relevant to each learning task. We show that using precise spike timing to train the LSM and selecting the readout presynaptic neurons leads to a significant increase in performance on binary classification tasks, in decoding neural activity from multielectrode array recordings, as well as in a speech recognition task, compared with what is achieved using the standard architecture and training methods.},
  keywords = {article,audio,LSM,speech}
}

@software{goupyGgoupySpikingConvNet2025,
  title = {Ggoupy/{{SpikingConvNet}}},
  author = {Goupy, Gaspard},
  date = {2025-03-06T20:27:52Z},
  origdate = {2022-05-14T22:56:24Z},
  url = {https://github.com/ggoupy/SpikingConvNet},
  urldate = {2025-03-13},
  abstract = {Convolutional spiking neural network implementing STDP},
  keywords = {mnist-classification,software,spiking-neural-networks,stdp,synaptic-plasticity,unsupervised-learning}
}

@article{guoAntidamageAbilityBiological2025,
  title = {Anti-Damage Ability of Biological Plausible Spiking Neural Network with Synaptic Time Delay Based on Speech Recognition under Random Attack},
  author = {Guo, Lei and Ding, Weihang and Wu, Youxi and Man, Menghua and Guo, Miaomiao},
  date = {2025-03-15},
  journaltitle = {Engineering Applications of Artificial Intelligence},
  shortjournal = {Engineering Applications of Artificial Intelligence},
  volume = {144},
  pages = {110061},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2025.110061},
  url = {https://www.sciencedirect.com/science/article/pii/S0952197625000612},
  urldate = {2025-03-24},
  abstract = {The complex electromagnetic environment can reduce the performance of neuromorphic hardware. The robustness of a brain-inspired model is beneficial for maintaining its performance under exterior attack. However, the synaptic plasticity in a brain-inspired model still lacks biological plausibility. The objective of this paper is to enhance the anti-damage ability of brain-inspired model under random attack by improving the biological plausibility of its synaptic plasticity. In this study, we propose a new spiking neural network (SNN) as a brain-inspired model, in which the topology is constrained by functional magnetic resonance imaging (fMRI) data on the human brain, its nodes are Izhikevich neuron models, and its edges are synaptic plasticity models with a random time delay that conforms to the range of the biological synaptic time delay (STD), and called it as fMRI-SNN with synaptic random time delay (SRTD). Then, taking speech recognition (SR) as a case study, we certify the recognition performance of fMRI-SNN with SRTD. To evaluate the anti-damage ability of fMRI-SNN with SRTD, we compare its SR accuracy before and after random attack. To elucidate the anti-damage mechanism, we discuss the neuroelectric characteristics, adaptive regulation of synaptic plasticity, and dynamic topological characteristics of fMRI-SNN with SRTD under random attack. The results indicate that our approach enhances the anti-damage ability of the brain-inspired model, and our discussion elucidates its anti-damage mechanism. Our results prompt that the brain-inspired model with biological plausibility can enhance its information processing ability.},
  keywords = {Anti-damage ability,Functional magnetic resonance imaging,snn-speech,Speech recognition,Spiking neural network,Synaptic time delay},
  file = {/home/pomme/Zotero/storage/FJ8SI82L/S0952197625000612.html}
}

@article{hamianNovelTrainingApproach2024,
  title = {A {{Novel Training Approach}} in {{Deep Spiking Neural Network Based}} on {{Fuzzy Weighting}} and {{Meta-heuristic Algorithm}}},
  author = {Hamian, Melika and Faez, Karim and Nazari, Soheila and Sabeti, Malihe},
  date = {2024-02-19},
  journaltitle = {International Journal of Computational Intelligence Systems},
  shortjournal = {Int J Comput Intell Syst},
  volume = {17},
  number = {1},
  pages = {35},
  issn = {1875-6883},
  doi = {10.1007/s44196-024-00425-8},
  url = {https://doi.org/10.1007/s44196-024-00425-8},
  urldate = {2025-03-24},
  abstract = {The challenge of supervised learning in spiking neural networks (SNNs) for digit classification from speech signals is examined in this study. Meta-heuristic algorithms and a fuzzy logic framework are used to train SNNs. Using gray wolf optimization (GWO), the features obtained from audio signals are reduced depending on the dispersion of each feature. Then, it combines fuzzy weighting system (FWS) and spike time-dependent flexibility (STDP) approach to implement the learning rule in SNN. The FWS rule produces a uniformly distributed random weight in the STDP flexibility window, so that the system requires fewer training parameters. Finally, these neurons are fed data to estimate the training weights and threshold values of the neurons using wild horse algorithm (WHO). With the parameters given, these rule weights are applied to appropriately display the class's share in extracting the relevant feature. The suggested network can classify speech signals into categories with 97.17\% accuracy. The dataset was obtained using neurons operating at sparse biological rates below 600~Hz in the TIDIGITS test database. The suggested method has been evaluated on the IRIS and Trip Data datasets, where the classification results showed a 98.93\% and 97.36\% efficiency, respectively. Compared to earlier efforts, this study's results demonstrate that the strategy is both computationally simpler and more accurate. The accuracy of classification of digits, IRIS and Trip Data has increased by 4.9, 3.46 and 1.24\%, respectively. The principal goal of this research is to improve the accuracy of SNN by developing a new high-precision training method.},
  langid = {english},
  keywords = {Artificial Intelligence,Digit recognition system,Fuzzy weighting system (FWS),Gray wolf optimization (GWO),snn-speech,Spiking neural network (SNN),Wild horse algorithm (WHO)},
  file = {/home/pomme/Zotero/storage/VSUL6ERU/Hamian et al. - 2024 - A Novel Training Approach in Deep Spiking Neural Network Based on Fuzzy Weighting and Meta-heuristic.pdf}
}

@online{HttpsApiresearchrepositoryuwaeduauWs,
  title = {{{https://api.research-repository.uwa.edu.au/ws/portalfiles/portal/187295294/2109.12894v4.pdf}}},
  url = {https://api.research-repository.uwa.edu.au/ws/portalfiles/portal/187295294/2109.12894v4.pdf},
  urldate = {2025-03-13}
}

@online{HttpsArxivorgPdf,
  title = {{{https://arxiv.org/pdf/1901.09049}}},
  url = {https://arxiv.org/pdf/1901.09049},
  urldate = {2025-03-13}
}

@software{IGITUGrazLSM2025,
  title = {{{IGITUGraz}}/{{LSM}}},
  date = {2025-01-17T14:32:06Z},
  origdate = {2017-08-24T19:45:00Z},
  url = {https://github.com/IGITUGraz/LSM},
  urldate = {2025-03-13},
  abstract = {Liquid State Machines in Python and NEST},
  organization = {Institute for Theoretical Computer Science, TU Graz},
  keywords = {liquid-state-machine,lsm,recurrent-neural-networks,software,spiking-neural-networks}
}

@online{jiangKLIFOptimizedSpiking2023,
  title = {{{KLIF}}: {{An}} Optimized Spiking Neuron Unit for Tuning Surrogate Gradient Slope and Membrane Potential},
  shorttitle = {{{KLIF}}},
  author = {Jiang, Chunming and Zhang, Yilei},
  date = {2023-02-18},
  eprint = {2302.09238},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2302.09238},
  url = {http://arxiv.org/abs/2302.09238},
  urldate = {2025-03-27},
  abstract = {Spiking neural networks (SNNs) have attracted much attention due to their ability to process temporal information, low power consumption, and higher biological plausibility. However, it is still challenging to develop efficient and high-performing learning algorithms for SNNs. Methods like artificial neural network (ANN)-to-SNN conversion can transform ANNs to SNNs with slight performance loss, but it needs a long simulation to approximate the rate coding. Directly training SNN by spike-based backpropagation (BP) such as surrogate gradient approximation is more flexible. Yet now, the performance of SNNs is not competitive compared with ANNs. In this paper, we propose a novel k-based leaky Integrate-and-Fire (KLIF) neuron model to improve the learning ability of SNNs. Compared with the popular leaky integrate-and-fire (LIF) model, KLIF adds a learnable scaling factor to dynamically update the slope and width of the surrogate gradient curve during training and incorporates a ReLU activation function that selectively delivers membrane potential to spike firing and resetting. The proposed spiking unit is evaluated on both static MNIST, Fashion-MNIST, CIFAR-10 datasets, as well as neuromorphic N-MNIST, CIFAR10-DVS, and DVS128-Gesture datasets. Experiments indicate that KLIF performs much better than LIF without introducing additional computational cost and achieves state-of-the-art performance on these datasets with few time steps. Also, KLIF is believed to be more biological plausible than LIF. The good performance of KLIF can make it completely replace the role of LIF in SNN for various tasks.},
  pubstate = {prepublished},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,KLIF},
  file = {/home/pomme/Zotero/storage/PT5E8UXI/Jiang and Zhang - 2023 - KLIF An optimized spiking neuron unit for tuning surrogate gradient slope and membrane potential.pdf;/home/pomme/Zotero/storage/FJTHCHY5/2302.html}
}

@online{kindigAIPowerConsumption,
  title = {{{AI Power Consumption}}: {{Rapidly Becoming Mission-Critical}}},
  shorttitle = {{{AI Power Consumption}}},
  author = {Kindig, Beth},
  url = {https://www.forbes.com/sites/bethkindig/2024/06/20/ai-power-consumption-rapidly-becoming-mission-critical/},
  urldate = {2025-03-12},
  abstract = {Generative AI and rising GPU shipments is pushing data centers to scale to 100,000-plus accelerators, putting emphasis on power as a mission-critical problem to solve.},
  langid = {english},
  organization = {Forbes},
  keywords = {news}
}

@article{maassComputationalPowerCircuits2004,
  title = {On the Computational Power of Circuits of Spiking Neurons},
  author = {Maass, Wolfgang and Markram, Henry},
  date = {2004-12-01},
  journaltitle = {Journal of Computer and System Sciences},
  shortjournal = {Journal of Computer and System Sciences},
  volume = {69},
  number = {4},
  pages = {593--616},
  issn = {0022-0000},
  doi = {10.1016/j.jcss.2004.04.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0022000004000406},
  urldate = {2025-03-27},
  abstract = {Complex real-time computations on multi-modal time-varying input streams are carried out by generic cortical microcircuits. Obstacles for the development of adequate theoretical models that could explain the seemingly universal power of cortical microcircuits for real-time computing are the complexity and diversity of their computational units (neurons and synapses), as well as the traditional emphasis on offline computing in almost all theoretical approaches towards neural computation. In this article, we initiate a rigorous mathematical analysis of the real-time computing capabilities of a new generation of models for neural computation, liquid state machines, that can be implemented with—in fact benefit from—diverse computational units. Hence, realistic models for cortical microcircuits represent special instances of such liquid state machines, without any need to simplify or homogenize their diverse computational units. We present proofs of two theorems about the potential computational power of such models for real-time computing, both on analog input streams and for spike trains as inputs.},
  file = {/home/pomme/Zotero/storage/GU6MUVRD/Maass and Markram - 2004 - On the computational power of circuits of spiking neurons.pdf;/home/pomme/Zotero/storage/7IVNTCM2/S0022000004000406.html}
}

@article{maassNetworksSpikingNeurons1997,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  shorttitle = {Networks of Spiking Neurons},
  author = {Maass, Wolfgang},
  date = {1997-12-01},
  journaltitle = {Neural Networks},
  shortjournal = {Neural Networks},
  volume = {10},
  number = {9},
  pages = {1659--1671},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(97)00011-7},
  url = {https://www.sciencedirect.com/science/article/pii/S0893608097000117},
  urldate = {2025-03-26},
  abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
  keywords = {Computational complexity,Integrate-and-fire neutron,Lower bounds,Sigmoidal neural nets,Spiking neuron,third-generation},
  file = {/home/pomme/Zotero/storage/U6IUTDR5/S0893608097000117.html}
}

@article{merollaMillionSpikingneuronIntegrated2014,
  title = {A Million Spiking-Neuron Integrated Circuit with a Scalable Communication Network and Interface},
  author = {Merolla, Paul A. and Arthur, John V. and Alvarez-Icaza, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
  date = {2014},
  journaltitle = {Science},
  volume = {345},
  number = {6197},
  eprint = {24745271},
  eprinttype = {jstor},
  pages = {668--673},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  url = {https://www.jstor.org/stable/24745271},
  urldate = {2025-03-27},
  abstract = {Inspired by the brain's structure, we have developed an efficient, scalable, and flexible non–von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
  keywords = {TrueNorth},
  file = {/home/pomme/Zotero/storage/AGBUEKT2/Merolla et al. - 2014 - A million spiking-neuron integrated circuit with a scalable communication network and interface.pdf}
}

@article{nazariAccurateFastLearning2025,
  title = {An Accurate and Fast Learning Approach in the Biologically Spiking Neural Network},
  author = {Nazari, Soheila and Amiri, Masoud},
  date = {2025-02-24},
  journaltitle = {Scientific Reports},
  shortjournal = {Sci Rep},
  volume = {15},
  number = {1},
  pages = {6585},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-90113-0},
  url = {https://www.nature.com/articles/s41598-025-90113-0},
  urldate = {2025-03-23},
  abstract = {Computations adapted from the interactions of neurons in the nervous system have the potential to be a strong foundation for building computers with cognitive functions including decision-making, generalization, and real-time learning. In this context, a proposed intelligent machine is built on nervous system mechanisms. As a result, the output and middle layer of the machine is made up of a population of pyramidal neurons and interneurons, AMPA/GABA receptors, and excitatory and inhibitory neurotransmitters. The input layer of the machine is derived from the retinal model. A machine with a structure appropriate to biological evidence needs to learn based on biological evidence. Similar to this, the PSAC (Power-STDP Actor-Critic) learning algorithm was developed as a new learning mechanism based on unsupervised and reinforcement learning procedure. Four datasets MNIST, EMNIST, CIFAR10, and CIFAR100 were used to confirm the performance of the proposed learning algorithm compared to deep and spiking networks, and respectively accuracies of 97.7\%, 97.95\% (digits) and 93.73\% (letters), 93.6\%, and 75\% have been obtained, which shows an improvement in accuracy compared to previous spiking networks. The suggested learning strategy not only outperforms the earlier spike-based learning techniques in terms of accuracy but also exhibits a faster rate of convergence throughout the training phase.},
  langid = {english},
  keywords = {article,Cognitive neuroscience,Computational neuroscience,Synaptic plasticity},
  file = {/home/pomme/Zotero/storage/7QKEWH65/Nazari and Amiri - 2025 - An accurate and fast learning approach in the biologically spiking neural network.pdf}
}

@article{nowotnyLossShapingEnhances2025,
  title = {Loss Shaping Enhances Exact Gradient Learning with {{Eventprop}} in Spiking Neural Networks},
  author = {Nowotny, Thomas and Turner, James P and Knight, James C},
  date = {2025-01},
  journaltitle = {Neuromorphic Computing and Engineering},
  shortjournal = {Neuromorph. Comput. Eng.},
  volume = {5},
  number = {1},
  pages = {014001},
  publisher = {IOP Publishing},
  issn = {2634-4386},
  doi = {10.1088/2634-4386/ada852},
  url = {https://dx.doi.org/10.1088/2634-4386/ada852},
  urldate = {2025-03-24},
  abstract = {Event-based machine learning promises more energy-efficient AI on future neuromorphic hardware. Here, we investigate how the recently discovered Eventprop algorithm for gradient descent on exact gradients in spiking neural networks (SNNs) can be scaled up to challenging keyword recognition benchmarks. We implemented Eventprop in the GPU-enhanced neural networks framework (GeNN) and used it for training recurrent SNNs on the Spiking Heidelberg Digits (SHD) and Spiking Speech Commands (SSC) datasets. We found that learning depended strongly on the loss function and extended Eventprop to a wider class of loss functions to enable effective training. We then tested a large number of data augmentations and regularisations as well as exploring different network structures; and heterogeneous and trainable timescales. We found that when combined with two specific augmentations, the right regularisation and a delay line input, Eventprop networks with one recurrent layer achieved state-of-the-art performance on SHD and good accuracy on SSC. In comparison to a leading surrogate-gradient-based SNN training method, our GeNN Eventprop implementation is 3× faster and uses 4× less memory. This work is a significant step towards a low-power neuromorphic alternative to current machine learning paradigms.},
  langid = {english},
  keywords = {snn-speech},
  file = {/home/pomme/Zotero/storage/4BH959B9/Nowotny et al. - 2025 - Loss shaping enhances exact gradient learning with Eventprop in spiking neural networks.pdf}
}

@article{rathiDIETSNNLowLatencySpiking2023,
  title = {{{DIET-SNN}}: {{A Low-Latency Spiking Neural Network With Direct Input Encoding}} and {{Leakage}} and {{Threshold Optimization}}},
  shorttitle = {{{DIET-SNN}}},
  author = {Rathi, Nitin and Roy, Kaushik},
  date = {2023-06},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {34},
  number = {6},
  pages = {3174--3182},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3111897},
  url = {https://ieeexplore.ieee.org/abstract/document/9556508},
  urldate = {2025-03-27},
  abstract = {Bioinspired spiking neural networks (SNNs), operating with asynchronous binary signals (or spikes) distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. The state-of-the-art SNNs suffer from high inference latency, resulting from inefficient input encoding and suboptimal settings of the neuron parameters (firing threshold and membrane leak). We propose DIET-SNN, a low-latency deep spiking network trained with gradient descent to optimize the membrane leak and the firing threshold along with other network parameters (weights). The membrane leak and threshold of each layer are optimized with end-to-end backpropagation to achieve competitive accuracy at reduced latency. The input layer directly processes the analog pixel values of an image without converting it to spike train. The first convolutional layer converts analog inputs into spikes where leaky-integrate-and-fire (LIF) neurons integrate the weighted inputs and generate an output spike when the membrane potential crosses the trained firing threshold. The trained membrane leak selectively attenuates the membrane potential, which increases activation sparsity in the network. The reduced latency combined with high activation sparsity provides massive improvements in computational efficiency. We evaluate DIET-SNN on image classification tasks from CIFAR and ImageNet datasets on VGG and ResNet architectures. We achieve top-1 accuracy of 69\% with five timesteps (inference latency) on the ImageNet dataset with 12\textbackslash times less compute energy than an equivalent standard artificial neural network (ANN). In addition, DIET-SNN performs 20– 500\textbackslash times faster inference compared to other state-of-the-art SNN models.},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}} and {{Learning Systems}}},
  keywords = {Backpropagation,Backpropagation through time (BPTT),Biological neural networks,Computational modeling,convolutional neural networks,Encoding,Neurons,spiking neural networks (SNNs),supervised learning,Task analysis,Training,weights},
  file = {/home/pomme/Zotero/storage/UMC24RGZ/9556508.html}
}

@online{ReleasesGennteamGenn,
  title = {Releases · Genn-Team/Genn},
  url = {https://github.com/genn-team/genn/releases},
  urldate = {2025-03-29},
  abstract = {GeNN is a GPU-enhanced Neuronal Network simulation environment based on code generation for Nvidia CUDA. - genn-team/genn},
  langid = {english},
  organization = {GitHub},
  file = {/home/pomme/Zotero/storage/ELMG7PB2/releases.html}
}

@online{SpikingHeidelbergDigits,
  title = {Spiking {{Heidelberg Digits}} and {{Spiking Speech Commands}} – {{Zenke Lab}}},
  url = {https://zenkelab.org/resources/spiking-heidelberg-datasets-shd/},
  urldate = {2025-03-13},
  langid = {american},
  keywords = {data}
}

@online{SpikingNeuralNetworks,
  title = {Spiking Neural Networks for Physiological and Speech Signals: A Review | {{Biomedical Engineering Letters}}},
  url = {https://link.springer.com/article/10.1007/s13534-024-00404-0},
  urldate = {2025-03-24},
  keywords = {snn-speech},
  file = {/home/pomme/Zotero/storage/LYL7P59Y/s13534-024-00404-0.html}
}

@online{SpokenDigitsClassification,
  title = {Spoken {{Digits Classification Based}} on {{Spiking Neural Networks}} with {{Memristor-Based STDP}}},
  url = {https://ieeexplore.ieee.org/document/10216628/citations?tabFilter=papers#citations},
  urldate = {2025-03-24},
  abstract = {Spiking neural networks are commonly attributed to the third generation of neural networks. They mimic biological neurons more closely by processing information in the form of impulses (spikes) and are characterized by low power consumption and ease of hardware implementation. This paper shows two approaches to the task of classifying audio data represented by the spoken digits dataset using spiking neural networks with memristive plasticity. It is shown that both supervised and unsupervised learning methods based on local plasticity can be successfully used for audio classification. The models achieve accuracies ranging from 80\% to 94\% depending on the network topology, plasticity type and the way of decoding output neuronal activity. The results obtained in the paper can be a step towards creating neuromorhic devices for recognizing audio signals.},
  langid = {american},
  keywords = {snn-speech},
  file = {/home/pomme/Zotero/storage/83SYZ273/citations.html}
}

@online{StateAI2025,
  title = {The State of {{AI}} in 2025: {{Global}} Survey | {{McKinsey}}},
  url = {https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai},
  urldate = {2025-03-12},
  keywords = {news}
}

@article{sunLearnableAxonalDelay2023,
  title = {Learnable Axonal Delay in Spiking Neural Networks Improves Spoken Word Recognition},
  author = {Sun, Pengfei and Chua, Yansong and Devos, Paul and Botteldooren, Dick},
  date = {2023-11-09},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {17},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2023.1275944},
  url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2023.1275944/full},
  urldate = {2025-03-24},
  abstract = {{$<$}p{$>$}Spiking neural networks (SNNs), which are composed of biologically plausible spiking neurons, and combined with bio-physically realistic auditory periphery models, offer a means to explore and understand human auditory processing-especially in tasks where precise timing is essential. However, because of the inherent temporal complexity in spike sequences, the performance of SNNs has remained less competitive compared to artificial neural networks (ANNs). To tackle this challenge, a fundamental research topic is the configuration of spike-timing and the exploration of more intricate architectures. In this work, we demonstrate a learnable axonal delay combined with local skip-connections yields state-of-the-art performance on challenging benchmarks for spoken word recognition. Additionally, we introduce an auxiliary loss term to further enhance accuracy and stability. Experiments on the neuromorphic speech benchmark datasets, NTIDIDIGITS and SHD, show improvements in performance when incorporating our delay module in comparison to vanilla feedforward SNNs. Specifically, with the integration of our delay module, the performance on NTIDIDIGITS and SHD improves by 14\% and 18\%, respectively. When paired with local skip-connections and the auxiliary loss, our approach surpasses both recurrent and convolutional neural networks, yet uses 10 × fewer parameters for NTIDIDIGITS and 7 × fewer for SHD.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Auditory modelling,Axonal delay,neuromorphic computing,snn-speech,Speech Processing,Spiking Neural network,supervised learning},
  file = {/home/pomme/Zotero/storage/9KARM7ZU/Sun et al. - 2023 - Learnable axonal delay in spiking neural networks improves spoken word recognition.pdf}
}

@article{wangLTMDLearningImprovement2022,
  title = {{{LTMD}}: {{Learning Improvement}} of {{Spiking Neural Networks}} with {{Learnable Thresholding Neurons}} and {{Moderate Dropout}}},
  shorttitle = {{{LTMD}}},
  author = {Wang, Siqi and Cheng, Tee Hiang and Lim, Meng-Hiot},
  date = {2022-12-06},
  journaltitle = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {28350--28362},
  url = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/b5fd95d6b16d3172e307103a97f19e1b-Abstract-Conference.html},
  urldate = {2025-03-27},
  langid = {english},
  keywords = {threshold},
  file = {/home/pomme/Zotero/storage/CVTKQP82/Wang et al. - 2022 - LTMD Learning Improvement of Spiking Neural Networks with Learnable Thresholding Neurons and Modera.pdf}
}

@software{wolffCowolffSimpleSpikingNeuralNetworkSTDP2025,
  title = {Cowolff/{{Simple-Spiking-Neural-Network-STDP}}},
  author = {Wolff, Cornelius},
  date = {2025-03-04T09:21:48Z},
  origdate = {2022-05-18T15:13:44Z},
  url = {https://github.com/cowolff/Simple-Spiking-Neural-Network-STDP},
  urldate = {2025-03-13},
  abstract = {A simple from scratch implementation of a Spiking-Neural-Network with STDP in Python which is beeing trained on MNIST.},
  keywords = {ann,mnist,python,snn,snn-stdp,software}
}

@online{xiaoOnlineTrainingTime2022,
  title = {Online {{Training Through Time}} for {{Spiking Neural Networks}}},
  author = {Xiao, Mingqing and Meng, Qingyan and Zhang, Zongpeng and He, Di and Lin, Zhouchen},
  date = {2022-12-31},
  eprint = {2210.04195},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2210.04195},
  url = {http://arxiv.org/abs/2210.04195},
  urldate = {2025-03-20},
  abstract = {Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Recent progress in training methods has enabled successful deep SNNs on large-scale tasks with low latency. Particularly, backpropagation through time (BPTT) with surrogate gradients (SG) is popularly used to achieve high performance in a very small number of time steps. However, it is at the cost of large memory consumption for training, lack of theoretical clarity for optimization, and inconsistency with the online property of biological learning and rules on neuromorphic hardware. Other works connect spike representations of SNNs with equivalent artificial neural network formulation and train SNNs by gradients from equivalent mappings to ensure descent directions. But they fail to achieve low latency and are also not online. In this work, we propose online training through time (OTTT) for SNNs, which is derived from BPTT to enable forward-in-time learning by tracking presynaptic activities and leveraging instantaneous loss and gradients. Meanwhile, we theoretically analyze and prove that gradients of OTTT can provide a similar descent direction for optimization as gradients based on spike representations under both feedforward and recurrent conditions. OTTT only requires constant training memory costs agnostic to time steps, avoiding the significant memory costs of BPTT for GPU training. Furthermore, the update rule of OTTT is in the form of three-factor Hebbian learning, which could pave a path for online on-chip learning. With OTTT, it is the first time that two mainstream supervised SNN training methods, BPTT with SG and spike representation-based training, are connected, and meanwhile in a biologically plausible form. Experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR10-DVS demonstrate the superior performance of our method on large-scale static and neuromorphic datasets in small time steps.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@article{yuSTSCSNNSpatioTemporalSynaptic2022,
  title = {{{STSC-SNN}}: {{Spatio-Temporal Synaptic Connection}} with Temporal Convolution and Attention for Spiking Neural Networks},
  shorttitle = {{{STSC-SNN}}},
  author = {Yu, Chengting and Gu, Zheming and Li, Da and Wang, Gaoang and Wang, Aili and Li, Erping},
  date = {2022-12-23},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {16},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2022.1079357},
  url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2022.1079357/full},
  urldate = {2025-03-24},
  abstract = {{$<$}p{$>$}Spiking neural networks (SNNs), as one of the algorithmic models in neuromorphic computing, have gained a great deal of research attention owing to temporal information processing capability, low power consumption, and high biological plausibility. The potential to efficiently extract spatio-temporal features makes it suitable for processing event streams. However, existing synaptic structures in SNNs are almost full-connections or spatial 2D convolution, neither of which can extract temporal dependencies adequately. In this work, we take inspiration from biological synapses and propose a Spatio-Temporal Synaptic Connection SNN (STSC-SNN) model to enhance the spatio-temporal receptive fields of synaptic connections, thereby establishing temporal dependencies across layers. Specifically, we incorporate temporal convolution and attention mechanisms to implement synaptic filtering and gating functions. We show that endowing synaptic models with temporal dependencies can improve the performance of SNNs on classification tasks. In addition, we investigate the impact of performance {$<$}italic{$>$}via{$<$}/italic{$>$} varied spatial-temporal receptive fields and reevaluate the temporal modules in SNNs. Our approach is tested on neuromorphic datasets, including DVS128 Gesture (gesture recognition), N-MNIST, CIFAR10-DVS (image classification), and SHD (speech digit recognition). The results show that the proposed model outperforms the state-of-the-art accuracy on nearly all datasets.{$<$}/p{$>$}},
  langid = {english},
  keywords = {attention mechanism,Backpropagation (BP),feedforward lateral inhibition (FLI),neuromorphic recognition,snn-speech,spatio-temporal synaptic connection (STSC),spike response filter (SRF),Spiking neural network (SNN)},
  file = {/home/pomme/Zotero/storage/D4ETVKG7/Yu et al. - 2022 - STSC-SNN Spatio-Temporal Synaptic Connection with temporal convolution and attention for spiking ne.pdf}
}

@article{zhangDigitalLiquidState2015,
  title = {A {{Digital Liquid State Machine}} with {{Biologically Inspired Learning}} and {{Its Application}} to {{Speech Recognition}}},
  author = {Zhang, Y. and Li, P. and Jin, Y. and Choe, Y.},
  date = {2015},
  journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {26},
  number = {11},
  pages = {2635--2649},
  doi = {10.1109/TNNLS.2015.2388544},
  abstract = {This paper presents a bioinspired digital liquid-state machine (LSM) for low-power very-large-scale-integration (VLSI)-based machine learning applications. To the best of the authors' knowledge, this is the first work that employs a bioinspired spike-based learning algorithm for the LSM. With the proposed online learning, the LSM extracts information from input patterns on the fly without needing intermediate data storage as required in offline learning methods such as ridge regression. The proposed learning rule is local such that each synaptic weight update is based only upon the firing activities of the corresponding presynaptic and postsynaptic neurons without incurring global communications across the neural network. Compared with the backpropagation-based learning, the locality of computation in the proposed approach lends itself to efficient parallel VLSI implementation. We use subsets of the TI46 speech corpus to benchmark the bioinspired digital LSM. To reduce the complexity of the spiking neural network model without performance degradation for speech recognition, we study the impacts of synaptic models on the fading memory of the reservoir and hence the network performance. Moreover, we examine the tradeoffs between synaptic weight resolution, reservoir size, and recognition performance and present techniques to further reduce the overhead of hardware implementation. Our simulation results show that in terms of isolated word recognition evaluated using the TI46 speech corpus, the proposed digital LSM rivals the state-of-the-art hidden Markov-model-based recognizer Sphinx-4 and outperforms all other reported recognizers including the ones that are based upon the LSM or neural networks. © 2012 IEEE.},
  keywords = {article,Hardware implementation,liquid-state machine (LSM),speech recognition,spike-based learning.}
}

@article{zhouDirectTrainingHighperformance2024,
  title = {Direct Training High-Performance Deep Spiking Neural Networks: A Review of Theories and Methods},
  shorttitle = {Direct Training High-Performance Deep Spiking Neural Networks},
  author = {Zhou, Chenlin and Zhang, Han and Yu, Liutao and Ye, Yumin and Zhou, Zhaokun and Huang, Liwei and Ma, Zhengyu and Fan, Xiaopeng and Zhou, Huihui and Tian, Yonghong},
  date = {2024-07-31},
  journaltitle = {Frontiers in Neuroscience},
  shortjournal = {Front. Neurosci.},
  volume = {18},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2024.1383844},
  url = {https://www.frontiersin.org/journals/neuroscience/articles/10.3389/fnins.2024.1383844/full},
  urldate = {2025-03-11},
  abstract = {{$<$}p{$>$}Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks (ANNs), in virtue of their high biological plausibility, rich spatial-temporal dynamics, and event-driven computation. The direct training algorithms based on the surrogate gradient method provide sufficient flexibility to design novel SNN architectures and explore the spatial-temporal dynamics of SNNs. According to previous studies, the performance of models is highly dependent on their sizes. Recently, direct training deep SNNs have achieved great progress on both neuromorphic datasets and large-scale static datasets. Notably, transformer-based SNNs show comparable performance with their ANN counterparts. In this paper, we provide a new perspective to summarize the theories and methods for training deep SNNs with high performance in a systematic and comprehensive way, including theory fundamentals, spiking neuron models, advanced SNN models and residual architectures, software frameworks and neuromorphic hardware, applications, and future trends.{$<$}/p{$>$}},
  langid = {english},
  keywords = {article,Deep spiking neural network,Direct training,energy efficiency,high performance,Residual connection,Transformer-based SNNs}
}
