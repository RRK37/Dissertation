@article{adrianImpulsesProducedSensory1926,
  title = {The Impulses Produced by Sensory Nerve Endings: {{Part}} 3. {{Impulses}} Set up by {{Touch}} and {{Pressure}}},
  shorttitle = {The Impulses Produced by Sensory Nerve Endings},
  author = {Adrian, E. D. and Zotterman, Yngve},
  year = {1926},
  month = aug,
  journal = {The Journal of Physiology},
  volume = {61},
  number = {4},
  pages = {465},
  doi = {10.1113/jphysiol.1926.sp002308},
  urldate = {2025-03-31},
  langid = {english},
  pmid = {16993807},
  keywords = {ratecoding},
  file = {C:\Users\ricus\Zotero\storage\4F9IFSSW\Adrian and Zotterman - 1926 - The impulses produced by sensory nerve endings Part 3. Impulses set up by Touch and Pressure.pdf}
}

@article{arnaudyargaAcceleratingSpikingNeural2025,
  title = {Accelerating Spiking Neural Networks with Parallelizable Leaky Integrate-and-Fire Neurons*},
  author = {Arnaud Yarga, Sidi Yaya and Wood, Sean U N},
  year = {2025},
  month = mar,
  journal = {Neuromorphic Computing and Engineering},
  volume = {5},
  number = {014012},
  publisher = {IOP Publishing},
  issn = {2634-4386},
  doi = {10.1088/2634-4386/adb7fe},
  urldate = {2025-03-24},
  abstract = {Spiking neural networks (SNNs) express higher biological plausibility and excel at learning spatiotemporal features while consuming less energy than conventional artificial neural networks, particularly on neuromorphic hardware. The leaky integrate-and-fire (LIF) neuron stands out as one of the most widely used spiking neurons in deep learning. However, its sequential information processing leads to slow training on lengthy sequences, presenting a critical challenge for real-world applications that rely on extensive datasets. This paper introduces the parallelizable LIF (ParaLIF) neuron, which accelerates SNNs by parallelizing their simulation over time, for both feedforward and recurrent architectures. Compared to LIF in neuromorphic speech, image and gesture classification tasks, ParaLIF demonstrates speeds up to 200 times faster and, on average, achieves greater accuracy with similar sparsity. When integrated into state-of-the-art architectures, ParaLIF's accuracy matches or exceeds the highest performance reported in the literature on various neuromorphic datasets. These findings highlight ParaLIF as a promising approach for the development of rapid, accurate and energy-efficient SNNs, particularly well-suited for handling massive datasets containing long sequences.},
  langid = {english},
  keywords = {snn-speech},
  file = {C:\Users\ricus\Zotero\storage\KYM8M6PN\Arnaud Yarga and Wood - 2025 - Accelerating spiking neural networks with parallelizable leaky integrate-and-fire neurons.pdf}
}

@misc{attarAidinattarSnn2025,
  title = {Aidinattar/Snn},
  author = {Attar, Aidin},
  year = {2025},
  month = feb,
  urldate = {2025-03-13},
  abstract = {Implementation of Spiking Neural Networks (SNNs) using SpykeTorch, featuring STDP and R-STDP training methods for efficient neural computation.},
  copyright = {MIT},
  keywords = {artificial-intelligence,deep-learning,neural-network,pytorch,snn,software,spyketorch,stdp}
}

@article{augeSurveyEncodingTechniques2021,
  title = {A {{Survey}} of {{Encoding Techniques}} for {{Signal Processing}} in {{Spiking Neural Networks}}},
  author = {Auge, Daniel and Hille, Julian and Mueller, Etienne and Knoll, Alois},
  year = {2021},
  month = dec,
  journal = {Neural Processing Letters},
  volume = {53},
  number = {6},
  pages = {4693--4710},
  issn = {1573-773X},
  doi = {10.1007/s11063-021-10562-2},
  urldate = {2025-03-12},
  abstract = {Biologically inspired spiking neural networks are increasingly popular in the field of artificial intelligence due to their ability to solve complex problems while being power efficient. They do so by leveraging the timing of discrete spikes as main information carrier. Though, industrial applications are still lacking, partially because the question of how to encode incoming data into discrete spike events cannot be uniformly answered. In this paper, we summarise the signal encoding schemes presented in the literature and propose a uniform nomenclature to prevent the vague usage of ambiguous definitions. Therefore we survey both, the theoretical foundations as well as applications of the encoding schemes. This work provides a foundation in spiking signal encoding and gives an overview over different application-oriented implementations which utilise the schemes.},
  langid = {english},
  keywords = {article,Artificial Intelligence,Neural coding,Neuromorphic computing,Rate coding,Spiking neural networks,Temporal coding}
}

@article{bashirCognitiveMachinesEvaluating2025,
  title = {Toward {{Cognitive Machines}}: {{Evaluating Single Device Based Spiking Neural Networks}} for {{Brain-Inspired Computing}}},
  shorttitle = {Toward {{Cognitive Machines}}},
  author = {Bashir, Faisal and Alzahrani, Ali and Abbas, Haider and Zahoor, Furqan},
  year = {2025},
  month = feb,
  journal = {ACS Applied Electronic Materials},
  volume = {7},
  number = {4},
  pages = {1329--1341},
  publisher = {American Chemical Society},
  doi = {10.1021/acsaelm.4c02015},
  urldate = {2025-03-11},
  abstract = {A brain-inspired computing paradigm known as ``neuromorphic computing'' seeks to replicate the information processing processes of biological neural systems in order to create computing systems that are effective, low-power, and adaptable. Spiking neural networks (SNNs) based on a single device are at the forefront of brain-inspired computing, which aims to mimic the processing powers of the human brain. Neuromorphic devices, which enable the hardware implementation of artificial neural networks (ANNs), are at the heart of neuromorphic computing. These devices replicate the dynamics and functions of neurons and synapses. This mini-review assesses the latest advancements in neuromorphic computing, with an emphasis on small, energy-efficient devices that mimic biological synapses and neurons. Key neuromorphic functions like spike-timing-dependent plasticity, multistate storage, and dynamic filtering are demonstrated by a variety of single-device models, such as memristors, transistors, and magnetic and ferroelectric devices. The integrate-and-fire (IF) neuron is a key model in these systems because it allows for mathematical analysis while successfully capturing key aspects of neural processing. This review examines the potential of SNNs for scalable, low-power neuromorphic computing applications, highlighting both the benefits and constraints of implementing them with single-device architectures. This review highlights the increasing importance of single-device SNNs in the creation of effective, flexible cognitive devices.},
  keywords = {article}
}

@article{bellecBiologicallyInspiredAlternatives2019,
  title = {Biologically Inspired Alternatives to Backpropagation through Time for Learning in Recurrent Neural Nets},
  author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
  year = {2019},
  month = feb,
  journal = {arXiv.org},
  publisher = {Cornell University Library arXiv.org},
  address = {Ithaca, United States},
  urldate = {2025-03-20},
  abstract = {The way how recurrently connected networks of spiking neurons in the brain acquire powerful information processing capabilities through learning has remained a mystery. This lack of understanding is linked to a lack of learning algorithms for recurrent networks of spiking neurons (RSNNs) that are both functionally powerful and can be implemented by known biological mechanisms. Since RSNNs are simultaneously a primary target for implementations of brain-inspired circuits in neuromorphic hardware, this lack of algorithmic insight also hinders technological progress in that area. The gold standard for learning in recurrent neural networks in machine learning is back-propagation through time (BPTT), which implements stochastic gradient descent with regard to a given loss function. But BPTT is unrealistic from a biological perspective, since it requires a transmission of error signals backwards in time and in space, i.e., from post- to presynaptic neurons. We show that an online merging of locally available information during a computation with suitable top-down learning signals in real-time provides highly capable approximations to BPTT. For tasks where information on errors arises only late during a network computation, we enrich locally available information through feedforward eligibility traces of synapses that can easily be computed in an online manner. The resulting new generation of learning algorithms for recurrent neural networks provides a new understanding of network learning in the brain that can be tested experimentally. In addition, these algorithms provide efficient methods for on-chip training of RSNNs in neuromorphic hardware.},
  chapter = {Computer Science},
  copyright = {{\copyright} 2019. This work is published under http://arxiv.org/licenses/nonexclusive-distrib/1.0/ (the ``License'').  Notwithstanding the ProQuest Terms and Conditions, you may use this content in accordance with the terms of the License.},
  langid = {english},
  keywords = {Algorithms,article,Back propagation networks,Brain,Computation,Data processing,Error signals,Hardware,Machine learning,Neural networks,Neurons,Recurrent neural networks,Spiking,Synapses}
}

@article{bellecEligibilityTracesProvide,
  title = {Eligibility Traces Provide a Data-Inspired Alternative to Backpropagation through Time},
  author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  abstract = {Learning in recurrent neural networks (RNNs) is most often implemented by gradient descent using backpropagation through time (BPTT), but BPTT does not model accurately how the brain learns. Instead, many experimental results on synaptic plasticity can be summarized as three-factor learning rules involving eligibility traces of the local neural activity and a third factor. We present here eligibility propagation (e-prop), a new factorization of the loss gradients in RNNs that fits the framework of three factor learning rules when derived for biophysical spiking neuron models. When tested on the TIMIT speech recognition benchmark, it is competitive with BPTT both for training artificial LSTM networks and spiking RNNs. Further analysis suggests that the diversity of learning signals and the consideration of slow internal neural dynamics are decisive to the learning efficiency of e-prop.},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\PV3QFIH8\Bellec et al. - Eligibility traces provide a data-inspired alternative to backpropagation through time.pdf}
}

@inproceedings{bellecEligibilityTracesProvide2019,
  title = {Eligibility Traces Provide a Data-Inspired Alternative to Backpropagation through Time},
  booktitle = {Real {{Neurons}} \{{\textbackslash}\&\} {{Hidden Units}}: {{Future}} Directions at the Intersection of Neuroscience and Artificial Intelligence @ {{NeurIPS}} 2019},
  author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  year = {2019},
  month = oct,
  urldate = {2025-04-21},
  abstract = {Learning in recurrent neural networks (RNNs) is most often implemented by gradient descent using backpropagation through time (BPTT), but BPTT does not model accurately how the brain learns. Instead, many experimental results on synaptic plasticity can be summarized as three-factor learning rules involving eligibility traces of the local neural activity and a third factor. We present here eligibility propagation (e-prop), a new factorization of the loss gradients in RNNs that fits the framework of three factor learning rules when derived for biophysical spiking neuron models. When tested on the TIMIT speech recognition benchmark, it is competitive with BPTT both for training artificial LSTM networks and spiking RNNs. Further analysis suggests that the diversity of learning signals and the consideration of slow internal neural dynamics are decisive to the learning efficiency of e-prop.},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\7DXCPYYG\Bellec et al. - 2019 - Eligibility traces provide a data-inspired alternative to backpropagation through time.pdf}
}

@inproceedings{bellecEligibilityTracesProvide2019a,
  title = {Eligibility Traces Provide a Data-Inspired Alternative to Backpropagation through Time},
  booktitle = {Real {{Neurons}} \{{\textbackslash}\&\} {{Hidden Units}}: {{Future}} Directions at the Intersection of Neuroscience and Artificial Intelligence @ {{NeurIPS}} 2019},
  author = {Bellec, Guillaume and Scherr, Franz and Hajek, Elias and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
  year = {2019},
  month = oct,
  urldate = {2025-04-21},
  abstract = {Learning in recurrent neural networks (RNNs) is most often implemented by gradient descent using backpropagation through time (BPTT), but BPTT does not model accurately how the brain learns. Instead, many experimental results on synaptic plasticity can be summarized as three-factor learning rules involving eligibility traces of the local neural activity and a third factor. We present here eligibility propagation (e-prop), a new factorization of the loss gradients in RNNs that fits the framework of three factor learning rules when derived for biophysical spiking neuron models. When tested on the TIMIT speech recognition benchmark, it is competitive with BPTT both for training artificial LSTM networks and spiking RNNs. Further analysis suggests that the diversity of learning signals and the consideration of slow internal neural dynamics are decisive to the learning efficiency of e-prop.},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\CFG9J83M\Bellec et al. - 2019 - Eligibility traces provide a data-inspired alternative to backpropagation through time.pdf}
}

@article{bittarSurrogateGradientSpiking2022,
  title = {A Surrogate Gradient Spiking Baseline for Speech Command Recognition},
  author = {Bittar, Alexandre and Garner, Philip N.},
  year = {2022},
  month = aug,
  journal = {Frontiers in Neuroscience},
  volume = {16},
  pages = {865897},
  issn = {1662-4548},
  doi = {10.3389/fnins.2022.865897},
  urldate = {2025-04-21},
  abstract = {Artificial neural networks (ANNs) are the basis of recent advances in artificial intelligence (AI); they typically use real valued neuron responses. By contrast, biological neurons are known to operate using spike trains. In principle, spiking neural networks (SNNs) may have a greater representational capability than ANNs, especially for time series such as speech; however their adoption has been held back by both a lack of stable training algorithms and a lack of compatible baselines. We begin with a fairly thorough review of literature around the conjunction of ANNs and SNNs. Focusing on surrogate gradient approaches, we proceed to define a simple but relevant evaluation based on recent speech command tasks. After evaluating a representative selection of architectures, we show that a combination of adaptation, recurrence and surrogate gradients can yield light spiking architectures that are not only able to compete with ANN solutions, but also retain a high degree of compatibility with them in modern deep learning frameworks. We conclude tangibly that SNNs are appropriate for future research in AI, in particular for speech processing applications, and more speculatively that they may also assist in inference about biological function.},
  pmcid = {PMC9479696},
  pmid = {36117617},
  file = {C:\Users\ricus\Zotero\storage\F7WFI9SZ\Bittar and Garner - 2022 - A surrogate gradient spiking baseline for speech command recognition.pdf}
}

@article{bittarSurrogateGradientSpiking2022a,
  title = {A Surrogate Gradient Spiking Baseline for Speech Command Recognition},
  author = {Bittar, Alexandre and Garner, Philip N.},
  year = {2022},
  month = aug,
  journal = {Frontiers in Neuroscience},
  volume = {16},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2022.865897},
  urldate = {2025-05-04},
  abstract = {Artificial neural networks (ANNs) are the basis of recent advances in artificial intelligence (AI); they typically use real valued neuron responses. By contrast, biological neurons are known to operate using spike trains. In principle, spiking neural networks (SNNs) may have a greater representational capability than ANNs, especially for time series such as speech; however their adoption has been held back by both a lack of stable training algorithms and a lack of compatible baselines. We begin with a fairly thorough review of literature around the conjunction of ANNs and SNNs. Focusing on surrogate gradient approaches, we proceed to define a simple but relevant evaluation based on recent speech command tasks. After evaluating a representative selection of architectures, we show that a combination of adaptation, recurrence and surrogate gradients can yield light spiking architectures that are not only able to compete with ANN solutions, but also retain a high degree of compatibility with them in modern deep learning frameworks. We conclude tangibly that SNNs are appropriate for future research in AI, in particular for speech processing applications, and more speculatively that they may also assist in inference about biological function.},
  langid = {english},
  keywords = {artificial intelligence,deep learning,physiologically plausible models,Signal processing,speech recognition,spiking neurons,Surrogate Gradient Learning},
  file = {C:\Users\ricus\Zotero\storage\VLPBCYKP\Bittar and Garner - 2022 - A surrogate gradient spiking baseline for speech command recognition.pdf}
}

@article{caoSpikingDeepConvolutional2015,
  title = {Spiking {{Deep Convolutional Neural Networks}} for {{Energy-Efficient Object Recognition}}},
  author = {Cao, Yongqiang and Chen, Yang and Khosla, Deepak},
  year = {2015},
  month = may,
  journal = {International Journal of Computer Vision},
  volume = {113},
  number = {1},
  pages = {54--66},
  issn = {1573-1405},
  doi = {10.1007/s11263-014-0788-3},
  urldate = {2025-03-12},
  abstract = {Deep-learning neural networks such as convolutional neural network (CNN) have shown great potential as a solution for difficult vision problems, such as object recognition. Spiking neural networks (SNN)-based architectures have shown great potential as a solution for realizing ultra-low power consumption using spike-based neuromorphic hardware. This work describes a novel approach for converting a deep CNN into a SNN that enables mapping CNN to spike-based hardware architectures. Our approach first tailors the CNN architecture to fit the requirements of SNN, then trains the tailored CNN in the same way as one would with CNN, and finally applies the learned network weights to an SNN architecture derived from the tailored CNN. We evaluate the resulting SNN on publicly available Defense Advanced Research Projects Agency (DARPA) Neovision2 Tower and CIFAR-10 datasets and show similar object recognition accuracy as the original CNN. Our SNN implementation is amenable to direct mapping to spike-based neuromorphic hardware, such as the ones being developed under the DARPA SyNAPSE program. Our hardware mapping analysis suggests that SNN implementation on such spike-based hardware is two orders of magnitude more energy-efficient than the original CNN implementation on off-the-shelf FPGA-based hardware.},
  langid = {english},
  keywords = {article,Artificial Intelligence,Convolutional neural networks,Deep learning,Machine learning,Neuromorphic circuits,Object recognition,Spiking neural networks}
}

@article{chenEssentialCharacteristicsMemristors2023,
  title = {Essential {{Characteristics}} of {{Memristors}} for {{Neuromorphic Computing}}},
  author = {Chen, Wenbin and Song, Lekai and Wang, Shengbo and Zhang, Zhiyuan and Wang, Guanyu and Hu, Guohua and Gao, Shuo},
  year = {2023},
  journal = {Advanced Electronic Materials},
  volume = {9},
  number = {2},
  pages = {2200833},
  issn = {2199-160X},
  doi = {10.1002/aelm.202200833},
  urldate = {2025-04-21},
  abstract = {The memristor is a resistive switch where its resistive state is programable based on the applied voltage or current. Memristive devices are thus capable of storing and computing information simultaneously, breaking the Von Neumann bottleneck. Since the first nanomemristor made by Hewlett-Packard in 2008, advances so far have enabled nanostructured, low-power, high-durability devices that exhibit superior performance over conventional CMOS devices. Herein, the development of memristors based on different physical mechanisms is reviewed. In particular, device stability, integration density, power consumption, switching speed, retention, and endurance of memristors, that are crucial for neuromorphic computing, are discussed in detail. An overview of various neural networks with a focus on building a memristor-based spike neural network neuromorphic computing system is then provided. Finally, the existing issues and challenges in implementing such neuromorphic computing systems are analyzed, and an outlook for brain-like computing is proposed.},
  copyright = {{\copyright} 2022 The Authors. Advanced Electronic Materials published by Wiley-VCH GmbH},
  langid = {english},
  keywords = {memristors,neural networks,neuromorphic computing,reliability,variability},
  file = {C\:\\Users\\ricus\\Zotero\\storage\\6J32K94G\\Chen et al. - 2023 - Essential Characteristics of Memristors for Neuromorphic Computing.pdf;C\:\\Users\\ricus\\Zotero\\storage\\VSME96I2\\10.1002aelm.html}
}

@article{daviesLoihiNeuromorphicManycore2018,
  title = {Loihi: {{A Neuromorphic Manycore Processor}} with {{On-Chip Learning}}},
  shorttitle = {Loihi},
  author = {Davies, Mike and Srinivasa, Narayan and Lin, Tsung-Han and Chinya, Gautham and Cao, Yongqiang and Choday, Sri Harsha and Dimou, Georgios and Joshi, Prasad and Imam, Nabil and Jain, Shweta and Liao, Yuyun and Lin, Chit-Kwan and Lines, Andrew and Liu, Ruokun and Mathaikutty, Deepak and McCoy, Steven and Paul, Arnab and Tse, Jonathan and Venkataramanan, Guruguhanathan and Weng, Yi-Hsin and Wild, Andreas and Yang, Yoonseok and Wang, Hong},
  year = {2018},
  month = jan,
  journal = {IEEE Micro},
  volume = {38},
  number = {1},
  pages = {82--99},
  issn = {1937-4143},
  doi = {10.1109/MM.2018.112130359},
  urldate = {2025-05-15},
  abstract = {Loihi is a 60-mm2 chip fabricated in Intels 14-nm process that advances the state-of-the-art modeling of spiking neural networks in silicon. It integrates a wide range of novel features for the field, such as hierarchical connectivity, dendritic compartments, synaptic delays, and, most importantly, programmable synaptic learning rules. Running a spiking convolutional form of the Locally Competitive Algorithm, Loihi can solve LASSO optimization problems with over three orders of magnitude superior energy-delay-product compared to conventional solvers running on a CPU iso-process/voltage/area. This provides an unambiguous example of spike-based computation, outperforming all known conventional solutions.},
  keywords = {Algorithm design and analysis,artificial intelligence,Biological neural networks,Computational modeling,Computer architecture,machine learning,neuromorphic computing,Neuromorphics,Neurons},
  file = {C:\Users\ricus\Zotero\storage\2T7PYCKZ\Davies et al. - 2018 - Loihi A Neuromorphic Manycore Processor with On-Chip Learning.pdf}
}

@article{dengMachineLearningParadigms2013,
  title = {Machine {{Learning Paradigms}} for {{Speech Recognition}}: {{An Overview}}},
  shorttitle = {Machine {{Learning Paradigms}} for {{Speech Recognition}}},
  author = {Deng, Li and Li, Xiao},
  year = {2013},
  month = may,
  journal = {IEEE Transactions on Audio, Speech, and Language Processing},
  volume = {21},
  number = {5},
  pages = {1060--1089},
  issn = {1558-7916, 1558-7924},
  doi = {10.1109/TASL.2013.2244083},
  urldate = {2025-04-19},
  abstract = {Automatic Speech Recognition (ASR) has historically been a driving force behind many machine learning (ML) techniques, including the ubiquitously used hidden Markov model, discriminative learning, structured sequence learning, Bayesian learning, and adaptive learning. Moreover, ML can and occasionally does use ASR as a large-scale, realistic application to rigorously test the effectiveness of a given technique, and to inspire new problems arising from the inherently sequential and dynamic nature of speech. On the other hand, even though ASR is available commercially for some applications, it is largely an unsolved problem---for almost all applications, the performance of ASR is not on par with human performance. New insight from modern ML methodology shows great promise to advance the state-of-the-art in ASR technology. This overview article provides readers with an overview of modern ML techniques as utilized in the current and as relevant to future ASR research and systems. The intent is to foster further cross-pollination between the ML and ASR communities than has occurred in the past. The article is organized according to the major ML paradigms that are either popular already or have potential for making significant contributions to ASR technology. The paradigms presented and elaborated in this overview include: generative and discriminative learning; supervised, unsupervised, semi-supervised, and active learning; adaptive and multi-task learning; and Bayesian learning. These learning paradigms are motivated and discussed in the context of ASR technology and applications. We finally present and analyze recent developments of deep learning and learning with sparse representations, focusing on their direct relevance to advancing ASR technology.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\I2PAFMPK\Deng and Li - 2013 - Machine Learning Paradigms for Speech Recognition An Overview.pdf}
}

@misc{EligibilityTracesProvide,
  title = {Eligibility Traces Provide a Data-Inspired Alternative to Backpropagation through Time {\textbar} {{Franz Scherr}}},
  urldate = {2025-04-21},
  howpublished = {https://www.franzscherr.com/publication/eligibility\_traces/},
  file = {C:\Users\ricus\Zotero\storage\W3ZRHQ7F\eligibility_traces.html}
}

@inproceedings{fangIncorporatingLearnableMembrane2021,
  title = {Incorporating {{Learnable Membrane Time Constant}} to {{Enhance Learning}} of {{Spiking Neural Networks}}},
  booktitle = {2021 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timoth{\'e}e and Huang, Tiejun and Tian, Yonghong},
  year = {2021},
  month = oct,
  pages = {2641--2651},
  publisher = {IEEE Computer Society},
  doi = {10.1109/ICCV48922.2021.00266},
  urldate = {2025-03-27},
  abstract = {Spiking Neural Networks (SNNs) have attracted enormous research interest due to temporal information processing capability, low power consumption, and high biological plausibility. However, the formulation of efficient and high-performance learning algorithms for SNNs is still challenging. Most existing learning methods learn weights only, and require manual tuning of the membrane-related parameters that determine the dynamics of a single spiking neuron. These parameters are typically chosen to be the same for all neurons, which limits the diversity of neurons and thus the expressiveness of the resulting SNNs. In this paper, we take inspiration from the observation that membrane-related parameters are different across brain regions, and propose a training algorithm that is capable of learning not only the synaptic weights but also the membrane time constants of SNNs. We show that incorporating learnable membrane time constants can make the network less sensitive to initial values and can speed up learning. In addition, we reevaluate the pooling methods in SNNs and find that max-pooling will not lead to significant information loss and have the advantage of low computation cost and binary compatibility. We evaluate the proposed method for image classification tasks on both traditional static MNIST, Fashion-MNIST, CIFAR-10 datasets, and neuromorphic N-MNIST, CIFAR10-DVS, DVS128 Gesture datasets. The experiment results show that the proposed method outperforms the state-of-the-art accuracy on nearly all datasets, using fewer time-steps. Our codes are available at https://github.com/fangwei123456/Parametric-Leaky-Integrate-and-Fire-Spiking-Neuron.},
  isbn = {978-1-6654-2812-5},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\ZJF93NK4\Fang et al. - 2021 - Incorporating Learnable Membrane Time Constant to Enhance Learning of Spiking Neural Networks.pdf}
}

@inproceedings{fangIncorporatingLearnableMembrane2021a,
  title = {Incorporating {{Learnable Membrane Time Constant To Enhance Learning}} of {{Spiking Neural Networks}}},
  booktitle = {Proceedings of the {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}}},
  author = {Fang, Wei and Yu, Zhaofei and Chen, Yanqi and Masquelier, Timoth{\'e}e and Huang, Tiejun and Tian, Yonghong},
  year = {2021},
  pages = {2661--2671},
  urldate = {2025-03-27},
  langid = {english},
  keywords = {timeconstant},
  file = {C:\Users\ricus\Zotero\storage\3B8HHKKS\Fang et al. - 2021 - Incorporating Learnable Membrane Time Constant To Enhance Learning of Spiking Neural Networks.pdf}
}

@article{fedorovaAdvancingNeuralNetworks2024,
  title = {Advancing {{Neural Networks}}: {{Innovations}} and {{Impacts}} on {{Energy Consumption}}},
  shorttitle = {Advancing {{Neural Networks}}},
  author = {Fedorova, Alina and Jovi{\v s}i{\'c}, Nikola and Vallverd{\`u}, Jordi and Battistoni, Silvia and Jovi{\v c}i{\'c}, Milo{\v s} and Medojevi{\'c}, Milovan and Toschev, Alexander and Alshanskaia, Evgeniia and Talanov, Max and Erokhin, Victor},
  year = {2024},
  journal = {Advanced Electronic Materials},
  volume = {10},
  number = {12},
  pages = {2400258},
  issn = {2199-160X},
  doi = {10.1002/aelm.202400258},
  urldate = {2025-03-12},
  abstract = {The energy efficiency of Artificial Intelligence (AI) systems is a crucial and actual issue that may have an important impact on an ecological, economic and technological level. Spiking Neural Networks (SNNs) are strongly suggested as valid candidates able to overcome Artificial Neural Networks (ANNs) in this specific contest. In this study, the proposal involves the review and comparison of energy consumption of the popular Artificial Neural Network architectures implemented on the CPU and GPU hardware compared with Spiking Neural Networks implemented in specialized memristive hardware and biological neural network human brain. As a result, the energy efficiency of Spiking Neural Networks can be indicated from 5 to 8 orders of magnitude. Some Spiking Neural Networks solutions are proposed including continuous feedback-driven self-learning approaches inspired by biological Spiking Neural Networks as well as pure memristive solutions for Spiking Neural Networks.},
  copyright = {{\copyright} 2024 The Author(s). Advanced Electronic Materials published by Wiley-VCH GmbH},
  langid = {english},
  keywords = {article,artificial neural network,energy consumption,LSTM,memristive device,ResNet,spiking neural network,transformer}
}

@article{florescuLearningPreciseSpike2019,
  title = {Learning with {{Precise Spike Times}}: {{A New Decoding Algorithm}} for {{Liquid State Machines}}},
  shorttitle = {Learning with {{Precise Spike Times}}},
  author = {Florescu, Dorian and Coca, Daniel},
  year = {2019},
  month = sep,
  journal = {Neural Computation},
  volume = {31},
  number = {9},
  pages = {1825--1852},
  issn = {0899-7667},
  doi = {10.1162/neco_a_01218},
  urldate = {2025-03-12},
  abstract = {There is extensive evidence that biological neural networks encode information in the precise timing of the spikes generated and transmitted by neurons, which offers several advantages over rate-based codes. Here we adopt a vector space formulation of spike train sequences and introduce a new liquid state machine (LSM) network architecture and a new forward orthogonal regression algorithm to learn an input-output signal mapping or to decode the brain activity. The proposed algorithm uses precise spike timing to select the presynaptic neurons relevant to each learning task. We show that using precise spike timing to train the LSM and selecting the readout presynaptic neurons leads to a significant increase in performance on binary classification tasks, in decoding neural activity from multielectrode array recordings, as well as in a speech recognition task, compared with what is achieved using the standard architecture and training methods.},
  keywords = {article,audio,LSM,speech}
}

@misc{garofoloTIMITAcousticPhoneticContinuous1993,
  title = {{{TIMIT Acoustic-Phonetic Continuous Speech Corpus}}},
  author = {Garofolo, John S. and Lamel, Lori F. and Fisher, William M. and Pallett, David S. and Dahlgren, Nancy L. and Zue, Victor and Fiscus, Jonathan G.},
  year = {1993},
  month = jan,
  publisher = {Linguistic Data Consortium},
  doi = {10.35111/17gk-bn40},
  urldate = {2025-05-20},
  abstract = {{$<$}h3{$>$}Introduction{$<$}/h3{$><$}br{$>$}  {$<$}p{$>$}The TIMIT corpus of read speech is designed to provide speech data for acoustic-phonetic studies and for the development and evaluation of automatic speech recognition systems. TIMIT contains broadband recordings of 630 speakers of eight major dialects of American English, each reading ten phonetically rich sentences. The TIMIT corpus includes time-aligned orthographic, phonetic and word transcriptions as well as a 16-bit, 16kHz speech waveform file for each utterance. Corpus design was a joint effort among the Massachusetts Institute of Technology (MIT), SRI International (SRI) and Texas Instruments, Inc. (TI). The speech was recorded at TI, transcribed at MIT and verified and prepared for CD-ROM production by the National Institute of Standards and Technology (NIST).{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}The TIMIT corpus transcriptions have been hand verified. Test and training subsets, balanced for phonetic and dialectal coverage, are specified. Tabular computer-searchable information is included as well as written documentation.{$<$}/p{$><$}br{$>$}  {$<$}h3{$>$}Samples{$<$}/h3{$><$}br{$>$}  {$<$}ul{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC93S1.phn" rel="nofollow"{$>$}phonemes{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC93S1.txt" rel="nofollow"{$>$}transcripts{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC93S1.wav" rel="nofollow"{$>$}audio{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}li{$><$}a href="desc/addenda/LDC93S1.wrd" rel="nofollow"{$>$}word list{$<$}/a{$><$}/li{$><$}br{$>$}  {$<$}/ul{$><$}/br{$>$}  Portions {\copyright} 1993 Trustees of the University of Pennsylvania},
  file = {C:\Users\ricus\Zotero\storage\TYT36BZA\1881146593179904768.html}
}

@misc{goupyGgoupySpikingConvNet2025,
  title = {Ggoupy/{{SpikingConvNet}}},
  author = {Goupy, Gaspard},
  year = {2025},
  month = mar,
  urldate = {2025-03-13},
  abstract = {Convolutional spiking neural network implementing STDP},
  keywords = {mnist-classification,software,spiking-neural-networks,stdp,synaptic-plasticity,unsupervised-learning}
}

@misc{GradientDescentGradient2019,
  title = {Gradient {{Descent}} - {{Gradient}} Descent - {{Product Manager}}'s {{Artificial Intelligence Learning Library}}},
  year = {2019},
  month = jan,
  journal = {产品经理的人工智能学习库},
  urldate = {2025-05-14},
  abstract = {Gradient descent is a first-order iterative optimization algorithm used to find the minimum value of a function. In order to find the local minimum of the function using the gradient descent, a step size proportional to the negative of the gradient (or approximation gradient) of the function at the current point is required. Conversely, if a step size proportional to the positive value of the gradient is used, the local maximum of the function is approached; then the process is referred to as a gradient rise.},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\9TRBBV9K\gradient-descent.html}
}

@article{guoAntidamageAbilityBiological2025,
  title = {Anti-Damage Ability of Biological Plausible Spiking Neural Network with Synaptic Time Delay Based on Speech Recognition under Random Attack},
  author = {Guo, Lei and Ding, Weihang and Wu, Youxi and Man, Menghua and Guo, Miaomiao},
  year = {2025},
  month = mar,
  journal = {Engineering Applications of Artificial Intelligence},
  volume = {144},
  pages = {110061},
  issn = {0952-1976},
  doi = {10.1016/j.engappai.2025.110061},
  urldate = {2025-03-24},
  abstract = {The complex electromagnetic environment can reduce the performance of neuromorphic hardware. The robustness of a brain-inspired model is beneficial for maintaining its performance under exterior attack. However, the synaptic plasticity in a brain-inspired model still lacks biological plausibility. The objective of this paper is to enhance the anti-damage ability of brain-inspired model under random attack by improving the biological plausibility of its synaptic plasticity. In this study, we propose a new spiking neural network (SNN) as a brain-inspired model, in which the topology is constrained by functional magnetic resonance imaging (fMRI) data on the human brain, its nodes are Izhikevich neuron models, and its edges are synaptic plasticity models with a random time delay that conforms to the range of the biological synaptic time delay (STD), and called it as fMRI-SNN with synaptic random time delay (SRTD). Then, taking speech recognition (SR) as a case study, we certify the recognition performance of fMRI-SNN with SRTD. To evaluate the anti-damage ability of fMRI-SNN with SRTD, we compare its SR accuracy before and after random attack. To elucidate the anti-damage mechanism, we discuss the neuroelectric characteristics, adaptive regulation of synaptic plasticity, and dynamic topological characteristics of fMRI-SNN with SRTD under random attack. The results indicate that our approach enhances the anti-damage ability of the brain-inspired model, and our discussion elucidates its anti-damage mechanism. Our results prompt that the brain-inspired model with biological plausibility can enhance its information processing ability.},
  keywords = {Anti-damage ability,Functional magnetic resonance imaging,snn-speech,Speech recognition,Spiking neural network,Synaptic time delay},
  file = {C:\Users\ricus\Zotero\storage\FJ8SI82L\S0952197625000612.html}
}

@article{guoDirectLearningbasedDeep2023,
  title = {Direct Learning-Based Deep Spiking Neural Networks: A Review},
  shorttitle = {Direct Learning-Based Deep Spiking Neural Networks},
  author = {Guo, Yufei and Huang, Xuhui and Ma, Zhe},
  year = {2023},
  month = jun,
  journal = {Frontiers in Neuroscience},
  volume = {17},
  pages = {1209795},
  issn = {1662-4548},
  doi = {10.3389/fnins.2023.1209795},
  urldate = {2025-05-04},
  abstract = {The spiking neural network (SNN), as a promising brain-inspired computational model with binary spike information transmission mechanism, rich spatially-temporal dynamics, and event-driven characteristics, has received extensive attention. However, its intricately discontinuous spike mechanism brings difficulty to the optimization of the deep SNN. Since the surrogate gradient method can greatly mitigate the optimization difficulty and shows great potential in directly training deep SNNs, a variety of direct learning-based deep SNN works have been proposed and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these direct learning-based deep SNN works, mainly categorized into accuracy improvement methods, efficiency improvement methods, and temporal dynamics utilization methods. In addition, we also divide these categorizations into finer granularities further to better organize and introduce them. Finally, the challenges and trends that may be faced in future research are prospected.},
  pmcid = {PMC10313197},
  pmid = {37397460},
  file = {C:\Users\ricus\Zotero\storage\9ASRHB4A\Guo et al. - 2023 - Direct learning-based deep spiking neural networks a review.pdf}
}

@article{hamianNovelTrainingApproach2024,
  title = {A {{Novel Training Approach}} in {{Deep Spiking Neural Network Based}} on {{Fuzzy Weighting}} and {{Meta-heuristic Algorithm}}},
  author = {Hamian, Melika and Faez, Karim and Nazari, Soheila and Sabeti, Malihe},
  year = {2024},
  month = feb,
  journal = {International Journal of Computational Intelligence Systems},
  volume = {17},
  number = {1},
  pages = {35},
  issn = {1875-6883},
  doi = {10.1007/s44196-024-00425-8},
  urldate = {2025-03-24},
  abstract = {The challenge of supervised learning in spiking neural networks (SNNs) for digit classification from speech signals is examined in this study. Meta-heuristic algorithms and a fuzzy logic framework are used to train SNNs. Using gray wolf optimization (GWO), the features obtained from audio signals are reduced depending on the dispersion of each feature. Then, it combines fuzzy weighting system (FWS) and spike time-dependent flexibility (STDP) approach to implement the learning rule in SNN. The FWS rule produces a uniformly distributed random weight in the STDP flexibility window, so that the system requires fewer training parameters. Finally, these neurons are fed data to estimate the training weights and threshold values of the neurons using wild horse algorithm (WHO). With the parameters given, these rule weights are applied to appropriately display the class's share in extracting the relevant feature. The suggested network can classify speech signals into categories with 97.17\% accuracy. The dataset was obtained using neurons operating at sparse biological rates below 600~Hz in the TIDIGITS test database. The suggested method has been evaluated on the IRIS and Trip Data datasets, where the classification results showed a 98.93\% and 97.36\% efficiency, respectively. Compared to earlier efforts, this study's results demonstrate that the strategy is both computationally simpler and more accurate. The accuracy of classification of digits, IRIS and Trip Data has increased by 4.9, 3.46 and 1.24\%, respectively. The principal goal of this research is to improve the accuracy of SNN by developing a new high-precision training method.},
  langid = {english},
  keywords = {Artificial Intelligence,Digit recognition system,Fuzzy weighting system (FWS),Gray wolf optimization (GWO),snn-speech,Spiking neural network (SNN),Wild horse algorithm (WHO)},
  file = {C:\Users\ricus\Zotero\storage\VSUL6ERU\Hamian et al. - 2024 - A Novel Training Approach in Deep Spiking Neural Network Based on Fuzzy Weighting and Meta-heuristic.pdf}
}

@article{hintonDeepNeuralNetworks2012,
  title = {Deep {{Neural Networks}} for {{Acoustic Modeling}} in {{Speech Recognition}}},
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
  year = {2012},
  abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feedforward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks with many hidden layers, that are trained using new methods have been shown to outperform Gaussian mixture models on a variety of speech recognition benchmarks, sometimes by a large margin. This paper provides an overview of this progress and represents the shared views of four research groups who have had recent successes in using deep neural networks for acoustic modeling in speech recognition.},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\MN5U7X6I\Hinton et al. - 2012 - Deep Neural Networks for Acoustic Modeling in Speech Recognition.pdf}
}

@misc{HttpsApiresearchrepositoryuwaeduauWs,
  title = {{{https://api.research-repository.uwa.edu.au/ws/portalfiles/portal/187295294/2109.12894v4.pdf}}},
  urldate = {2025-03-13},
  howpublished = {https://api.research-repository.uwa.edu.au/ws/portalfiles/portal/187295294/2109.12894v4.pdf}
}

@misc{HttpsArxivorgPdf,
  title = {{{https://arxiv.org/pdf/1901.09049}}},
  urldate = {2025-03-13},
  howpublished = {https://arxiv.org/pdf/1901.09049}
}

@misc{HttpsStaticgoogleusercontentcomMedia,
  title = {{{https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf}}},
  urldate = {2025-04-19},
  howpublished = {https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf},
  file = {C:\Users\ricus\Zotero\storage\6ULWA49K\38131.pdf}
}

@misc{IGITUGrazLSM2025,
  title = {{{IGITUGraz}}/{{LSM}}},
  year = {2025},
  month = jan,
  urldate = {2025-03-13},
  abstract = {Liquid State Machines in Python and NEST},
  copyright = {GPL-3.0},
  howpublished = {Institute for Theoretical Computer Science, TU Graz},
  keywords = {liquid-state-machine,lsm,recurrent-neural-networks,software,spiking-neural-networks}
}

@article{izhikevichSimpleModelSpiking2003,
  title = {Simple Model of Spiking Neurons},
  author = {Izhikevich, E.M.},
  year = {2003},
  month = nov,
  journal = {IEEE Transactions on Neural Networks},
  volume = {14},
  number = {6},
  pages = {1569--1572},
  issn = {1941-0093},
  doi = {10.1109/TNN.2003.820440},
  urldate = {2025-04-24},
  abstract = {A model is presented that reproduces spiking and bursting behavior of known types of cortical neurons. The model combines the biologically plausibility of Hodgkin-Huxley-type dynamics and the computational efficiency of integrate-and-fire neurons. Using this model, one can simulate tens of thousands of spiking cortical neurons in real time (1 ms resolution) using a desktop PC.},
  keywords = {Bifurcation,Biological system modeling,Biology computing,Biomembranes,Brain modeling,Computational modeling,Large-scale systems,Mathematical analysis,Mathematical model,Neurons},
  file = {C:\Users\ricus\Zotero\storage\4FLHLZ4P\Izhikevich - 2003 - Simple model of spiking neurons.pdf}
}

@article{izhikevichWhichModelUse2004,
  title = {Which Model to Use for Cortical Spiking Neurons?},
  author = {Izhikevich, E.M.},
  year = {2004},
  month = sep,
  journal = {IEEE Transactions on Neural Networks},
  volume = {15},
  number = {5},
  pages = {1063--1070},
  issn = {1941-0093},
  doi = {10.1109/TNN.2004.832719},
  urldate = {2025-04-24},
  abstract = {We discuss the biological plausibility and computational efficiency of some of the most useful models of spiking and bursting neurons. We compare their applicability to large-scale simulations of cortical neural networks.},
  keywords = {Artificial neural networks,Biological neural networks,Biological system modeling,Computational efficiency,Computational modeling,Fires,Frequency,Information processing,Large-scale systems,Neurons},
  file = {C:\Users\ricus\Zotero\storage\4Z5N5PZS\Izhikevich - 2004 - Which model to use for cortical spiking neurons.pdf}
}

@misc{jiangKLIFOptimizedSpiking2023,
  title = {{{KLIF}}: {{An}} Optimized Spiking Neuron Unit for Tuning Surrogate Gradient Slope and Membrane Potential},
  shorttitle = {{{KLIF}}},
  author = {Jiang, Chunming and Zhang, Yilei},
  year = {2023},
  month = feb,
  number = {arXiv:2302.09238},
  eprint = {2302.09238},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2302.09238},
  urldate = {2025-03-27},
  abstract = {Spiking neural networks (SNNs) have attracted much attention due to their ability to process temporal information, low power consumption, and higher biological plausibility. However, it is still challenging to develop efficient and high-performing learning algorithms for SNNs. Methods like artificial neural network (ANN)-to-SNN conversion can transform ANNs to SNNs with slight performance loss, but it needs a long simulation to approximate the rate coding. Directly training SNN by spike-based backpropagation (BP) such as surrogate gradient approximation is more flexible. Yet now, the performance of SNNs is not competitive compared with ANNs. In this paper, we propose a novel k-based leaky Integrate-and-Fire (KLIF) neuron model to improve the learning ability of SNNs. Compared with the popular leaky integrate-and-fire (LIF) model, KLIF adds a learnable scaling factor to dynamically update the slope and width of the surrogate gradient curve during training and incorporates a ReLU activation function that selectively delivers membrane potential to spike firing and resetting. The proposed spiking unit is evaluated on both static MNIST, Fashion-MNIST, CIFAR-10 datasets, as well as neuromorphic N-MNIST, CIFAR10-DVS, and DVS128-Gesture datasets. Experiments indicate that KLIF performs much better than LIF without introducing additional computational cost and achieves state-of-the-art performance on these datasets with few time steps. Also, KLIF is believed to be more biological plausible than LIF. The good performance of KLIF can make it completely replace the role of LIF in SNN for various tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,KLIF},
  file = {C\:\\Users\\ricus\\Zotero\\storage\\PT5E8UXI\\Jiang and Zhang - 2023 - KLIF An optimized spiking neuron unit for tuning surrogate gradient slope and membrane potential.pdf;C\:\\Users\\ricus\\Zotero\\storage\\FJTHCHY5\\2302.html}
}

@misc{kindigAIPowerConsumption,
  title = {{{AI Power Consumption}}: {{Rapidly Becoming Mission-Critical}}},
  shorttitle = {{{AI Power Consumption}}},
  author = {Kindig, Beth},
  journal = {Forbes},
  urldate = {2025-03-12},
  abstract = {Generative AI and rising GPU shipments is pushing data centers to scale to 100,000-plus accelerators, putting emphasis on power as a mission-critical problem to solve.},
  chapter = {Consumer Tech},
  howpublished = {https://www.forbes.com/sites/bethkindig/2024/06/20/ai-power-consumption-rapidly-becoming-mission-critical/},
  langid = {english},
  keywords = {news}
}

@misc{koopmanOvercomingLimitationsLayer2024,
  title = {Overcoming the {{Limitations}} of {{Layer Synchronization}} in {{Spiking Neural Networks}}},
  author = {Koopman, Roel and Yousefzadeh, Amirreza and Shahsavari, Mahyar and Tang, Guangzhi and Sifalakis, Manolis},
  year = {2024},
  month = aug,
  number = {arXiv:2408.05098},
  eprint = {2408.05098},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.05098},
  urldate = {2025-05-04},
  abstract = {Currently, neural-network processing in machine learning applications relies on layer synchronization, whereby neurons in a layer aggregate incoming currents from all neurons in the preceding layer, before evaluating their activation function. This is practiced even in artificial Spiking Neural Networks (SNNs), which are touted as consistent with neurobiology, in spite of processing in the brain being, in fact asynchronous. A truly asynchronous system however would allow all neurons to evaluate concurrently their threshold and emit spikes upon receiving any presynaptic current. Omitting layer synchronization is potentially beneficial, for latency and energy efficiency, but asynchronous execution of models previously trained with layer synchronization may entail a mismatch in network dynamics and performance. We present a study that documents and quantifies this problem in three datasets on our simulation environment that implements network asynchrony, and we show that models trained with layer synchronization either perform sub-optimally in absence of the synchronization, or they will fail to benefit from any energy and latency reduction, when such a mechanism is in place. We then "make ends meet" and address the problem with unlayered backprop, a novel backpropagation-based training method, for learning models suitable for asynchronous processing. We train with it models that use different neuron execution scheduling strategies, and we show that although their neurons are more reactive, these models consistently exhibit lower overall spike density (up to 50\%), reach a correct decision faster (up to 2x) without integrating all spikes, and achieve superior accuracy (up to 10\% higher). Our findings suggest that asynchronous event-based (neuromorphic) AI computing is indeed more efficient, but we need to seriously rethink how we train our SNN models, to benefit from it.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\ricus\\Zotero\\storage\\IZMFVK3W\\Koopman et al. - 2024 - Overcoming the Limitations of Layer Synchronization in Spiking Neural Networks.pdf;C\:\\Users\\ricus\\Zotero\\storage\\MXLSDP84\\2408.html}
}

@article{leeTrainingDeepSpiking2018,
  title = {Training {{Deep Spiking Convolutional Neural Networks With STDP-Based Unsupervised Pre-training Followed}} by {{Supervised Fine-Tuning}}},
  author = {Lee, Chankyu and Panda, Priyadarshini and Srinivasan, Gopalakrishnan and Roy, Kaushik},
  year = {2018},
  month = aug,
  journal = {Frontiers in Neuroscience},
  volume = {12},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2018.00435},
  urldate = {2025-05-04},
  abstract = {Spiking Neural Networks (SNNs) are fast becoming a promising candidate for brain-inspired neuromorphic computing because of their inherent power efficiency and impressive inference accuracy across several cognitive tasks such as image classification and speech recognition. The recent efforts in SNNs have been focused on implementing deeper networks with multiple hidden layers to incorporate exponentially more difficult functional representations. In this paper, we propose a pre-training scheme using biologically plausible unsupervised learning, namely Spike-Timing-Dependent-Plasticity (STDP), in order to better initialize the parameters in multi-layer systems prior to supervised optimization. The multi-layer SNN is comprised of alternating convolutional and pooling layers followed by fully-connected layers, which are populated with leaky integrate-and-fire spiking neurons. We train the deep SNNs in two phases wherein, first, convolutional kernels are pre-trained in a layer-wise manner with unsupervised learning followed by fine-tuning the synaptic weights with spike-based supervised gradient descent backpropagation. Our experiments on digit recognition demonstrate that the STDP-based pre-training with gradient-based optimization provides improved robustness, faster ({\textasciitilde}2.5 {\texttimes}) training time and better generalization compared with purely gradient-based training without pre-training.},
  langid = {english},
  keywords = {Convolutional Neural Network,Gradient Descent Backpropagation,leaky integrate and fire neuron,Spike Timing Dependent Plasticity,Spike-based Training,Spiking Neural network},
  file = {C:\Users\ricus\Zotero\storage\GIIMRILL\Lee et al. - 2018 - Training Deep Spiking Convolutional Neural Networks With STDP-Based Unsupervised Pre-training Follow.pdf}
}

@article{liResearchProgressNeural2023,
  title = {Research {{Progress}} of {{Neural Synapses Based}} on {{Memristors}}},
  author = {Li, Yamin and Su, Kang and Chen, Haoran and Zou, Xiaofeng and Wang, Changhong and Man, Hongtao and Liu, Kai and Xi, Xin and Li, Tuo},
  year = {2023},
  month = jan,
  journal = {Electronics},
  volume = {12},
  number = {15},
  pages = {3298},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-9292},
  doi = {10.3390/electronics12153298},
  urldate = {2025-04-21},
  abstract = {The memristor, characterized by its nano-size, nonvolatility, and continuously adjustable resistance, is a promising candidate for constructing brain-inspired computing. It operates based on ion migration, enabling it to store and retrieve electrical charges. This paper reviews current research on synapses using digital and analog memristors. Synapses based on digital memristors have been utilized to construct positive, zero, and negative weights for artificial neural networks, while synapses based on analog memristors have demonstrated their ability to simulate the essential functions of neural synapses, such as short-term memory (STM), long-term memory (LTM), spike-timing-dependent plasticity (STDP), spike-rate-dependent plasticity (SRDP), and paired-pulse facilitation (PPF). Furthermore, synapses based on analog memristors have shown potential for performing advanced functions such as experiential learning, associative learning, and nonassociative learning. Finally, we highlight some challenges of building large-scale artificial neural networks using memristors.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {analog,artificial neural networks,digital,memristor,synapse},
  file = {C:\Users\ricus\Zotero\storage\64EGAP9V\Li et al. - 2023 - Research Progress of Neural Synapses Based on Memristors.pdf}
}

@article{liuSSTDPSupervisedSpike2021,
  title = {{{SSTDP}}: {{Supervised Spike Timing Dependent Plasticity}} for {{Efficient Spiking Neural Network Training}}},
  shorttitle = {{{SSTDP}}},
  author = {Liu, Fangxin and Zhao, Wenbo and Chen, Yongbiao and Wang, Zongwu and Yang, Tao and Jiang, Li},
  year = {2021},
  month = nov,
  journal = {Frontiers in Neuroscience},
  volume = {15},
  pages = {756876},
  issn = {1662-4548},
  doi = {10.3389/fnins.2021.756876},
  urldate = {2025-05-04},
  abstract = {Spiking Neural Networks (SNNs) are a pathway that could potentially empower low-power event-driven neuromorphic hardware due to their spatio-temporal information processing capability and high biological plausibility. Although SNNs are currently more efficient than artificial neural networks (ANNs), they are not as accurate as ANNs. Error backpropagation is the most common method for directly training neural networks, promoting the prosperity of ANNs in various deep learning fields. However, since the signals transmitted in the SNN are non-differentiable discrete binary spike events, the activation function in the form of spikes presents difficulties for the gradient-based optimization algorithms to be directly applied in SNNs, leading to a performance gap (i.e., accuracy and latency) between SNNs and ANNs. This paper introduces a new learning algorithm, called SSTDP, which bridges the gap between backpropagation (BP)-based learning and spike-time-dependent plasticity (STDP)-based learning to train SNNs efficiently. The scheme incorporates the global optimization process from BP and the efficient weight update derived from STDP. It not only avoids the non-differentiable derivation in the BP process but also utilizes the local feature extraction property of STDP. Consequently, our method can lower the possibility of vanishing spikes in BP training and reduce the number of time steps to reduce network latency. In SSTDP, we employ temporal-based coding and use Integrate-and-Fire (IF) neuron as the neuron model to provide considerable computational benefits. Our experiments show the effectiveness of the proposed SSTDP learning algorithm on the SNN by achieving the best classification accuracy 99.3\% on the Caltech 101 dataset, 98.1\% on the MNIST dataset, and 91.3\% on the CIFAR-10 dataset compared to other SNNs trained with other learning methods. It also surpasses the best inference accuracy of the directly trained SNN with 25{\textasciitilde}32{\texttimes} less inference latency. Moreover, we analyze event-based computations to demonstrate the efficacy of the SNN for inference operation in the spiking domain, and SSTDP methods can achieve 1.3{\textasciitilde}37.7{\texttimes} fewer addition operations per inference. The code is available at: https://github.com/MXHX7199/SNN-SSTDP.},
  pmcid = {PMC8603828},
  pmid = {34803591},
  file = {C:\Users\ricus\Zotero\storage\M3IYA5HT\Liu et al. - 2021 - SSTDP Supervised Spike Timing Dependent Plasticity for Efficient Spiking Neural Network Training.pdf}
}

@article{maasNetworksSpikingNeurons1997,
  title = {Networks of Spiking Neurons: The Third Generation of Neural Network Models},
  shorttitle = {Networks of Spiking Neurons},
  author = {Maas, Wofgang},
  year = {1997},
  month = dec,
  journal = {Trans. Soc. Comput. Simul. Int.},
  volume = {14},
  number = {4},
  pages = {1659--1671},
  issn = {0740-6797}
}

@article{maassComputationalPowerCircuits2004,
  title = {On the Computational Power of Circuits of Spiking Neurons},
  author = {Maass, Wolfgang and Markram, Henry},
  year = {2004},
  month = dec,
  journal = {Journal of Computer and System Sciences},
  volume = {69},
  number = {4},
  pages = {593--616},
  issn = {0022-0000},
  doi = {10.1016/j.jcss.2004.04.001},
  urldate = {2025-03-27},
  abstract = {Complex real-time computations on multi-modal time-varying input streams are carried out by generic cortical microcircuits. Obstacles for the development of adequate theoretical models that could explain the seemingly universal power of cortical microcircuits for real-time computing are the complexity and diversity of their computational units (neurons and synapses), as well as the traditional emphasis on offline computing in almost all theoretical approaches towards neural computation. In this article, we initiate a rigorous mathematical analysis of the real-time computing capabilities of a new generation of models for neural computation, liquid state machines, that can be implemented with---in fact benefit from---diverse computational units. Hence, realistic models for cortical microcircuits represent special instances of such liquid state machines, without any need to simplify or homogenize their diverse computational units. We present proofs of two theorems about the potential computational power of such models for real-time computing, both on analog input streams and for spike trains as inputs.},
  file = {C\:\\Users\\ricus\\Zotero\\storage\\GU6MUVRD\\Maass and Markram - 2004 - On the computational power of circuits of spiking neurons.pdf;C\:\\Users\\ricus\\Zotero\\storage\\7IVNTCM2\\S0022000004000406.html}
}

@article{maassNetworksSpikingNeurons1997,
  title = {Networks of Spiking Neurons: {{The}} Third Generation of Neural Network Models},
  shorttitle = {Networks of Spiking Neurons},
  author = {Maass, Wolfgang},
  year = {1997},
  month = dec,
  journal = {Neural Networks},
  volume = {10},
  number = {9},
  pages = {1659--1671},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(97)00011-7},
  urldate = {2025-03-26},
  abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology.},
  keywords = {Computational complexity,Integrate-and-fire neutron,Lower bounds,Sigmoidal neural nets,Spiking neuron,third-generation},
  file = {C:\Users\ricus\Zotero\storage\U6IUTDR5\S0893608097000117.html}
}

@article{merollaMillionSpikingneuronIntegrated2014,
  title = {A Million Spiking-Neuron Integrated Circuit with a Scalable Communication Network and Interface},
  author = {Merolla, Paul A. and Arthur, John V. and {Alvarez-Icaza}, Rodrigo and Cassidy, Andrew S. and Sawada, Jun and Akopyan, Filipp and Jackson, Bryan L. and Imam, Nabil and Guo, Chen and Nakamura, Yutaka and Brezzo, Bernard and Vo, Ivan and Esser, Steven K. and Appuswamy, Rathinakumar and Taba, Brian and Amir, Arnon and Flickner, Myron D. and Risk, William P. and Manohar, Rajit and Modha, Dharmendra S.},
  year = {2014},
  journal = {Science},
  volume = {345},
  number = {6197},
  eprint = {24745271},
  eprinttype = {jstor},
  pages = {668--673},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  urldate = {2025-03-27},
  abstract = {Inspired by the brain's structure, we have developed an efficient, scalable, and flexible non--von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts.},
  keywords = {TrueNorth},
  file = {C:\Users\ricus\Zotero\storage\AGBUEKT2\Merolla et al. - 2014 - A million spiking-neuron integrated circuit with a scalable communication network and interface.pdf}
}

@article{nazariAccurateFastLearning2025,
  title = {An Accurate and Fast Learning Approach in the Biologically Spiking Neural Network},
  author = {Nazari, Soheila and Amiri, Masoud},
  year = {2025},
  month = feb,
  journal = {Scientific Reports},
  volume = {15},
  number = {1},
  pages = {6585},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-025-90113-0},
  urldate = {2025-03-23},
  abstract = {Computations adapted from the interactions of neurons in the nervous system have the potential to be a strong foundation for building computers with cognitive functions including decision-making, generalization, and real-time learning. In this context, a proposed intelligent machine is built on nervous system mechanisms. As a result, the output and middle layer of the machine is made up of a population of pyramidal neurons and interneurons, AMPA/GABA receptors, and excitatory and inhibitory neurotransmitters. The input layer of the machine is derived from the retinal model. A machine with a structure appropriate to biological evidence needs to learn based on biological evidence. Similar to this, the PSAC (Power-STDP Actor-Critic) learning algorithm was developed as a new learning mechanism based on unsupervised and reinforcement learning procedure. Four datasets MNIST, EMNIST, CIFAR10, and CIFAR100 were used to confirm the performance of the proposed learning algorithm compared to deep and spiking networks, and respectively accuracies of 97.7\%, 97.95\% (digits) and 93.73\% (letters), 93.6\%, and 75\% have been obtained, which shows an improvement in accuracy compared to previous spiking networks. The suggested learning strategy not only outperforms the earlier spike-based learning techniques in terms of accuracy but also exhibits a faster rate of convergence throughout the training phase.},
  copyright = {2025 The Author(s)},
  langid = {english},
  keywords = {article,Cognitive neuroscience,Computational neuroscience,Synaptic plasticity},
  file = {C:\Users\ricus\Zotero\storage\7QKEWH65\Nazari and Amiri - 2025 - An accurate and fast learning approach in the biologically spiking neural network.pdf}
}

@article{neftciSurrogateGradientLearning2019,
  title = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}: {{Bringing}} the {{Power}} of {{Gradient-Based Optimization}} to {{Spiking Neural Networks}}},
  shorttitle = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}},
  author = {Neftci, Emre O. and Hesham, Mostafa and Friedemann, Zenke},
  year = {2019},
  month = nov,
  journal = {IEEE Signal Processing Magazine},
  volume = {36},
  number = {6},
  pages = {51--63},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  issn = {1053-5888},
  doi = {10.1109/msp.2019.2931595},
  urldate = {2025-05-04},
  file = {C:\Users\ricus\Zotero\storage\M96W8PN2\1360579818148514560.html}
}

@article{nowotnyLossShapingEnhances2025,
  title = {Loss Shaping Enhances Exact Gradient Learning with {{Eventprop}} in Spiking Neural Networks},
  author = {Nowotny, Thomas and Turner, James P and Knight, James C},
  year = {2025},
  month = jan,
  journal = {Neuromorphic Computing and Engineering},
  volume = {5},
  number = {1},
  pages = {014001},
  publisher = {IOP Publishing},
  issn = {2634-4386},
  doi = {10.1088/2634-4386/ada852},
  urldate = {2025-03-24},
  abstract = {Event-based machine learning promises more energy-efficient AI on future neuromorphic hardware. Here, we investigate how the recently discovered Eventprop algorithm for gradient descent on exact gradients in spiking neural networks (SNNs) can be scaled up to challenging keyword recognition benchmarks. We implemented Eventprop in the GPU-enhanced neural networks framework (GeNN) and used it for training recurrent SNNs on the Spiking Heidelberg Digits (SHD) and Spiking Speech Commands (SSC) datasets. We found that learning depended strongly on the loss function and extended Eventprop to a wider class of loss functions to enable effective training. We then tested a large number of data augmentations and regularisations as well as exploring different network structures; and heterogeneous and trainable timescales. We found that when combined with two specific augmentations, the right regularisation and a delay line input, Eventprop networks with one recurrent layer achieved state-of-the-art performance on SHD and good accuracy on SSC. In comparison to a leading surrogate-gradient-based SNN training method, our GeNN Eventprop implementation is 3{\texttimes} faster and uses 4{\texttimes} less memory. This work is a significant step towards a low-power neuromorphic alternative to current machine learning paradigms.},
  langid = {english},
  keywords = {snn-speech},
  file = {C:\Users\ricus\Zotero\storage\4BH959B9\Nowotny et al. - 2025 - Loss shaping enhances exact gradient learning with Eventprop in spiking neural networks.pdf}
}

@misc{nowotnyTnowotnyGenn_eventprop2025,
  title = {Tnowotny/Genn\_eventprop},
  author = {Nowotny, Thomas},
  year = {2025},
  month = jan,
  urldate = {2025-05-20},
  abstract = {Implementation of eventprop learning in GeNN},
  copyright = {GPL-3.0}
}

@inproceedings{panayotovLibrispeechASRCorpus2015,
  title = {Librispeech: {{An ASR}} Corpus Based on Public Domain Audio Books},
  shorttitle = {Librispeech},
  booktitle = {2015 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2015},
  month = apr,
  pages = {5206--5210},
  issn = {2379-190X},
  doi = {10.1109/ICASSP.2015.7178964},
  urldate = {2025-05-20},
  abstract = {This paper introduces a new corpus of read English speech, suitable for training and evaluating speech recognition systems. The LibriSpeech corpus is derived from audiobooks that are part of the LibriVox project, and contains 1000 hours of speech sampled at 16 kHz. We have made the corpus freely available for download, along with separately prepared language-model training data and pre-built language models. We show that acoustic models trained on LibriSpeech give lower error rate on the Wall Street Journal (WSJ) test sets than models trained on WSJ itself. We are also releasing Kaldi scripts that make it easy to build these systems.},
  keywords = {Bioinformatics,Blogs,Corpus,Electronic publishing,Genomics,Information services,LibriVox,Resource description framework,Speech Recognition},
  file = {C:\Users\ricus\Zotero\storage\F9PUKLZ3\Panayotov et al. - 2015 - Librispeech An ASR corpus based on public domain audio books.pdf}
}

@misc{PapersCodeSSC,
  title = {Papers with {{Code}} - {{SSC Dataset}}},
  urldate = {2025-05-04},
  abstract = {The SSC dataset is a spiking version of the Speech Commands dataset release by Google (Speech Commands). SSC was  generated using Lauscher, an artificial cochlea model. The SSC dataset consists of utterances recorded from a larger number of speakers under controlled conditions. Spikes were generated in 700 input channels, and it contains 35 word categories from a large number of speakers. A full description of the dataset and how it was created can be found in the paper below. Please cite this paper if you make use of the dataset. Cramer, B.; Stradmann, Y.; Schemmel, J.; and Zenke, F. "The Heidelberg Spiking Data Sets for the Systematic Evaluation of Spiking Neural Networks". IEEE Transactions on Neural Networks and Learning Systems 33, 2744--2757, 2022.},
  howpublished = {https://paperswithcode.com/dataset/ssc},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\5PX8TVEU\ssc.html}
}

@misc{r.garyleonardTIDIGITS1993,
  title = {{{TIDIGITS}}},
  author = {{R. Gary Leonard} and {Doddington, George R.}},
  year = {1993},
  pages = {1597000 KB},
  publisher = {Linguistic Data Consortium},
  doi = {10.35111/72XZ-6X59},
  urldate = {2025-05-20},
  abstract = {{$<$}p{$>$}This\&nbsp;corpus\&nbsp;contains speech which was originally designed and collected at Texas Instruments, Inc. (TI) for the purpose of designing and evaluating algorithms for speaker-independent recognition of connected digit sequences. There are 326 speakers (111 men, 114 women, 50 boys and 51 girls) each pronouncing 77 digit sequences. Each speaker group is partitioned into test and training subsets.{$<$}/p{$><$}br{$>$}  {$<$}p{$>$}The corpus was collected at TI in 1982 in a quiet acoustic enclosure using an Electro-Voice RE-16 Dynamic Cardiod microphone, digitized at 20kHz. The waveform files are in the NIST SPHERE format.{$<$}/p{$><$}br{$>$}  {$<$}p{$>\&$}nbsp;{$<$}/p{$><$}br{$>$}  {$<$}p{$><$}strong{$>$}Updates{$<$}/strong{$><$}/p{$><$}br{$>$}  {$<$}p{$>$}As of April, 2015, TIDIGITS is also available in flac compressed wav. This package is available to licensees as an additional download. Not included in this version are the folders relating to handling the shortened sphere files of the original corpus.{$<$}/p{$><$}/br{$>$}  Portions {\copyright} 1993 Trustees of the University of Pennsylvania}
}

@article{rathiDIETSNNLowLatencySpiking2023,
  title = {{{DIET-SNN}}: {{A Low-Latency Spiking Neural Network With Direct Input Encoding}} and {{Leakage}} and {{Threshold Optimization}}},
  shorttitle = {{{DIET-SNN}}},
  author = {Rathi, Nitin and Roy, Kaushik},
  year = {2023},
  month = jun,
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {34},
  number = {6},
  pages = {3174--3182},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2021.3111897},
  urldate = {2025-03-27},
  abstract = {Bioinspired spiking neural networks (SNNs), operating with asynchronous binary signals (or spikes) distributed over time, can potentially lead to greater computational efficiency on event-driven hardware. The state-of-the-art SNNs suffer from high inference latency, resulting from inefficient input encoding and suboptimal settings of the neuron parameters (firing threshold and membrane leak). We propose DIET-SNN, a low-latency deep spiking network trained with gradient descent to optimize the membrane leak and the firing threshold along with other network parameters (weights). The membrane leak and threshold of each layer are optimized with end-to-end backpropagation to achieve competitive accuracy at reduced latency. The input layer directly processes the analog pixel values of an image without converting it to spike train. The first convolutional layer converts analog inputs into spikes where leaky-integrate-and-fire (LIF) neurons integrate the weighted inputs and generate an output spike when the membrane potential crosses the trained firing threshold. The trained membrane leak selectively attenuates the membrane potential, which increases activation sparsity in the network. The reduced latency combined with high activation sparsity provides massive improvements in computational efficiency. We evaluate DIET-SNN on image classification tasks from CIFAR and ImageNet datasets on VGG and ResNet architectures. We achieve top-1 accuracy of 69\% with five timesteps (inference latency) on the ImageNet dataset with 12{\textbackslash}times less compute energy than an equivalent standard artificial neural network (ANN). In addition, DIET-SNN performs 20-- 500{\textbackslash}times faster inference compared to other state-of-the-art SNN models.},
  keywords = {Backpropagation,Backpropagation through time (BPTT),Biological neural networks,Computational modeling,convolutional neural networks,Encoding,Neurons,spiking neural networks (SNNs),supervised learning,Task analysis,Training,weights},
  file = {C:\Users\ricus\Zotero\storage\UMC24RGZ\9556508.html}
}

@misc{ReleasesGennteamGenn,
  title = {Releases {$\cdot$} Genn-Team/Genn},
  journal = {GitHub},
  urldate = {2025-03-29},
  abstract = {GeNN is a GPU-enhanced Neuronal Network simulation environment based on code generation for Nvidia CUDA. - genn-team/genn},
  howpublished = {https://github.com/genn-team/genn/releases},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\ELMG7PB2\releases.html}
}

@article{ResonateandfireNeurons2001,
  title = {Resonate-and-Fire Neurons},
  year = {2001},
  month = jul,
  journal = {Neural Networks},
  volume = {14},
  number = {6-7},
  pages = {883--894},
  publisher = {Pergamon},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(01)00078-8},
  urldate = {2025-05-04},
  abstract = {We suggest a simple spiking model---resonate-and-fire neuron, which is similar to the integrate-and-fire neuron except that the state variable is comple{\dots}},
  langid = {american},
  file = {C:\Users\ricus\Zotero\storage\227I52AD\S0893608001000788.html}
}

@article{rostamiEpropSpiNNaker22022,
  title = {E-Prop on {{SpiNNaker}} 2: {{Exploring}} Online Learning in Spiking {{RNNs}} on Neuromorphic Hardware},
  shorttitle = {E-Prop on {{SpiNNaker}} 2},
  author = {Rostami, Amirhossein and Vogginger, Bernhard and Yan, Yexin and Mayr, Christian G.},
  year = {2022},
  month = nov,
  journal = {Frontiers in Neuroscience},
  volume = {16},
  pages = {1018006},
  issn = {1662-4548},
  doi = {10.3389/fnins.2022.1018006},
  urldate = {2025-04-21},
  abstract = {Introduction In recent years, the application of deep learning models at the edge has gained attention. Typically, artificial neural networks (ANNs) are trained on graphics processing units (GPUs) and optimized for efficient execution on edge devices. Training ANNs directly at the edge is the next step with many applications such as the adaptation of models to specific situations like changes in environmental settings or optimization for individuals, e.g., optimization for speakers for speech processing. Also, local training can preserve privacy. Over the last few years, many algorithms have been developed to reduce memory footprint and computation. Methods A specific challenge to train recurrent neural networks (RNNs) for processing sequential data is the need for the Back Propagation Through Time (BPTT) algorithm to store the network state of all time steps. This limitation is resolved by the biologically-inspired E-prop approach for training Spiking Recurrent Neural Networks (SRNNs). We implement the E-prop algorithm on a prototype of the SpiNNaker 2 neuromorphic system. A parallelization strategy is developed to split and train networks on the ARM cores of SpiNNaker 2 to make efficient use of both memory and compute resources. We trained an SRNN from scratch on SpiNNaker 2 in real-time on the Google Speech Command dataset for keyword spotting. Result We achieved an accuracy of 91.12\% while requiring only 680 KB of memory for training the network with 25 K weights. Compared to other spiking neural networks with equal or better accuracy, our work is significantly more memory-efficient. Discussion In addition, we performed a memory and time profiling of the E-prop algorithm. This is used on the one hand to discuss whether E-prop or BPTT is better suited for training a model at the edge and on the other hand to explore architecture modifications to SpiNNaker 2 to speed up online learning. Finally, energy estimations predict that the SRNN can be trained on SpiNNaker2 with 12 times less energy than using a NVIDIA V100 GPU.},
  pmcid = {PMC9742366},
  pmid = {36518534},
  file = {C:\Users\ricus\Zotero\storage\T92K8SA6\Rostami et al. - 2022 - E-prop on SpiNNaker 2 Exploring online learning in spiking RNNs on neuromorphic hardware.pdf}
}

@article{rostamiEpropSpiNNaker22022a,
  title = {E-Prop on {{SpiNNaker}} 2: {{Exploring}} Online Learning in Spiking {{RNNs}} on Neuromorphic Hardware},
  shorttitle = {E-Prop on {{SpiNNaker}} 2},
  author = {Rostami, Amirhossein and Vogginger, Bernhard and Yan, Yexin and Mayr, Christian G.},
  year = {2022},
  month = nov,
  journal = {Frontiers in Neuroscience},
  volume = {16},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2022.1018006},
  urldate = {2025-04-21},
  abstract = {Introduction In recent years, the application of deep learning models at the edge has gained attention. Typically, artificial neural networks (ANNs) are trained on graphics processing units (GPUs) and optimized for efficient execution on edge devices. Training ANNs directly at the edge is the next step with many applications such as the adaptation of models to specific situations like changes in environmental settings or optimization for individuals, e.g., optimization for speakers for speech processing. Also, local training can preserve privacy. Over the last few years, many algorithms have been developed to reduce memory footprint and computation. Methods A specific challenge to train recurrent neural networks (RNNs) for processing sequential data is the need for the Back Propagation Through Time (BPTT) algorithm to store the network state of all time steps. This limitation is resolved by the biologically-inspired E-prop approach for training Spiking Recurrent Neural Networks (SRNNs). We implement the E-prop algorithm on a prototype of the SpiNNaker 2 neuromorphic system. A parallelization strategy is developed to split and train networks on the ARM cores of SpiNNaker 2 to make efficient use of both memory and compute resources. We trained an SRNN from scratch on SpiNNaker 2 in real-time on the Google Speech Command dataset for keyword spotting. Result We achieved an accuracy of 91.12\% while requiring only 680 KB of memory for training the network with 25 K weights. Compared to other spiking neural networks with equal or better accuracy, our work is significantly more memory-efficient. Discussion In addition, we performed a memory and time profiling of the E-prop algorithm. This is used on the one hand to discuss whether E-prop or BPTT is better suited for training a model at the edge and on the other hand to explore architecture modifications to SpiNNaker 2 to speed up online learning. Finally, energy estimations predict that the SRNN can be trained on SpiNNaker2 with 12 times less energy than using a NVIDIA V100 GPU.},
  langid = {english},
  keywords = {e-prop,memory footprint,neuromorphic hardware,Online Learning,parallelism,SpiNNaker 2,training at the edge},
  file = {C:\Users\ricus\Zotero\storage\TT86LFSZ\Rostami et al. - 2022 - E-prop on SpiNNaker 2 Exploring online learning in spiking RNNs on neuromorphic hardware.pdf}
}

@misc{savareseLolemacsPytorcheventprop2024,
  title = {Lolemacs/Pytorch-Eventprop},
  author = {Savarese, Pedro},
  year = {2024},
  month = oct,
  urldate = {2025-05-20},
  abstract = {A PyTorch implementation of EventProp [https://arxiv.org/abs/2009.08378], a method to train Spiking Neural Networks}
}

@inproceedings{sboevSpokenDigitsClassification2024,
  title = {Spoken {{Digits Classification Using}} a {{Spiking Neural Network}} with {{Fixed Synaptic Weights}}},
  booktitle = {Biologically {{Inspired Cognitive Architectures}} 2023},
  author = {Sboev, Alexander and Balykov, Maksim and Kunitsyn, Dmitry and Serenko, Alexey},
  editor = {Samsonovich, Alexei V. and Liu, Tingting},
  year = {2024},
  pages = {767--774},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-50381-8_83},
  abstract = {The paper evaluates the applicability of an approach based on the usage of a spiking neural network with synaptic weights fixed from a uniform random distribution to solving audio data classification problems. On the example of the Free Spoken Digits Dataset pronounceable digit classification problem using a linear classifier trained on the output frequencies of spiking neurons as a decoder, an average accuracy of 94\% was obtained. This shows that the proposed spiking neural network performs such a transformation of the audio data that makes it linearly separable. Numerical experiments demonstrated the stability of the algorithm to the parameters of the spike layer, and it was shown that the constants of the threshold potential and the membrane leakage time can be both equal and different for different neurons.},
  isbn = {978-3-031-50381-8},
  langid = {english},
  keywords = {Audio classification,Reservoir computing,Spiking neural networks},
  file = {C:\Users\ricus\Zotero\storage\G8WKT4BV\Sboev et al. - 2024 - Spoken Digits Classification Using a Spiking Neural Network with Fixed Synaptic Weights.pdf}
}

@misc{shoesmithEventpropTrainingEfficient2025,
  title = {Eventprop Training for Efficient Neuromorphic Applications},
  author = {Shoesmith, Thomas and Knight, James C. and M{\'e}sz{\'a}ros, Bal{\'a}zs and Timcheck, Jonathan and Nowotny, Thomas},
  year = {2025},
  month = mar,
  number = {arXiv:2503.04341},
  eprint = {2503.04341},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.04341},
  urldate = {2025-04-14},
  abstract = {Neuromorphic computing can reduce the energy requirements of neural networks and holds the promise to `repatriate' AI workloads back from the cloud to the edge. However, training neural networks on neuromorphic hardware has remained elusive. Here, we instead present a pipeline for training spiking neural networks on GPUs, using the efficient event-driven Eventprop algorithm implemented in mlGeNN, and deploying them on Intel's Loihi 2 neuromorphic chip. Our benchmarking on keyword spotting tasks indicates that there is almost no loss in accuracy between GPU and Loihi 2 implementations and that classifying a sample on Loihi 2 is up to 10X faster and uses 200X less energy than on an NVIDIA Jetson Orin Nano.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Emerging Technologies,Computer Science - Neural and Evolutionary Computing},
  file = {C:\Users\ricus\Zotero\storage\RX2NVHA2\Shoesmith et al. - 2025 - Eventprop training for efficient neuromorphic applications.pdf}
}

@misc{SingleNeuromorphicMemristor,
  title = {Single Neuromorphic Memristor Closely Emulates Multiple Synaptic Mechanisms for Energy Efficient Neural Networks {\textbar} {{Nature Communications}}},
  urldate = {2025-04-21},
  howpublished = {https://www.nature.com/articles/s41467-024-51093-3},
  file = {C:\Users\ricus\Zotero\storage\NID6ZER2\s41467-024-51093-3.html}
}

@misc{SpikingHeidelbergDigits,
  title = {Spiking {{Heidelberg Digits}} and {{Spiking Speech Commands}} -- {{Zenke Lab}}},
  urldate = {2025-03-13},
  langid = {american},
  keywords = {data}
}

@misc{SpikingNeuralNetworks,
  title = {Spiking Neural Networks for Physiological and Speech Signals: A Review {\textbar} {{Biomedical Engineering Letters}}},
  urldate = {2025-03-24},
  howpublished = {https://link.springer.com/article/10.1007/s13534-024-00404-0},
  keywords = {snn-speech},
  file = {C:\Users\ricus\Zotero\storage\LYL7P59Y\s13534-024-00404-0.html}
}

@misc{SpokenDigitsClassification,
  title = {Spoken {{Digits Classification Based}} on {{Spiking Neural Networks}} with {{Memristor-Based STDP}}},
  urldate = {2025-03-24},
  abstract = {Spiking neural networks are commonly attributed to the third generation of neural networks. They mimic biological neurons more closely by processing information in the form of impulses (spikes) and are characterized by low power consumption and ease of hardware implementation. This paper shows two approaches to the task of classifying audio data represented by the spoken digits dataset using spiking neural networks with memristive plasticity. It is shown that both supervised and unsupervised learning methods based on local plasticity can be successfully used for audio classification. The models achieve accuracies ranging from 80\% to 94\% depending on the network topology, plasticity type and the way of decoding output neuronal activity. The results obtained in the paper can be a step towards creating neuromorhic devices for recognizing audio signals.},
  howpublished = {https://ieeexplore.ieee.org/document/10216628/citations?tabFilter=papers\#citations},
  langid = {american},
  keywords = {snn-speech},
  file = {C:\Users\ricus\Zotero\storage\83SYZ273\citations.html}
}

@misc{SpokenDigitsClassificationa,
  title = {Spoken {{Digits Classification Using}} a {{Spiking Neural Network}} with {{Fixed Synaptic Weights}}},
  urldate = {2025-04-21},
  howpublished = {http://ouci.dntb.gov.ua/en/works/4NBX0KY4/},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\QLWYPCSC\4NBX0KY4.html}
}

@misc{StateAI2025,
  title = {The State of {{AI}} in 2025: {{Global}} Survey {\textbar} {{McKinsey}}},
  urldate = {2025-03-12},
  howpublished = {https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai},
  keywords = {news}
}

@article{sunLearnableAxonalDelay2023,
  title = {Learnable Axonal Delay in Spiking Neural Networks Improves Spoken Word Recognition},
  author = {Sun, Pengfei and Chua, Yansong and Devos, Paul and Botteldooren, Dick},
  year = {2023},
  month = nov,
  journal = {Frontiers in Neuroscience},
  volume = {17},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2023.1275944},
  urldate = {2025-03-24},
  abstract = {{$<$}p{$>$}Spiking neural networks (SNNs), which are composed of biologically plausible spiking neurons, and combined with bio-physically realistic auditory periphery models, offer a means to explore and understand human auditory processing-especially in tasks where precise timing is essential. However, because of the inherent temporal complexity in spike sequences, the performance of SNNs has remained less competitive compared to artificial neural networks (ANNs). To tackle this challenge, a fundamental research topic is the configuration of spike-timing and the exploration of more intricate architectures. In this work, we demonstrate a learnable axonal delay combined with local skip-connections yields state-of-the-art performance on challenging benchmarks for spoken word recognition. Additionally, we introduce an auxiliary loss term to further enhance accuracy and stability. Experiments on the neuromorphic speech benchmark datasets, NTIDIDIGITS and SHD, show improvements in performance when incorporating our delay module in comparison to vanilla feedforward SNNs. Specifically, with the integration of our delay module, the performance on NTIDIDIGITS and SHD improves by 14\% and 18\%, respectively. When paired with local skip-connections and the auxiliary loss, our approach surpasses both recurrent and convolutional neural networks, yet uses 10 {\texttimes} fewer parameters for NTIDIDIGITS and 7 {\texttimes} fewer for SHD.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Auditory modelling,Axonal delay,neuromorphic computing,snn-speech,Speech Processing,Spiking Neural network,supervised learning},
  file = {C:\Users\ricus\Zotero\storage\9KARM7ZU\Sun et al. - 2023 - Learnable axonal delay in spiking neural networks improves spoken word recognition.pdf}
}

@misc{SurrogateGradientLearning,
  title = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}: {{Bringing}} the {{Power}} of {{Gradient-Based Optimization}} to {{Spiking Neural Networks}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/8891809}
}

@misc{SurrogateGradientLearninga,
  title = {Surrogate {{Gradient Learning}} in {{Spiking Neural Networks}}: {{Bringing}} the {{Power}} of {{Gradient-Based Optimization}} to {{Spiking Neural Networks}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2025-05-04},
  howpublished = {https://ieeexplore.ieee.org/abstract/document/8891809}
}

@article{thorpeSpeedProcessingHuman1996,
  title = {Speed of Processing in the Human Visual System},
  author = {Thorpe, Simon and Fize, Denis and Marlot, Catherine},
  year = {1996},
  volume = {381},
  langid = {english},
  file = {C:\Users\ricus\Zotero\storage\IKIYD4X4\Thorpe et al. - 1996 - Speed of processing in the human visual system.pdf}
}

@mastersthesis{veenIncludingSTDPEligibility2021,
  title = {Including {{STDP}} to Eligibility Propagation in Multi-Layer Recurrent Spiking Neural Networks},
  author = {van der Veen, Werner},
  year = {2021},
  urldate = {2025-04-21},
  abstract = {Spiking neural networks (SNNs) in neuromorphic systems are more energy efficient compared to deep learning--based methods, but there is no clear competitive learning algorithm for training such SNNs. Eligibility propagation (e-prop) offers an efficient and biologically plausible way to train competitive recurrent SNNs in low-power neuromorphic hardware. In this report, previous performance of e-prop on a speech classification task is reproduced, and the effects of including STDP-like behavior are analyzed. Including STDP to the ALIF neuron model improves the classification performance, but this is not the case for the Izhikevich e-prop neuron. Finally, it was found that e-prop implemented in a single-layer recurrent SNN consistently outperforms a multi-layer variant.},
  langid = {english},
  file = {C\:\\Users\\ricus\\Zotero\\storage\\RMW6DBKS\\Veen - 2021 - Including STDP to eligibility propagation in multi-layer recurrent spiking neural networks.pdf;C\:\\Users\\ricus\\Zotero\\storage\\5436HL4Z\\24344.html}
}

@inproceedings{vlasovSpokenDigitsClassification2022,
  title = {Spoken {{Digits Classification Based}} on {{Spiking Neural Networks}} with {{Memristor-Based STDP}}},
  booktitle = {2022 {{International Conference}} on {{Computational Science}} and {{Computational Intelligence}} ({{CSCI}})},
  author = {Vlasov, Danila and Davydov, Yury and Serenko, Alexey and Rybka, Roman and Sboev, Alexander},
  year = {2022},
  month = dec,
  pages = {330--335},
  issn = {2769-5654},
  doi = {10.1109/CSCI58124.2022.00066},
  urldate = {2025-04-21},
  abstract = {Spiking neural networks are commonly attributed to the third generation of neural networks. They mimic biological neurons more closely by processing information in the form of impulses (spikes) and are characterized by low power consumption and ease of hardware implementation. This paper shows two approaches to the task of classifying audio data represented by the spoken digits dataset using spiking neural networks with memristive plasticity. It is shown that both supervised and unsupervised learning methods based on local plasticity can be successfully used for audio classification. The models achieve accuracies ranging from 80\% to 94\% depending on the network topology, plasticity type and the way of decoding output neuronal activity. The results obtained in the paper can be a step towards creating neuromorhic devices for recognizing audio signals.},
  keywords = {Decoding,Distance measurement,memristors,Power demand,Scientific computing,Signal processing,Signal processing algorithms,sound classification,spiking neural networks,spoken digits,STDP,Training},
  file = {C:\Users\ricus\Zotero\storage\ANNJC682\Vlasov et al. - 2022 - Spoken Digits Classification Based on Spiking Neural Networks with Memristor-Based STDP.pdf}
}

@article{wangLTMDLearningImprovement2022,
  title = {{{LTMD}}: {{Learning Improvement}} of {{Spiking Neural Networks}} with {{Learnable Thresholding Neurons}} and {{Moderate Dropout}}},
  shorttitle = {{{LTMD}}},
  author = {Wang, Siqi and Cheng, Tee Hiang and Lim, Meng-Hiot},
  year = {2022},
  month = dec,
  journal = {Advances in Neural Information Processing Systems},
  volume = {35},
  pages = {28350--28362},
  urldate = {2025-03-27},
  langid = {english},
  keywords = {threshold},
  file = {C:\Users\ricus\Zotero\storage\CVTKQP82\Wang et al. - 2022 - LTMD Learning Improvement of Spiking Neural Networks with Learnable Thresholding Neurons and Modera.pdf}
}

@article{weilenmannSingleNeuromorphicMemristor2024,
  title = {Single Neuromorphic Memristor Closely Emulates Multiple Synaptic Mechanisms for Energy Efficient Neural Networks},
  author = {Weilenmann, Christoph and Ziogas, Alexandros Nikolaos and Zellweger, Till and Portner, Kevin and Mladenovi{\'c}, Marko and Kaniselvan, Manasa and Moraitis, Timoleon and Luisier, Mathieu and Emboras, Alexandros},
  year = {2024},
  month = aug,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {6898},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-51093-3},
  urldate = {2025-04-21},
  abstract = {Biological neural networks do not only include long-term memory and weight multiplication capabilities, as commonly assumed in artificial neural networks, but also more complex functions such as short-term memory, short-term plasticity, and meta-plasticity - all collocated within each synapse. Here, we demonstrate memristive nano-devices based on SrTiO3 that inherently emulate all these synaptic functions. These memristors operate in a non-filamentary, low conductance regime, which enables stable and energy efficient operation. They can act as multi-functional hardware synapses in a class of bio-inspired deep neural networks (DNN) that make use of both long- and short-term synaptic dynamics and are capable of meta-learning or learning-to-learn. The resulting bio-inspired DNN is then trained to play the video game Atari Pong, a complex reinforcement learning task in a dynamic environment. Our analysis shows that the energy consumption of the DNN with multi-functional memristive synapses decreases by about two orders of magnitude as compared to a pure GPU implementation. Based on this finding, we infer that memristive devices with a better emulation of the synaptic functionalities do not only broaden the applicability of neuromorphic computing, but could also improve the performance and energy costs of certain artificial intelligence applications.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Electronic devices,Information technology},
  file = {C:\Users\ricus\Zotero\storage\X6THU2EI\Weilenmann et al. - 2024 - Single neuromorphic memristor closely emulates multiple synaptic mechanisms for energy efficient neu.pdf}
}

@misc{wolffCowolffSimpleSpikingNeuralNetworkSTDP2025,
  title = {Cowolff/{{Simple-Spiking-Neural-Network-STDP}}},
  author = {Wolff, Cornelius},
  year = {2025},
  month = mar,
  urldate = {2025-03-13},
  abstract = {A simple from scratch implementation of a Spiking-Neural-Network with STDP in Python which is beeing trained on MNIST.},
  copyright = {Apache-2.0},
  keywords = {ann,mnist,python,snn,snn-stdp,software}
}

@article{wuDeepSpikingNeural2020,
  title = {Deep {{Spiking Neural Networks}} for {{Large Vocabulary Automatic Speech Recognition}}},
  author = {Wu, Jibin and Y{\i}lmaz, Emre and Zhang, Malu and Li, Haizhou and Tan, Kay Chen},
  year = {2020},
  month = mar,
  journal = {Frontiers in Neuroscience},
  volume = {14},
  pages = {199},
  issn = {1662-4548},
  doi = {10.3389/fnins.2020.00199},
  urldate = {2025-04-21},
  abstract = {Artificial neural networks (ANN) have become the mainstream acoustic modeling technique for large vocabulary automatic speech recognition (ASR). A conventional ANN features a multi-layer architecture that requires massive amounts of computation. The brain-inspired spiking neural networks (SNN) closely mimic the biological neural networks and can operate on low-power neuromorphic hardware with spike-based computation. Motivated by their unprecedented energy-efficiency and rapid information processing capability, we explore the use of SNNs for speech recognition. In this work, we use SNNs for acoustic modeling and evaluate their performance on several large vocabulary recognition scenarios. The experimental results demonstrate competitive ASR accuracies to their ANN counterparts, while require only 10 algorithmic time steps and as low as 0.68 times total synaptic operations to classify each audio frame. Integrating the algorithmic power of deep SNNs with energy-efficient neuromorphic hardware, therefore, offer an attractive solution for ASR applications running locally on mobile and embedded devices.},
  pmcid = {PMC7090229},
  pmid = {32256308},
  file = {C:\Users\ricus\Zotero\storage\IWCP5F27\Wu et al. - 2020 - Deep Spiking Neural Networks for Large Vocabulary Automatic Speech Recognition.pdf}
}

@article{wunderlichEventbasedBackpropagationCan2021,
  title = {Event-Based Backpropagation Can Compute Exact Gradients for Spiking Neural Networks},
  author = {Wunderlich, Timo C. and Pehle, Christian},
  year = {2021},
  month = jun,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {12829},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-91786-z},
  urldate = {2025-05-05},
  abstract = {Spiking neural networks combine analog computation with event-based communication using discrete spikes. While the impressive advances of deep learning are enabled by training non-spiking artificial neural networks using the backpropagation algorithm, applying this algorithm to spiking networks was previously hindered by the existence of discrete spike events and discontinuities. For the first time, this work derives the backpropagation algorithm for a continuous-time spiking neural network and a general loss function by applying the adjoint method together with the proper partial derivative jumps, allowing for backpropagation through discrete spike events without approximations. This algorithm, EventProp, backpropagates errors at spike times in order to compute the exact gradient in an event-based, temporally and spatially sparse fashion. We use gradients computed via EventProp to train networks on the Yin-Yang and MNIST datasets using either a spike time or voltage based loss function and report competitive performance. Our work supports the rigorous study of gradient-based learning algorithms in spiking neural networks and provides insights toward their implementation in novel brain-inspired hardware.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Learning algorithms,Machine learning,Mathematics and computing},
  file = {C:\Users\ricus\Zotero\storage\CCPQ3YGA\Wunderlich and Pehle - 2021 - Event-based backpropagation can compute exact gradients for spiking neural networks.pdf}
}

@misc{xiaoOnlineTrainingTime2022,
  title = {Online {{Training Through Time}} for {{Spiking Neural Networks}}},
  author = {Xiao, Mingqing and Meng, Qingyan and Zhang, Zongpeng and He, Di and Lin, Zhouchen},
  year = {2022},
  month = dec,
  number = {arXiv:2210.04195},
  eprint = {2210.04195},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.04195},
  urldate = {2025-03-20},
  abstract = {Spiking neural networks (SNNs) are promising brain-inspired energy-efficient models. Recent progress in training methods has enabled successful deep SNNs on large-scale tasks with low latency. Particularly, backpropagation through time (BPTT) with surrogate gradients (SG) is popularly used to achieve high performance in a very small number of time steps. However, it is at the cost of large memory consumption for training, lack of theoretical clarity for optimization, and inconsistency with the online property of biological learning and rules on neuromorphic hardware. Other works connect spike representations of SNNs with equivalent artificial neural network formulation and train SNNs by gradients from equivalent mappings to ensure descent directions. But they fail to achieve low latency and are also not online. In this work, we propose online training through time (OTTT) for SNNs, which is derived from BPTT to enable forward-in-time learning by tracking presynaptic activities and leveraging instantaneous loss and gradients. Meanwhile, we theoretically analyze and prove that gradients of OTTT can provide a similar descent direction for optimization as gradients based on spike representations under both feedforward and recurrent conditions. OTTT only requires constant training memory costs agnostic to time steps, avoiding the significant memory costs of BPTT for GPU training. Furthermore, the update rule of OTTT is in the form of three-factor Hebbian learning, which could pave a path for online on-chip learning. With OTTT, it is the first time that two mainstream supervised SNN training methods, BPTT with SG and spike representation-based training, are connected, and meanwhile in a biologically plausible form. Experiments on CIFAR-10, CIFAR-100, ImageNet, and CIFAR10-DVS demonstrate the superior performance of our method on large-scale static and neuromorphic datasets in small time steps.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing}
}

@inproceedings{yargaAcceleratingSNNTraining2023,
  title = {Accelerating {{SNN Training}} with {{Stochastic Parallelizable Spiking Neurons}}},
  booktitle = {2023 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Yarga, Sidi Yaya Arnaud and Wood, Sean U. N.},
  year = {2023},
  month = jun,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN54540.2023.10191884},
  urldate = {2025-05-04},
  abstract = {Spiking neural networks (SNN) are able to learn spatiotemporal features while using less energy, especially on neuromorphic hardware. The most widely used spiking neuron in deep learning is the Leaky Integrate and Fire (LIF) neuron. LIF neurons operate sequentially, however, since the computation of state at time t relies on the state at time t-1 being computed. This limitation is shared with Recurrent Neural Networks (RNN) and results in slow training on Graphics Processing Units (GPU). In this paper, we propose the Stochastic Parallelizable Spiking Neuron (SPSN) to overcome the sequential training limitation of LIF neurons. By separating the linear integration component from the non-linear spiking function, SPSN can be run in parallel over time. The proposed approach results in performance comparable with the state-of-the-art for feedforward neural networks on the Spiking Heidelberg Digits (SHD) dataset, outperforming LIF networks while training 10 times faster and outperforming non-spiking networks with the same network architecture. For longer input sequences of 10 000 time-steps, we show that the proposed approach results in 4000 times faster training, thus demonstrating the potential of the proposed approach to accelerate SNN training for very large datasets.},
  keywords = {Graphics processing units,hardware acceleration,Linear systems,Membrane potentials,neuromorphic computing,Neuromorphics,Neurons,parallelization,Recurrent neural networks,spiking neural networks,stochastic neurons,Training},
  file = {C\:\\Users\\ricus\\Zotero\\storage\\ECNBGCC5\\Yarga and Wood - 2023 - Accelerating SNN Training with Stochastic Parallelizable Spiking Neurons.pdf;C\:\\Users\\ricus\\Zotero\\storage\\JSWSYW9L\\10191884.html}
}

@inproceedings{yargaAcceleratingSNNTraining2023a,
  title = {Accelerating {{SNN Training}} with {{Stochastic Parallelizable Spiking Neurons}}},
  booktitle = {2023 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Yarga, Sidi Yaya Arnaud and Wood, Sean U. N.},
  year = {2023},
  month = jun,
  pages = {1--8},
  issn = {2161-4407},
  doi = {10.1109/IJCNN54540.2023.10191884},
  urldate = {2025-05-04},
  abstract = {Spiking neural networks (SNN) are able to learn spatiotemporal features while using less energy, especially on neuromorphic hardware. The most widely used spiking neuron in deep learning is the Leaky Integrate and Fire (LIF) neuron. LIF neurons operate sequentially, however, since the computation of state at time t relies on the state at time t-1 being computed. This limitation is shared with Recurrent Neural Networks (RNN) and results in slow training on Graphics Processing Units (GPU). In this paper, we propose the Stochastic Parallelizable Spiking Neuron (SPSN) to overcome the sequential training limitation of LIF neurons. By separating the linear integration component from the non-linear spiking function, SPSN can be run in parallel over time. The proposed approach results in performance comparable with the state-of-the-art for feedforward neural networks on the Spiking Heidelberg Digits (SHD) dataset, outperforming LIF networks while training 10 times faster and outperforming non-spiking networks with the same network architecture. For longer input sequences of 10 000 time-steps, we show that the proposed approach results in 4000 times faster training, thus demonstrating the potential of the proposed approach to accelerate SNN training for very large datasets.},
  keywords = {Graphics processing units,hardware acceleration,Linear systems,Membrane potentials,neuromorphic computing,Neuromorphics,Neurons,parallelization,Recurrent neural networks,spiking neural networks,stochastic neurons,Training},
  file = {C\:\\Users\\ricus\\Zotero\\storage\\5QN454PF\\Yarga and Wood - 2023 - Accelerating SNN Training with Stochastic Parallelizable Spiking Neurons.pdf;C\:\\Users\\ricus\\Zotero\\storage\\R6F5VRGI\\10191884.html}
}

@article{yuSTSCSNNSpatioTemporalSynaptic2022,
  title = {{{STSC-SNN}}: {{Spatio-Temporal Synaptic Connection}} with Temporal Convolution and Attention for Spiking Neural Networks},
  shorttitle = {{{STSC-SNN}}},
  author = {Yu, Chengting and Gu, Zheming and Li, Da and Wang, Gaoang and Wang, Aili and Li, Erping},
  year = {2022},
  month = dec,
  journal = {Frontiers in Neuroscience},
  volume = {16},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2022.1079357},
  urldate = {2025-03-24},
  abstract = {{$<$}p{$>$}Spiking neural networks (SNNs), as one of the algorithmic models in neuromorphic computing, have gained a great deal of research attention owing to temporal information processing capability, low power consumption, and high biological plausibility. The potential to efficiently extract spatio-temporal features makes it suitable for processing event streams. However, existing synaptic structures in SNNs are almost full-connections or spatial 2D convolution, neither of which can extract temporal dependencies adequately. In this work, we take inspiration from biological synapses and propose a Spatio-Temporal Synaptic Connection SNN (STSC-SNN) model to enhance the spatio-temporal receptive fields of synaptic connections, thereby establishing temporal dependencies across layers. Specifically, we incorporate temporal convolution and attention mechanisms to implement synaptic filtering and gating functions. We show that endowing synaptic models with temporal dependencies can improve the performance of SNNs on classification tasks. In addition, we investigate the impact of performance {$<$}italic{$>$}via{$<$}/italic{$>$} varied spatial-temporal receptive fields and reevaluate the temporal modules in SNNs. Our approach is tested on neuromorphic datasets, including DVS128 Gesture (gesture recognition), N-MNIST, CIFAR10-DVS (image classification), and SHD (speech digit recognition). The results show that the proposed model outperforms the state-of-the-art accuracy on nearly all datasets.{$<$}/p{$>$}},
  langid = {english},
  keywords = {attention mechanism,Backpropagation (BP),feedforward lateral inhibition (FLI),neuromorphic recognition,snn-speech,spatio-temporal synaptic connection (STSC),spike response filter (SRF),Spiking neural network (SNN)},
  file = {C:\Users\ricus\Zotero\storage\D4ETVKG7\Yu et al. - 2022 - STSC-SNN Spatio-Temporal Synaptic Connection with temporal convolution and attention for spiking ne.pdf}
}

@article{zhangDigitalLiquidState2015,
  title = {A {{Digital Liquid State Machine}} with {{Biologically Inspired Learning}} and {{Its Application}} to {{Speech Recognition}}},
  author = {Zhang, Y. and Li, P. and Jin, Y. and Choe, Y.},
  year = {2015},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  volume = {26},
  number = {11},
  pages = {2635--2649},
  doi = {10.1109/TNNLS.2015.2388544},
  abstract = {This paper presents a bioinspired digital liquid-state machine (LSM) for low-power very-large-scale-integration (VLSI)-based machine learning applications. To the best of the authors' knowledge, this is the first work that employs a bioinspired spike-based learning algorithm for the LSM. With the proposed online learning, the LSM extracts information from input patterns on the fly without needing intermediate data storage as required in offline learning methods such as ridge regression. The proposed learning rule is local such that each synaptic weight update is based only upon the firing activities of the corresponding presynaptic and postsynaptic neurons without incurring global communications across the neural network. Compared with the backpropagation-based learning, the locality of computation in the proposed approach lends itself to efficient parallel VLSI implementation. We use subsets of the TI46 speech corpus to benchmark the bioinspired digital LSM. To reduce the complexity of the spiking neural network model without performance degradation for speech recognition, we study the impacts of synaptic models on the fading memory of the reservoir and hence the network performance. Moreover, we examine the tradeoffs between synaptic weight resolution, reservoir size, and recognition performance and present techniques to further reduce the overhead of hardware implementation. Our simulation results show that in terms of isolated word recognition evaluated using the TI46 speech corpus, the proposed digital LSM rivals the state-of-the-art hidden Markov-model-based recognizer Sphinx-4 and outperforms all other reported recognizers including the ones that are based upon the LSM or neural networks. {\copyright} 2012 IEEE.},
  keywords = {article,Hardware implementation,liquid-state machine (LSM),speech recognition,spike-based learning.}
}

@misc{zhongSPikESSMSparsePrecise2024,
  title = {{{SPikE-SSM}}: {{A Sparse}}, {{Precise}}, and {{Efficient Spiking State Space Model}} for {{Long Sequences Learning}}},
  shorttitle = {{{SPikE-SSM}}},
  author = {Zhong, Yan and Zhao, Ruoyu and Wang, Chao and Guo, Qinghai and Zhang, Jianguo and Lu, Zhichao and Leng, Luziwei},
  year = {2024},
  month = oct,
  number = {arXiv:2410.17268},
  eprint = {2410.17268},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.17268},
  urldate = {2025-05-04},
  abstract = {Spiking neural networks (SNNs) provide an energy-efficient solution by utilizing the spike-based and sparse nature of biological systems. Since the advent of Transformers, SNNs have struggled to compete with artificial networks on long sequential tasks, until the recent emergence of state space models (SSMs), which offer superior computational efficiency and modeling capability. However, applying the highly capable SSMs to SNNs for long sequences learning poses three major challenges: (1) The membrane potential is determined by the past spiking history of the neuron, leading to reduced efficiency for sequence modeling in parallel computing scenarios. (2) Complex dynamics of biological spiking neurons are crucial for functionality but challenging to simulate and exploit effectively in large networks. (3) It is arduous to maintain high sparsity while achieving high accuracy for spiking neurons without resorting to dense computing, as utilized in artificial neuron-based SSMs. To address them, we propose a sparse, precise and efficient spiking SSM framework, termed SPikE-SSM. For (1), we propose a boundary compression strategy (PMBC) to accelerate the inference of the spiking neuron model, enabling parallel processing for long sequence learning. For (2), we propose a novel and concise neuron model incorporating reset-refractory mechanism to leverage the inherent temporal dimension for dynamic computing with biological interpretability. For (3), we hierarchically integrate the proposed neuron model to the original SSM block, and enhance the dynamics of SPikE-SSM by incorporating trainable thresholds and refractory magnitudes to balance accuracy and sparsity. Extensive experiments verify the effectiveness and robustness of SPikE-SSM on the long range arena benchmarks and large language dataset WikiText-103, showing the potential of dynamic spiking neurons in efficient long sequence learning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Neural and Evolutionary Computing},
  file = {C\:\\Users\\ricus\\Zotero\\storage\\BTUS829V\\Zhong et al. - 2024 - SPikE-SSM A Sparse, Precise, and Efficient Spiking State Space Model for Long Sequences Learning.pdf;C\:\\Users\\ricus\\Zotero\\storage\\88LABLDB\\2410.html}
}

@article{zhouDirectTrainingHighperformance2024,
  title = {Direct Training High-Performance Deep Spiking Neural Networks: A Review of Theories and Methods},
  shorttitle = {Direct Training High-Performance Deep Spiking Neural Networks},
  author = {Zhou, Chenlin and Zhang, Han and Yu, Liutao and Ye, Yumin and Zhou, Zhaokun and Huang, Liwei and Ma, Zhengyu and Fan, Xiaopeng and Zhou, Huihui and Tian, Yonghong},
  year = {2024},
  month = jul,
  journal = {Frontiers in Neuroscience},
  volume = {18},
  publisher = {Frontiers},
  issn = {1662-453X},
  doi = {10.3389/fnins.2024.1383844},
  urldate = {2025-03-11},
  abstract = {{$<$}p{$>$}Spiking neural networks (SNNs) offer a promising energy-efficient alternative to artificial neural networks (ANNs), in virtue of their high biological plausibility, rich spatial-temporal dynamics, and event-driven computation. The direct training algorithms based on the surrogate gradient method provide sufficient flexibility to design novel SNN architectures and explore the spatial-temporal dynamics of SNNs. According to previous studies, the performance of models is highly dependent on their sizes. Recently, direct training deep SNNs have achieved great progress on both neuromorphic datasets and large-scale static datasets. Notably, transformer-based SNNs show comparable performance with their ANN counterparts. In this paper, we provide a new perspective to summarize the theories and methods for training deep SNNs with high performance in a systematic and comprehensive way, including theory fundamentals, spiking neuron models, advanced SNN models and residual architectures, software frameworks and neuromorphic hardware, applications, and future trends.{$<$}/p{$>$}},
  langid = {english},
  keywords = {article,Deep spiking neural network,Direct training,energy efficiency,high performance,Residual connection,Transformer-based SNNs}
}
